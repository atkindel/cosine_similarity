---
title: |
 | Internally consistent estimation of multidimensional
 | word associations in text corpora
author: "Alexander T. Kindel\\footnote{Assistant Professor of Sociology, mÃ©dialab, Sciences Po, Paris, France. Contact: alexander.kindel@sciencespo.fr. I am grateful to B. Stewart for providing extensive guidance over the lifespan of this project. I also wish to thank A. Berg, D. Choi, T. Hansen, J. Lockhart, N. Torres-Echevarry, B. Rohr, and F. Wherry for helpful conversations regarding prior drafts of this paper; and N. Zhou and N. West for introducing me to an aleatoric representation of multidimensional word association problems.}"
date: "8 September 2023"
abstract: |
 |  The Word Embedding Association Test (WEAT) is a popular model for measuring word associations in text corpora (e.g., implicit biases, stereotypes, schemas). WEAT-like measurement models aim to estimate the difference in association between two concepts indexed by keyword lists over a set of word embeddings. I show that they do not consistently estimate this quantity. The underlying metric, mean cosine similarity, is a particular estimator of the expected unidimensional association between any two keywords, and it is not a consistent estimator for the multidimensional association between the two sets of words. Substantively, mean cosine similarity cannot discern what all of the keywords have in common: in keyword lists with at least four words, the metric does not guarantee that every sub-list containing at least three words is closest in association to itself. The degree of inconsistency is partially predictable from the conditioning of the cosine similarity matrix. Canonical correlation analysis provides a consistent and interpretable similarity metric for multidimensional word association problems.
 |
bibliography: Kindel_cosines_Science.bib
csl: science.csl
toc: FALSE
indent: TRUE
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: TRUE
    toc: FALSE
    dev: png
header-includes:
  - \usepackage{setspace}
  - \usepackage{enumitem}
  - \usepackage{epigraph}
  - \usepackage{cancel}
  - \usepackage{bbm}
  - \usepackage{rotating}
  - \usepackage{caption}
  - \captionsetup[figure]{labelfont=bf}
  - \setlength\epigraphwidth{0.9\textwidth}
  - \setlength\epigraphrule{0pt}
  - \renewcommand{\textflush}{flushepinormal}
  - \renewcommand{\epigraphflush}{center}
  - \setlist{listparindent=\parindent, parsep=0pt}
  - \doublespacing
  - \usepackage{indentfirst}
  - \setlength\parindent{24pt}
  - \setlength{\parskip}{0.5mm}
  - \usepackage[bottom]{footmisc}
  - \renewcommand{\footnotelayout}{\setstretch{1.05}}
  - \usepackage{dcolumn}
  - \usepackage{hhline}
---

\pagebreak

The Word Embedding Association Test (WEAT) is a popular method for measuring word associations in large text corpora using word embeddings [@caliskan2017semantics].^[Word embeddings are low-rank approximations to bivariate word association measures in text corpora. Embedding algorithms typically factor sparse $n \times n$ matrices of word association statistics into two dense rank $p$ vector spaces $W_R, W_C$ corresponding to the row and column spaces of the underlying measure. In practice, researchers use $W=W_R$ as the word vector space, or sometimes $W=W_R+W_C$ when the underlying measure is assumed to be symmetric. The results in this paper are agnostic to the choice of embedding algorithm; I assume only that $W$ lies in a subspace of $\mathbb{R}^p$. Results for widely-used and publicly available pretrained embeddings are included in the supplementary materials.] WEAT is modeled on the Implicit Association Test (IAT) in social psychology [@greenwald1998measuring]; like the IAT, it is meant to estimate differences in association between concepts encoded in keyword lists. However, WEAT-like measurement models do not consistently estimate this quantity. Given two keyword lists $A$ and $B$ with $k$ words each, the similarity metric underlying WEAT---mean cosine similarity (MCS)---is best understood as a certain estimator of the expected *unidimensional* word association: on average, how much does any pair of individual words $\{A_i, B_i\}$ have in common? This is a mathematically and conceptually distinct quantity from *multidimensional* word association: how much do all of the words in $A$ and all of the words in $B$ have in common? The confusion between these two estimands is the central topic of this paper.

I demonstrate that the MCS metric is an internally inconsistent estimator for multidimensional association. In particular, it fails a minimal admissibility criterion: given any list with more than three words, MCS is not guaranteed to say that all of the sublists are closest in resemblance to themselves. As a point of reference, I discuss an alternative multidimensional word association metric with close links to canonical correlation analysis [@hotelling1936cca]: the sum of the squared cosines of the principal angles between the subspaces spanned by the two word lists. Substantively, this is an estimator of how mutually predictive the two subspaces are. I refer to this quantity as the *canonical subspace metric*. 

Table 1 directly compares the performance of the two metrics on the ten WEATs presented in the original paper (see Materials and Methods). Notably, all ten tests exhibit a sign error or a magnitude error (or both). MCS yields estimates with the wrong sign for WEATs 2, 3, 5, 7, 8, and 9. The closest MCS estimate in magnitude is approximately 82% of the canonical estimate. MCS estimates tend to be inflated related to the canonical subspace metric, but magnitude errors in either direction are possible. The modal magnitude error approximately doubles the canonical subspace metric. The MCS estimate for WEAT5 is approximately 7.2 times too large, while the MCS estimate for WEAT8 is approximately 5.8 times too small.

To explain why MCS breaks down and why researchers should prefer an internally consistent multidimensional metric, the remainder of the paper compares the two metrics in greater mathematical and conceptual detail. I illustrate the difference between the two metrics with respect to a comparison between the "male" and "female" words in WEAT 7 and the "pleasant" and "unpleasant" words in WEAT 5 (see Table S1 and [@nosek2002harvesting]). Results are shown for publicly available GloVe embeddings of English Wikipedia [@pennington2014glove; @rodriguez2023multilanguage]. In the supplemental materials, I provide results for other commonly used pre-trained word embeddings and discuss a few WEAT-like measurement models that are widely used in applied settings.

**Minimal internal consistency in multidimensional similarity estimation.** A well-known requirement for geometric similarity measurement is *minimality*. This is typically phrased in terms of a conventional metric space axiom [@tversky1977features]:  
$$\forall x, y \in W: d(x, y) \geq d(x, x) = 0$$  
This is really two statements. The inequality statement can be generalized to multidimensional comparisons of row subspaces of $W$, but if we do this, the zero-equivalence statement is too strong. When we are comparing sets with cardinality greater than one, there is clearly the possibility of some non-uniformly distributed similarity internal to each set---otherwise we would not be willing to accept the unidimensional metric in the first place. So, our similarity metric should be sensitive to degrees of self-similarity. A more reasonable assumption is to allow the self-similarity to equal any scalar as long as this is the greatest similarity over all of the possible comparisons.^[For example, most readers will probably agree that $\{\text{he, him, his, himself}\}$ is more self-similar than $\{\text{heteroskedasticity, tomato, thirty-one, California}\}$. This does not require making any additional geometric assumptions, but doing so may be informative. In particular, if the remaining metric space axioms are added back into the definition and each $k$-subset is treated as a point in a space, then we have a "partial metric" in which pointwise distances incorporate the intrinsic size of each point [@matthews1994partial].]

A multidimensional similarity metric is *minimally internally consistent* if every $q$-subset of every $k$-subset of the rows of a real vector space $W$ is more similar to itself than any other $q$-subset of the same $k$-subset. In other words, if I choose any keyword list, then all of the sublists I can make by removing one or more words from that keyword list must be more similar to themselves than to any of the other same-size sublists with respect to my chosen similarity metric. To say the same thing formally, where $[W]^{kCq}$ denotes the set of $q$-subsets of some $k$-subset of $W$ $(k > q)$, then the following must be true:  
$$\forall i: \arg\sup_{j} \text{Sim}([W]^{kCq}_i, [W]^{kCq}_j) = i$$  
Define the index of inconsistency $\mathcal{I}_{q,k}$ for a similarity metric with respect to a given multidimensional word association problem as the discrete distribution of the number of maximally self-similar subsets belonging to all $q$-subsets of a list of cardinality $k$. A minimally internally consistent metric has places all of its mass on $q$ for any choice of $q$ and $k$. It is particularly worrisome if the expected value of $\mathcal{I}_{q,k}$ is decreasing as $k$ increases; adding more relevant keywords to an analysis should not make it less consistent.


**A canonical metric for multidimensional similarity.** To construct a multidimensional similarity metric that satisfies this criterion, define $W(k)$ as the space of subspaces spanned by $k$-subsets of the rows of $W$, and for all subspaces $A \in W(k)$ consider the orthogonal projection operator onto $A$, $\mathcal{P}(A) = A(A^TA)^{-1}A^T$. Then, equip $W(k)$ with the quadratic form $\text{Sim}_\text{CC; k}: W(k) \times W(k) \rightarrow \mathbb{R}$ corresponding to the Frobenius inner product of the orthogonal projections [@krzanowski1979between; @meyer2000matrix, p. 429--430; @borg2005modern, p. 439--441]:

$$
\begin{aligned}
\text{Sim}_\text{CC; k}(W_a, W_b) &= \left<\mathcal{P}(W_a),\mathcal{P}(W_b)\right>_F = \text{tr}(\mathcal{P}(W_a)^T\mathcal{P}(W_b)).
\end{aligned}
$$

$\text{Sim}_\text{CC; k}$ defines an inner product over the space of orthogonal projections onto $\mathcal{R}(W)$ [@horn2012matrix, p. 321]. It is always non-negative because the projection matrices are positive semi-definite. When the comparison is one-to-one it is exactly equivalent to the squared cosine similarity between the two vectors. Unlike mean cosine similarity, this quantity has the desired sensitivity to the dimensionality of the comparison. $\text{Sim}_\text{CC; k}(A, B)$ is constrained to lie between 0, indicating the subspaces share no common direction, and $p=\min(\text{rank}(A), \text{rank}(B))$, indicating the subspaces are isotropically directed relative to one-another up to the minimal rank. The metric can be scaled to lie between [0, 1] if we divide it by $\sqrt{\text{Sim}_\text{CC; k}(W_a, W_a)\text{Sim}_\text{CC; k}(W_b, W_b)}$, equivalent to the geometric mean of the lengths of the underlying word lists. The dimensionwise alignment is equivalent to the vector of singular values obtained by taking the singular value decomposition of the projection matrix product; each singular value corresponds exactly to the cosine of the angle between the $i$th pair of singular vectors. I refer to the total metric as the *canonical subspace metric* and its component quantities as the *canonical congruences*.

The metric is very closely related to canonical correlation analysis, but there are two critical modifications. First, the analysis is carried out without recentering the subspaces, so it is not a local correlation measure. We avoid recentering because the subsets of word vectors we have selected into the analysis already share a fixed point at the origin. If we subtract their local means, the overall difference in frequency of use between the two keyword lists distorts the metric.^[This can also be motivated from the perspective of the sparse high-dimensional word association measure approximated by the word embedding matrix. The word vectors lie on the surface of a high-dimensional convex body in $\mathbb{R}^n$ that we have mapped onto a hyperelliptical cross-section in $\mathbb{R}^k$. This surface captures most of the variation in position in the larger space [@dvoretzky1961convex]. However, we must not forget that we are ignoring the remaining $n-k$ dimensions. Imagine then that the word embedding matrix $W$ is missing $n-k$ columns of zeroes. It is safe to omit these columns when we take the unidimensional cosine because this does not affect the calculation of the angular metric once we have constructed the low-rank approximation. But, if we recenter subspaces of $W$ without remembering the remaining dimensions, our estimated location for the centroid will be very far off from the value we should have used, which is generally quite close to the zero vector. Additionally, if we take the mean of the cosine metrics as if they were computed over $k$ dimensions, we are considerably understating the dimensionality of the underlying measure.] Second, we carry out the analysis between row subspaces of $W$, and not the column spaces of two distinct matrices. Rather than studying two sets of measures on the same set of observations, we are studying two different sets of observations with respect to the same set of measures, i.e. the variation we have estimated by the word embedding algorithm. This quantity has been discussed in many areas of multivariate analysis; in psychometrics it is called Tucker's congruence coefficient [@tucker1951method; @korth1975distribution], and in the French school of data analysis it is more often called the RV coefficient [@escoufier1973traitement; @robert1976unifying].^[Kornblith and colleagues [@kornblith2019similarity] propose using a kernelized version of this quantity to measure similarity between neural network layers. This measure could be used if researchers were using word lists with cardinality larger than $\text{rank}(W)$. In practice, this is not necessary for word association problems of the scale targeted by WEAT-like models.]


**Why does WEAT fail to measure multidimensional semantic association?** We can now show that the score components in WEAT do not have the desired consistency property. Any WEAT measure is defined by two operations on the input word vector space $W$. First, the researchers selects disjoint keyword lists of length $k$ that appear in the vocabulary of $W$, which are presumed to represent "attributes" or their "targets". This step is typically performed using established IAT keyword lists. Given a keyword list, WEAT selects the row vector in $W$ with a label corresponding to this word. As a running example, Table S1 lists the keywords for a WEAT measure comparing the differential association between "male" or "female" words and "pleasant" or "unpleasant" words. Results pertaining to other sets of keyword lists are available in the supplementary materials.

Second, $W$ is equipped with a bivariate association metric (cosine similarity) that quantifies the amount of association between all $k^2$ pairs of word vectors across two keyword lists. The WEAT measure is then computed by taking the arithmetic grand mean of the $k \times k$ matrix of cosine similarities and comparing these scores to the other three mean cosine similarities. Formally, let $A, B, C, D$ be the subspaces of the row space of $W$ induced by four mutually disjoint keyword lists of equal length $k < p$.^[For brevity, I show results only for the case of equal word list lengths.]

The WEAT score with respect to $W_{A, B; C, D}$ is the grand mean difference in cosine similarities across $\{A,B\}$ and $\{C,D\}$: $\text{WEAT}(A, B, C, D) = 1/k^2\sum_i^k\sum_j^k(\cos(A_i,C_j) - \cos(A_i,D_j) + \cos(B_i,D_h) - \cos(B_i,C_j)$. Since this quantity is symmetric with respect to its arguments up to a sign change, it is sufficient to characterize the behavior of $\frac{1}{k^2}\sum_i^k\sum_j^k\cos(X_i, Y_i)$ for any two disjoint word vector subsets $X, Y \subset W$ of size $k$. Define the subspaces of $\mathcal{R}(W)$ spanned by $X, Y$ as $W_X, W_Y$; denote the power sets of $X, Y$ as $\text{Pow}(X), \text{Pow}(Y)$; and define the similarity metric $\psi(W_X, W_Y; k_i)$ between the subspaces corresponding to every $k_i$-subset in $\text{Pow}(X), \text{Pow}(Y)$, where $k_i \in [1, k_z-1]$.

To be minimally internally consistent, it is necessary for the expected total amount of association between $\text{Pow}(X)$ and $\text{Pow}(Y)$ to be nondecreasing in $k$. Informally, adding more words cannot subtract information. In general MCS is not nondecreasing in $k$. As the number of cosine similarity estimates corresponding to the size of the analysis increases, the association between any pair of words already in the list remains fixed with respect to the new words. This forces the MCS to converge to the expected coplanar angle in the input vector space as we increase $k$. In other words, MCS measures the expected cosine similarity between *any two vectors* in $X$ and $Y$, and it does not measure what or how much *all of the vectors* have in common. Thus WEAT imposes two inconsistent interpretations on $k$: from the reader's perspective incrementing $k$ tunes the measure toward a specific concept, but from the model's perspective it tunes toward generality. Considered independently, the keyword lists and the MCS metric appear unobjectionable enough; it is their combination that is fatal to the measure.

Researchers occasionally describe cosine similarity as "the dot product of the vectors after they have been normalized to unit length" (CITE CBN SI), but this is misleading because the normalizing projection operator is non-Euclidean. That is, *cosine similarity does not define an inner product space over the input word vectors*. Adding them does not consistently aggregate information about linear combinations of the component vectors. The cosine similarity space $\{W, \text{Sim}_\text{cos} \}$ is a locally weighted transformation of the usual inner product space defined by the Euclidean dot product $\{W, \cdot\}$ $1/\sqrt{\left<W_a,W_a\right>\left<W_b,W_b\right>}$. This space is scale invariant and linear in each argument only under a stringent condition on $W$: the Euclidean dot product between two vectors must equal negative one-half their respective norms everywhere in the vector space. A shorthand for this assumption is that the vector space must be *isotropic*. This identifies the fundamental problem with the quantity $\text{WEAT}(X, Y)$: addition and multiplication over cosine similarities are not defined in the conventional way unless $W$ is isotropic, and it is not possible for $W$ to be isotropic. To see why, define the index of isotropy $\text{Iso}(X)$ to be the dimension of the largest isotropic subspace of $X$ [@milnor1973symmetric, p. 56--57]. To establish that an inner product space is anisotropic with respect to the inner product operator $Q_\psi$, it suffices to show that $\text{Iso}(X) = 0$, i.e., that $X$ contains no self-orthogonal subspaces of dimension 1. This condition holds trivially for all popular word embedding models because their codomain is $\mathbb{R}^p$, so the dot product between any vector and itself is strictly greater than zero.

Applying the arithmetic mean to $\text{Sim}_\text{cos}$ despite this contradictory premise has an important substantive consequence: adding words to the inducing keyword lists always moves the representation toward generality rather than specificity. As $k$ increases, the mean cosine similarity converges in probability to the expected cosine between two vectors in $\mathbb{R}^p$ irrespective of the specific choice of input subspaces and basis dimensionality. Logically, this is the opposite of what we want the metric to do. The number of ways a set can be partitioned is proportional to the cardinality of the set. As we add more words to a word list, the number of ways that they can be similar increases, in addition to changing the total amount of similarity in the set. Consequently, if the word list is converging to a specific concept, we should gain *more information* about what we mean by adding to the word list, and not less.


**The canonical subspace metric is strictly more informative than the MCS metric.** Figure 1 demonstrates that the MCS estimator $\text{Sim}^*(I, J) := (1/k^2) \sum_i \sum_j \cos([W]^k_i, [W]^k_j)$ is not minimally internally consistent for $q \geq 3, k \geq 4$. To show this, we must consider the lattice of all possible analyses involving subsets of words in a keyword list $A$. Each column of heatmaps depicts the lattice of canonical subspace metrics (left) mean cosine similarities (center panel) between every subset of the keyword list at the top of the figure. Each cell of the heatmap corresponds to the metric over the corresponding $k$-subset. All 280 heatmaps for $q = 3$ are plotted by the unidimensional similarity condition number (X-axis) and their index of inconsistency (Y-axis) in the right panel.

This breakdown point feels quite arbitrary, so some informal intuition may be helpful. As the comparisons have more ways to overlap, eventually we observe subsets that incorporate a core set of words with many interrelations and an additional word that adds specificity along one dimension. In these mixed-relevance subsets, the contribution to the sum of the cosine similarity estimates with respect to the newly included "less related" word is much smaller than the more densely related subset, so the MCS metric will tend to be higher with the subsets that are entirely "more related." In practice, because there are only 16 triadic subset comparisons in a keyword list of cardinality 4, the corresponding metric may be "accidentally" consistent in the minimal case where the inconsistency is possible. However, the probability of this happening is closely related to the distribution of the unidimensional similarity metric, so it is unwise to rely on chance in this case. The inconsistency occurs almost surely for $k \geq 5$, but it is difficult to predict when it will occur in advance due to the combinatorial complexity of the underlying word association problem. A decent heuristic predictor for the index of inconsistency is the condition number of the cosine similarity matrix (Fig. 1; right panel).

Figure 2 displays the canonical subspace metric (Fig. 2; left panel) and the corresponding squared MCS metric (Fig. 2; center panel) over the lattice of combinations of word vectors. In the right panel, the distribution of metrics corresponding to each heatmap is displayed; here the mean *squared* cosine similarity is used to show the equivalence with the canonical subspace metric in the unidimensional regime. The expected mean cosine similarity over all possible sub-analyses of equivalent dimension is decreasing in the input dimension, while the canonical subspace metric increases monotonically in the number of dimensions as expected.

Figure 3 displays the canonical congruences for the four subspace pairs compared to the three possible expected distributions of minimal common subspace alignment in $W(k)$. The corresponding WEAT score component is shown for comparison (dotted line). Constructing a null comparison for the minimum common subspace alignments is moderately complex because there is not just one choice for the reference distribution. Each analysis implies three null distributions: the distribution obtained by randomizing both keyword lists, and the two distributions obtained by randomizing one list while holding the other fixed. There is no necessary relationship between these distributions, so there are many different answers to the question of whether an association is larger or smaller than what we would expect. One particularly interesting comparison is the comparison between the point estimate's relation to the two-way null and its relation to one or both of the one-way nulls. The association can be simultaneously smaller than what we would expect by a two-way random draw and larger than what we would expect by either one-way random draw, and vice-versa.

**Implications for word association measurement.** WEAT-like measures are widely used in the social sciences to measure a wide range of cultural processes observable in text that traditionally were measured by human raters [@kozlowski2019geometry; @nelson2021leveraging]. The key validity evidence for WEAT in this context is its agreement with human word association ratings (i.e., the IAT and similar psychological test-based methods). However, the convergent validity of the measure is relatively weak evidence if it is not also internally consistent. The canonical subspace metric is always a more consistent estimator of the targeted variation than MCS, and facilitates interpreting variation in the scale of observed associations in applied research. A general conclusion is that researchers should avoid computing WEAT-like measures of semantic association in applied statistical text analysis.

It is worth emphasizing that this result *cannot* be interpreted as evidence that stereotypes and biases do not exist. That conclusion would be profoundly mistaken: absence of evidence is not evidence of absence. The more fundamental problem is that this measurement approach cannot provide the evidence we would need to show that such biases are "imprinted" on word associations as such, rather than determined by the specific social contexts in which language is written and spoken. In particular, it is not clear why we should define (e.g.) stereotypical association for Black names with respect to white names, or biases against women with respect to men. The possibility of measuring multidimensional word associations raises significant trouble for the notion that words can be assumed to be categorically opposed in meaning in this way. In some contexts male/female and white/Black are treated as opposites, but this is only one limited perspective on the vast spectrum of similarities, differences, and ambiguities indexed by gender and racial identity [@butler1990gender, @hobbs2014chosen]. Reliance on keyword lists as a priori representations of "concepts" ("attributes", "identities", "schemas", etc.) renders this measurement approach dependent on the reader's intuitive acceptance of the concept label, rather than evidence that the keywords specifically pick out this concept. It is logically inconsistent to presume the meaning of words in advance of seeing their contexts if we are trying to learn word meanings by observing the contextual use of language [@wittgenstein1953pi].

\pagebreak

## References

\small

<div id="refs"></div>

\normalsize

\pagebreak

## Figures

### Captions

#### Figure 1.
Multidimensional similarity metrics within subspaces. Each column forms a lattice over the set of comparisons between subspaces. Each row increases the subspace dimensionality by one (top row is 1, bottom row is 7). Red dots indicate the row/column maximum (note the matrices are symmetric). A minimally internally consistent similarity measure for multidimensional word association problems must locate all of the maxima on the diagonal. The canonical subspace metric satisfies this criterion; the mean cosine similarity metric does not (heatmaps, left panel). The amount of inconsistency is partially predictable from the conditioning of the cosine similarity matrix, particularly if the overall variance is low (scatterplots, right panel).

#### Figure 2.
Multidimensional similarity metrics between subspaces. Cell colors indicate more commonality between subsets (heatmaps, left panel). The solid red (canonical subspace) and blue (mean squared cosine similarity) lines indicate their respective expectations. The dashed lines indicate the maximum and minimum values at each dimensionality. The canonical subspace metric increases as we gain more information about the common semantics of the input word. When the word association problem involves only one-to-one word pairings, the canonical subspace metric is equivalent to the *squared* cosine similarity. The mean cosine similarity decreases toward the global mean as more words are added to the analysis, whereas the canonical subspace metric increases.

#### Figure 3.
Canonical congruences (red diamonds) with 95% prediction intervals for the one- and two-way null distributions. Blue intervals correspond to the one-way null distributions holding the pleasant/unpleasant lists constant while randomizing the male/female lists. Yellow intervals correspond to the one-way null distributions holding the male/female lists constant while randomizing the pleasant/unpleasant lists. Green intervals randomize both lists. The mean cosine similarity metric for each comparison is plotted as the pink dotted line; the canonical subspace metric is plotted as the red dashed line.

```{r setup, echo=F, message=F, warning=F}
library(tidyverse)
library(magrittr)
library(rsvd)
library(ggpubr)
library(patchwork)
library(kableExtra)
library(here)

# Document settings
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE,  # don't pollute the PDF with error messages
                      cache=TRUE, cache.lazy=FALSE, eval=FALSE,  # don't rerun the analysis unless eval=T
                      dpi=300, fig.height=9)  # figure settings
set.seed(271828)
```

```{r data_setup}
# Load Rodriguez et al. GloVe
# https://alcembeddings.org/alcdata
glove.d <- read_delim(here("data", "glove_vectors_enwiki.txt"), " ", col_names = F)
glove.m <- do.call(cbind, glove.d[,-1])
rownames(glove.m) <- glove.d$X1
rm(glove.d)

# Alternative embeddings -- uncomment if needed

# # Load Pennington et al. GloVe
# # https://nlp.stanford.edu/projects/glove/
# glove.d2 <- read_table2(here("data", "glove.6B.300d.txt"), col_names = FALSE)
# glove.m2 <- do.call(cbind, glove.d2[,-1])
# rownames(glove.m2) <- glove.d2$X1
# rm(glove.d2)
# 
# # Load word2vec (currently using Google's public SGNS embeddings of Google News)
# # using sed, I removed the top line (the matrix dimensions) and changed "shortterm" to "short-term"
# # I also subsetted to the top 1.2M terms for memory reasons, but you don't have to
# # https://code.google.com/archive/p/word2vec/
# word2vec.d <- read_delim(here("data", "GoogleNews-vectors-negative300-top12M.txt"), " ", col_names = F)
# word2vec.m <- do.call(cbind, word2vec.d[,-1])
# rownames(word2vec.m) <- word2vec.d$X1
# rm(word2vec.d)

# WEAT keyword lists (gender sentiment analysis)
male_a <- str_split("male, man, boy, brother, he, him, his, son", ", ", Inf, T)
female_a <- str_split("female, woman, girl, sister, she, her, hers, daughter", ", ", Inf, T)
pleasant_b <- str_split("joy, love, peace, wonderful, pleasure, friend, laughter, happy", ", ", Inf, T)
unpleasant_b <- str_split("agony, terrible, horrible, nasty, evil, war, awful, failure", ", ", Inf, T)
ss.k <- length(male_a)

# This code complains if we can't support the test lexicon
# all_words <- c(male_a, female_a, pleasant_b, unpleasant_b)
# vocab_supported <- length(c(all_words[which(!all_words %in% glove.d$X1)], all_words[which(!all_words %in% glove.d$X1)])) == 0
# stopifnot(vocab_supported)

# set the input embeddings
# recommend removing the other two if you're not using them, they're big
embed.m <- glove.m  # default
# embed.m <- glove.m2
# embed.m <- word2vec.m
```

```{r canonical}
# Krzanowski (1979) common subspace metric
# if A and B are the subspaces and A(AtA)'AtB(BtB)'Bt = USV' is the svd then S is the
# vector of principal angles between the subspaces (set by the choice of # words)
# you can avoid the matrix inversion if you orthonormalize the subspaces first, but they're small anyways
# the spectral norm and the trace summarize the size of the common subspace nicely
# cohen & crane 1995 recommend using the distance metric sqrt(p-Krz(A, B)) where A and B are {n,p}
# s1/s2.oc options use the residual maker matrix instead of the projection matrix for this subspace
# using the randomized svd makes it go faster; maybe check the usual way too?
# you don't have to SVD it if you don't care about the component angles.
krz <- function(M, s1, s2, k, s1.oc=F, s2.oc=F, .tstat=F, .svd=F, .rsvd=F, .cbm=F) {
  # b1 <- svd(t(M[s1,]))$u  # if you do this, the inverse covariance matrices are identity
  # b2 <- svd(t(M[s2,]))$u
  b1 <- M[s1,]
  b2 <- M[s2,]
  if(k == 1) {
    # R is really annoying for this one
    b1 <- t(b1)
    b2 <- t(b2)
  }
  p1 <- crossprod(b1, solve(tcrossprod(b1))) %*% b1
  p2 <- crossprod(b2, solve(tcrossprod(b2))) %*% b2
  
  # often we just need the statistic
  # we can avoid the matrix multiplication by summing the Schur product margin
  # note px is hermitian so we don't need to transpose and rowsum=colsum
  if (.tstat) {
    # return(sum(diag(cbM)))
    return(sum(colSums(p1 * p2)))
  } 

  # otherwise we want the projection product
  cbM <- NA
  if(s1.oc & s2.oc) {
    r1 <- diag(1, nrow(p1), ncol(p1)) - p1
    r2 <- diag(1, nrow(p2), ncol(p2)) - p2
    cbM <- r1 %*% r2
  } else if (s1.oc & (!s2.oc)) {
    r1 <- diag(1, nrow(p1), ncol(p1)) - p1
    cbM <- r1 %*% p2
  } else if ((!s1.oc) & s2.oc) {
    r2 <- diag(1, nrow(p2), ncol(p2)) - p2
    cbM <- p1 %*% r2
  } else {
    cbM <- p1 %*% p2
  }
  
  if(.svd) {
    if(.rsvd) {
      return(rsvd(cbM, k))
    } else {
      return(svd(cbM))
    }
  } else if(.cbm) {
    return(cbM)
  } else {
    # just compute the singular values
    # these are the angles; the top-level stat is sum(v^2),
    #  i.e. the sum of the *eigenvalues*
    if(.rsvd) {
      return(rsvd(cbM, k=k, nu=0, nv=0)$d)
    } else {
      return(svd(cbM, nu=0, nv=0)$d[1:k])
    }
  }
}

# common subspace projection basis for subspaces b1, b2
krz.b <- function(b1, b2) {
  p1 <- t(b1) %*% solve(tcrossprod(b1)) %*% b1
  p2 <- t(b2) %*% solve(tcrossprod(b2)) %*% b2
  cbM <- p1 %*% p2
  return(svd(cbM))
}

# compute angles between word vectors
ss.cos <- function(M, s1, s2) {
  b1 <- M[s1,]
  b2 <- M[s2,]
  if(length(b1) == dim(M)[2]) {
    # R is really annoying for this one
    b1 <- t(b1)
    b2 <- t(b2)
  }
  csM <- sapply(1:nrow(b1), function(i) sapply(1:nrow(b2), function(j) lsa::cosine(b1[i,], b2[j,])))
  return(csM)
}

# sample n non-overlapping random subspaces from M of size 2k (to split in half)
rand.ss <- function(M, n, k) {
  return(lapply(1:n, function(i) sample(1:nrow(M), k*2)))
}

# same as above but use the word index.
rand.ss.w <- function(M, n, k) {
  return(lapply(1:n, function(i) sample(rownames(M), k*2)))
}

# construct Krzanowski measures over two word lists A,B and embedding M
# dimensionality is set by this choice (they should be equal sizes!)
# this takes a while because we have to do 3n SVDs. for testing we use only a 
# small number of resamples; for the paper we use a much larger sample.
sample_krz <- function(M, A, B, n=200, .rsvd=F) {
  a.wl <- c(A)
  b.wl <- c(B)
  ss.k <- length(a.wl)
  # word.index <- rownames(M)  # if needed!

  # compute subspace-specific measures
  krz.mf <- krz(M, a.wl, b.wl, ss.k)  # Krzanowski measure wrt A, B
  cos.mf <- ss.cos(M, a.wl, b.wl)  # cosine matrix wrt A, B
  
  # compute null K distributions
  rss <- rand.ss(M, n, ss.k)
  krss <- lapply(rss, function(x) krz(M, x[1:ss.k], x[(ss.k+1):(ss.k*2)], ss.k, .rsvd=.rsvd))  # S1, S2 totally random
  krss.a <- lapply(rss, function(x) krz(M, x[1:ss.k], a.wl, ss.k, .rsvd=.rsvd))  # wrt A, S1
  krss.b <- lapply(rss, function(x) krz(M, x[1:ss.k], b.wl, ss.k, .rsvd=.rsvd))  # wrt B, S1

  # compute null cosine distribution
  css <- lapply(rss, function(x) ss.cos(M, x[1:ss.k], x[(ss.k+1):(ss.k*2)]))
  css.a <- lapply(rss, function(x) ss.cos(M, x[1:ss.k], a.wl))
  css.b <- lapply(rss, function(x) ss.cos(M, x[1:ss.k], b.wl))
  
  # return measured object
  return(list(krz.m=krz.mf,
              cos.m=cos.mf,
              krss=do.call(rbind, krss),
              krss.a=do.call(rbind, krss.a),
              krss.b=do.call(rbind, krss.b),
              css=do.call(rbind, css),
              css.a=do.call(rbind, css.a),
              css.b=do.call(rbind, css.b)))
}

# compute every Krzanowski trace statistic over Pow(X) x Pow(Y) at dimensionality k
# the dimensionality has to be between [2, k-1]
# we don't compare the subspaces for unequally sized subsets
power_krz <- function(M, wA, wB, d, measure=c("K", "C", "H", "F")) {
  idxr <- which(sapply(wA, length) == d)
  dl <- min(idxr)
  du <- max(idxr)
  Mk <- NA
  if(measure == "K") {
    Mk <- sapply(wA[dl:du], function(sA) sapply(wB[dl:du], function(sB) krz(M, sA, sB, d, .tstat=T)))
  } else if (measure == "C") {
    Mk <- sapply(wA[dl:du], function(sA) sapply(wB[dl:du], function(sB) mean(ss.cos(M, sA, sB))))
  } else if (measure == "H") {
    Mk <- sapply(wA[dl:du], function(sA) sapply(wB[dl:du], function(sB) hausdorff(M, sA, sB)))
  } else if (measure == "F") {
    Mk <- sapply(wA[dl:du], function(sA) sapply(wB[dl:du], function(sB) frechet(M, sA, sB)))
  }
  rownames(Mk) <- sapply(wB[dl:du], paste, collapse=", ")
  colnames(Mk) <- sapply(wA[dl:du], paste, collapse=", ")
  return(Mk)
}

# function to compute summary stats
lattice_summary <- function(C, K) {
  data.frame(mk=sapply(K, function(x) mean(c(x))),
             vk=sapply(K, function(x) var(c(x))),
             kmax=sapply(K, function(x) max(c(x))),
             kmin=sapply(K, function(x) min(c(x))),
             mc=sapply(C, function(x) mean(c(x))),
             vc=sapply(C, function(x) var(c(x))),
             cmax=sapply(C, function(x) max(c(x))),
             cmin=sapply(C, function(x) min(c(x))),
             mc2=sapply(C, function(x) mean(c(x)^2)),
             vc2=sapply(C, function(x) var(c(x)^2)),
             cmax2=sapply(C, function(x) max(c(x)^2)),
             cmin2=sapply(C, function(x) min(c(x)^2)), 
             ki=1:7,
             nski=sapply(1:7, choose, n=8))
}

# at what choice of the top-level dimension does this start breaking down at sk=3?
sk.breakdown <- function(sk, swl) {
  male_ss_ps <- lapply(as.vector(sets::set_power(swl)), as.character)
  #ms.c.lattice <- lapply(1:(sk-1), function(ki) power_krz(glove.m, male_ss_ps, male_ss_ps, ki, "C"))
  ms.c.lattice <- lapply(1:(sk-1), function(ki) power_krz(embed.m, male_ss_ps, male_ss_ps, ki, "C"))
  expected.max <- which(diag(1, dim(ms.c.lattice[[3]])) == 1)
  observed.max <- which(ms.c.lattice[[3]] == matrixStats::colMaxs(ms.c.lattice[[3]]))
  return(expected.max == observed.max)
}

# get all k-subsets of the word list
subwordlists <- function(wl, sk) {
  return(wl[which(lapply(wl, length) == sk)])
}

# compute index of inconsistency over all the k-subsets
breakdown <- function(wl, sk) {
  wl_sks <- subwordlists(wl, sk)
  wl_sks_breakdown <- lapply(wl_sks, function(swl) sk.breakdown(sk, swl))
  return(wl_sks_breakdown)
}

# index of inconsistency in list wl at dimensionality k
# basically how much breakdown is there at m=3
# this takes a while bc we have to compute all kCq sublattices
k.inconsistency <- function(wl, k) {
  bks <- breakdown(wl, k)
  swl <- subwordlists(wl, k)
  bk.icon.idx <- rowSums(do.call(rbind, bks))
  #bk.icon.cos <- lapply(swl, function(L) ss.cos(glove.m, L, L))
  bk.icon.cos <- lapply(swl, function(L) ss.cos(embed.m, L, L))
  bk.icon.cm <- sapply(bk.icon.cos, function(C) mean(C))
  bk.icon.cv <- sapply(bk.icon.cos, function(C) var(c(C)))
  bk.svd <- lapply(bk.icon.cos, function(C) svd(C, nu=0, nv=0)$d)
  bk.icon.cond <- sapply(bk.svd, function(Cd) Cd[1]/Cd[length(Cd)])  # this works well if you take the geomean w. variance.
  bk.icon.condn <- sapply(bk.svd, function(Cd) norm(Cd, "2"))  # variance of singular values
  ss.which <- sapply(swl, function(S) paste(S, collapse=", "))
  bk.inconsistency <- data.frame(icon=bk.icon.idx,
                                 cm=bk.icon.cm,
                                 cv=bk.icon.cv,
                                 cond=bk.icon.cond,
                                 condn=bk.icon.condn,
                                 ss=ss.which)
  return(bk.inconsistency)
}

## Plotting functions ##

# helper for making the lattice heatmaps
ggheatmap2 <- function(X, k, title, stat_title, self=F) {
  Xa <- do.call(rbind, lapply(X, reshape2::melt))
  Xa$ik <- rep(1:k, sapply(X, function(x) nrow(x)^2))
  p <- NA
  if(self) {
    Xa %>%
      group_by(ik, Var1) %>% mutate(this.max = value == max(value)) %>% ungroup() %>%
      ggplot(aes(x=Var1, y=Var2, fill=value)) + 
      geom_tile() + 
      geom_point(aes(color=this.max), shape=15) +
      scale_fill_viridis_c(labels=function(l) sprintf("%.1f", l)) + 
      scale_color_manual(values=c(NA, "tomato"), ) +
      labs(x="", y="", fill=stat_title, title=title) +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.x = element_blank(),
            axis.ticks.y = element_blank(),
            strip.text = element_blank(),
            title = element_text(size=14),
            legend.position="bottom",
            legend.key.width=unit(1, "cm"),
            plot.margin=margin(0, 0, 0, 0, "cm")) +
      guides(color=F,
             fill = guide_colorbar(title.position="top",
                                   title.theme = element_text(size=12),
                                   label.theme = element_text(size=11))) +
      facet_wrap(~ik, ncol=1, scales="free") ->
      p
  } else {
    # no diagonality indicator
    Xa %>%
      group_by(ik, Var1) %>% mutate(this.max = value == max(value)) %>% ungroup() %>%
      ggplot(aes(x=Var1, y=Var2, fill=value)) + 
      geom_tile() + 
      scale_fill_viridis_c(labels=function(l) sprintf("%.1f", l)) + 
      labs(x="", y="", fill=stat_title, title=title) +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.x = element_blank(),
            axis.ticks.y = element_blank(),
            strip.text = element_blank(),
            title = element_text(size=14),
            legend.position="bottom",
            legend.key.width=unit(1, "cm"),
            plot.margin=margin(0, 0, 0, 0, "cm")) +
      guides(color=F,
             fill = guide_colorbar(title.position="top",
                                   title.theme = element_text(size=12),
                                   label.theme = element_text(size=11))) +
      facet_wrap(~ik, ncol=1, scales="free") ->
      p
  }
  p
}

# arranges lattice heatmaps
# gglatticemap2 <- function(K, C, title, dk, self=F) {
#   ggarrange(ggheatmap2(K, dk, self) + ggtitle(label=paste0("Canonical subspace metric: ", title)),
#             ggheatmap2(C, dk, self) + ggtitle(label=paste0("MCS metric: ", title)), nrow=1)
# }
gglatticemap3 <- function(m1, m2, m3, m4, titles, stat_title, dk, self=F, legend=T) {
  ggstatlabel(stat_title) + 
  (ggarrange(ggheatmap2(m1, dk, titles[1], stat_title, self),
            ggheatmap2(m2, dk, titles[2], stat_title, self),
            ggheatmap2(m3, dk, titles[3], stat_title, self),
            ggheatmap2(m4, dk, titles[4], stat_title, self),
            nrow=1, common.legend=legend, legend="bottom")) +
  plot_layout(ncol=1, heights=c(7, 80))
}

ggstatlabel <- function(label) {
  p <- ggplot(data.frame(l = label, x = 1, y = 1)) +
       geom_text(aes(x, y, label = l), size=14) + 
       theme_void() +
       coord_cartesian(clip = "off")
  return(p)
}

# plots the inconsistency index against condition number
gginconsistency <- function(Mi) {
  Mi %>% 
    ggplot(aes(y=icon, x=cond, color=cv,)) +
    geom_smooth(method="lm", linetype="dashed", color="black", size=1) +
    geom_jitter(width=0, height=0.2, size=3) +
    scale_color_viridis_c(option="plasma") +
    labs(x="Condition number", y="Index of inconsistency", color="Variance in \nunidimensional cosines") +
    theme(legend.position="right",
          text=element_text(size=16),
          aspect.ratio=1) +
    facet_wrap(~variate, nrow=4, ncol=1)
}

gginconsummary <- function(Mi) {
  Mi %>% ggplot(aes(x=ki)) + 
    # means
    geom_point(aes(y=mk/ki), color="tomato") + geom_line(aes(y=mk/ki), color="tomato") + 
    geom_point(aes(y=mc2), color="dodgerblue") + geom_line(aes(y=mc2), color="dodgerblue") +
    
    # ranges
    geom_point(aes(y=kmin/ki), color="tomato", shape=15) + 
    geom_line(aes(y=kmin/ki), color="tomato", linetype="dashed") +
    geom_point(aes(y=cmin2), color="dodgerblue", shape=15) + 
    geom_line(aes(y=cmin2), color="dodgerblue", linetype="dashed") +
    geom_point(aes(y=kmax/ki), color="tomato", shape=15) + 
    geom_line(aes(y=kmax/ki), color="tomato", linetype="dashed") + 
    geom_point(aes(y=cmax2), color="dodgerblue", shape=15) + 
    geom_line(aes(y=cmax2), color="dodgerblue", linetype="dashed") + 
    scale_x_continuous(breaks=1:7) +
    labs(x="Dimensionality", y="Metric (red = canonical subspace; blue = mean squared cosine similarity)") + 
    facet_wrap(~variate, ncol=1, scales="free_y")
}

# Get the top n words for column cidx
topwords <- function(M, cidx, n) {
  words <- M %>% slice_max(order_by=across(all_of(cidx)), n=n) %$% rn
  return(c(words))
}

# plot observed angles against the distribution of random co-orientations sharing 0 or 1 subspace
# vertical bars indicate 95% prediction interval; note asymmetry for point estimates near extremes.
# purple and pink are WEAT0 and WEAT1; tomato is Krzanowki's measure
# green errorbars are the two-way null; blue and yellow errorbars are the one-way nulls
krz_plot <- function(krzx, title, plot.cos=F) {
  krss.r <- krzx$krss
  krss.r.a <- krzx$krss.a
  krss.r.b <- krzx$krss.b
  css.r <- krzx$css
  css.r.a <- krzx$css.a
  css.r.b <- krzx$css.b
  krz.m <- krzx$krz.m
  cos.m <- krzx$cos.m
  ss.k <- nrow(cos.m) 
  
  # deprecated: label congruences by their most associated words
  #kwords <- sapply(1:ncol(kwords), function(cl) paste0(paste0(kwords[1:4,cl], collapse=", "), "\n", paste0(kwords[5:8,cl], collapse=", ")))
  
  if(plot.cos) {
    data.frame(xa=1,
               xb=1-0.05,
               xc=1+0.05,
               y1=mean(cos.m),
               y1x=mean(diag(cos.m)),
               y2a=mean(css.r), 
               ylla=sort(css.r)[0.05*length(css.r)], 
               yula=sort(css.r)[0.95*length(css.r)],
               y2b=mean(css.r.a), 
               yllb=sort(css.r.a)[0.05*length(css.r.a)], 
               yulb=sort(css.r.a)[0.95*length(css.r.a)],
               y2c=mean(css.r.b), 
               yllc=sort(css.r.b)[0.05*length(css.r.b)], 
               yulc=sort(css.r.b)[0.95*length(css.r.b)]) %>%
      ggplot(aes(x)) + 
      geom_point(aes(x=xa, y=y2a), color="forestgreen", size=2, shape=15) + 
      geom_errorbar(aes(x=xa, y=y2a, ymin=ylla, ymax=yula), color="forestgreen", width=0.01) + 
      geom_point(aes(x=xb, y=y2b), color="goldenrod3", size=2, shape=15) + 
      geom_errorbar(aes(x=xb, y=y2b, ymin=yllb, ymax=yulb), color="goldenrod3", width=0.01) + 
      geom_point(aes(x=xc, y=y2c), color="dodgerblue3", size=2, shape=15) + 
      geom_errorbar(aes(x=xc, y=y2c, ymin=yllc, ymax=yulc), color="dodgerblue3", width=0.01) + 
      geom_hline(aes(yintercept=y1), color="magenta", linetype="dotted") +
      geom_hline(aes(yintercept=y1x), color="magenta", linetype="dotdash") +
      ylim(-1, 1) + scale_x_continuous(breaks=NULL) +
      labs(x="", y="Cosine similarity")
  } else {
    data.frame(xa=1:ss.k,
               xb=(1:ss.k)-0.15,
               xc=(1:ss.k)+0.15,
               y1=krz.m, 
               y2a=colMeans(krss.r), 
               ylla=apply(krss.r, 2, function(x) sort(x)[0.05*nrow(krss.r)]), 
               yula=apply(krss.r, 2, function(x) sort(x)[0.95*nrow(krss.r)]),
               y2b=colMeans(krss.r.a), 
               yllb=apply(krss.r.a, 2, function(x) sort(x)[0.05*nrow(krss.r.a)]), 
               yulb=apply(krss.r.a, 2, function(x) sort(x)[0.95*nrow(krss.r.a)]),
               y2c=colMeans(krss.r.b), 
               yllc=apply(krss.r.b, 2, function(x) sort(x)[0.05*nrow(krss.r.b)]), 
               yulc=apply(krss.r.b, 2, function(x) sort(x)[0.95*nrow(krss.r.b)])) %>% 
      ggplot(aes(x)) + 
      geom_point(aes(x=xa, y=y2a), color="forestgreen", size=2, shape=15) + 
      geom_errorbar(aes(x=xa, y=y2a, ymin=ylla, ymax=yula), color="forestgreen", width=0.125) + 
      geom_point(aes(x=xb, y=y2b), color="goldenrod3", size=2, shape=15) + 
      geom_errorbar(aes(x=xb, y=y2b, ymin=yllb, ymax=yulb), color="goldenrod3", width=0.125) + 
      geom_point(aes(x=xc, y=y2c), color="dodgerblue3", size=2, shape=15) + 
      geom_errorbar(aes(x=xc, y=y2c, ymin=yllc, ymax=yulc), color="dodgerblue3", width=0.125) + 
      geom_point(aes(x=xa, y=y1), color="tomato", size=3, shape=18, alpha=0.9) + 
      geom_hline(yintercept=mean(krz.m), linetype="dashed", color="tomato") +
      geom_hline(yintercept=mean(c(cos.m)), linetype="dotted", color="magenta") +
      ylim(0, 1) +
      scale_x_continuous(breaks=seq(1,ss.k)) + #,
                         #labels=kwords) +
      theme(axis.text.x=element_text(size=11),
            axis.text.y=element_text(size=11)) +
      labs(x="", y="", title=title)
  }
}
```

```{r subspace_lattices}
# this chunk does all the heavy computing
# by default I turn this off and load the precomputed ones in the next chunk
# set eval=T to rerun this if you want (takes maybe 12 hours?); remember to turn off the next one
# the one you might care to rerun is the null distributions; the combinatorial ones are deterministic.

# it's useful to have the Euclidean norms laying around
embed.m.n <- apply(embed.m, 1, norm, "2")

# get all subsets with cardinality in [1, k-1]
# load(here("data", "gender_sentiment_powersets.rds"))
male_a_ps <- lapply(as.vector(sets::set_power(male_a))[1:(2^ss.k - 1)], as.character)
female_a_ps <- lapply(as.vector(sets::set_power(female_a))[1:(2^ss.k - 1)], as.character)
pleasant_b_ps <- lapply(as.vector(sets::set_power(pleasant_b))[1:(2^ss.k - 1)], as.character)
unpleasant_b_ps <- lapply(as.vector(sets::set_power(unpleasant_b))[1:(2^ss.k - 1)], as.character)

# then we can see how the measure distributes over the lattice of subspace permutations
# load(here("data", "gender_sentiment_sublattices.rds"))
g1.ac.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, pleasant_b_ps, ki, "K"))
g1.ac.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, pleasant_b_ps, ki, "C"))
g2.ac.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, unpleasant_b_ps, ki, "K"))
g2.ac.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, unpleasant_b_ps, ki, "C"))
g3.ac.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, pleasant_b_ps, ki, "K"))
g3.ac.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, pleasant_b_ps, ki, "C"))
g4.ac.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, unpleasant_b_ps, ki, "K"))
g4.ac.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, unpleasant_b_ps, ki, "C"))

# across summary statistics
# load(here("data", "gender_sentiment_sublattice_summaries.rds"))
g1.lattice.summarystats <- lattice_summary(g1.ac.c.lattice, g1.ac.k.lattice)
g2.lattice.summarystats <- lattice_summary(g2.ac.c.lattice, g2.ac.k.lattice)
g3.lattice.summarystats <- lattice_summary(g3.ac.c.lattice, g3.ac.k.lattice)
g4.lattice.summarystats <- lattice_summary(g4.ac.c.lattice, g4.ac.k.lattice)

# the self-similarity lattice is also really interesting
# really *this* is what should grow; the non-self product growing is a consequence of this I think?
# load(here("data", "gender_sentiment_self_lattices.rds"))
gm.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, male_a_ps, ki, "K"))
gm.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, male_a_ps, male_a_ps, ki, "C"))
gp.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, pleasant_b_ps, pleasant_b_ps, ki, "K"))
gp.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, pleasant_b_ps, pleasant_b_ps, ki, "C"))
gf.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, female_a_ps, ki, "K"))
gf.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, female_a_ps, female_a_ps, ki, "C"))
gu.k.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, unpleasant_b_ps, unpleasant_b_ps, ki, "K"))
gu.c.lattice <- lapply(1:(ss.k-1), function(ki) power_krz(embed.m, unpleasant_b_ps, unpleasant_b_ps, ki, "C"))

# index of inconsistency for all 4-subsets in lists of size 8
# we are looking at the 3-subsets of the 4-subsets of the keyword list
# i.e. this is the q-sublattices of the k-lattice.
# load(here("data", "gender_sentiment_inconsistency_indices.rds"))
male_sks_breakdown_4 <- k.inconsistency(male_a_ps, 4)  # 17/70 are inconsistent
female_sks_breakdown_4 <- k.inconsistency(female_a_ps, 4)  # 27/70 inconsistent
pleasant_sks_breakdown_4 <- k.inconsistency(pleasant_b_ps, 4)  # 1/70 inconsistent (wow!)
unpleasant_sks_breakdown_4 <- k.inconsistency(unpleasant_b_ps, 4)  # 13/70

# null distributions for each comparison
# load(here("data", "gender_sentiment_null_distributions.rds"))
n_samples <- 1000
g.ac <- sample_krz(embed.m, male_a, pleasant_b, n_samples)
g.ad <- sample_krz(embed.m, male_a, unpleasant_b, n_samples)
g.bc <- sample_krz(embed.m, female_a, pleasant_b, n_samples)
g.bd <- sample_krz(embed.m, female_a, unpleasant_b, n_samples)

# a good way of interpreting this is how predictable all of the other words are in the A|B and B|A subspaces
# you can perhaps think of this as words you should have added to the keyword list
mp.kd <- krz(embed.m, male_a, pleasant_b, 8, .svd=T)
mp.m.align <- sapply(1:ss.k, function(i) sapply(1:nrow(embed.m), function(j) lsa::cosine(mp.kd$u[,i], embed.m[j,])))
mp.p.align <- sapply(1:ss.k, function(i) sapply(1:nrow(embed.m), function(j) lsa::cosine(mp.kd$v[,i], embed.m[j,])))
mp.m.align.n <- apply(mp.m.align, 1, norm, "2")
mp.p.align.n <- apply(mp.p.align, 1, norm, "2")
mp.explain <- matrix(c(mp.m.align.n, mp.p.align.n, embed.m.n), ncol=3)
rownames(mp.explain) <- rownames(embed.m)

# you can verify that the disjoint subspaces are non-intersecting using the Zassenhaus algorithm
# the upper left k*2 x k*2 block of this matrix will have the identity matrix
# i.e. there is no basis for an intersection subspace because it does not exist
# pracma::rref(rbind(cbind(embed.m[male_a,], embed.m[male_a,]), cbind(embed.m[pleasant_b,], matrix(rep(0, 8*300), ncol=300, nrow=8))))

# save to disk, e.g. for w2v:
# save(male_a_ps, female_a_ps, pleasant_b_ps, unpleasant_b_ps, file=here("data", "word2vec_gender_sentiment_powersets.rds"))
# save(g1.ac.k.lattice, g2.ac.k.lattice, g3.ac.k.lattice, g4.ac.k.lattice, g1.ac.c.lattice, g2.ac.c.lattice, g3.ac.c.lattice, g4.ac.c.lattice, file=here("data", "word2vec_gender_sentiment_sublattices.rds"))
# save(g1.lattice.summarystats, g2.lattice.summarystats, g3.lattice.summarystats, g4.lattice.summarystats, file=here("data", "word2vec_gender_sentiment_sublattice_summaries.rds"))
# save(gm.k.lattice, gf.k.lattice, gp.k.lattice, gu.k.lattice, gm.c.lattice, gf.c.lattice, gp.c.lattice, gu.c.lattice, file=here("data", "word2vec_gender_sentiment_self_lattices.rds"))
# save(male_sks_breakdown_4, female_sks_breakdown_4, pleasant_sks_breakdown_4, unpleasant_sks_breakdown_4, file=here("data", "word2vec_gender_sentiment_inconsistency_indices.rds"))
# save(g.ac, g.ad, g.bc, g.bd, file=here("data", "word2vec_gender_sentiment_null_distributions.rds"))
```

```{r load_subspace_data}
# load precomputed data objects
load(here("data", "gender_sentiment_powersets.rds"))
load(here("data", "gender_sentiment_sublattices.rds"))
load(here("data", "gender_sentiment_sublattice_summaries.rds"))
load(here("data", "gender_sentiment_self_lattices.rds"))
load(here("data", "gender_sentiment_inconsistency_indices.rds"))
load(here("data", "gender_sentiment_null_distributions.rds"))
```

```{r plot_selfspace_lattices}
inconsistencies <- rbind(cbind(male_sks_breakdown_4, variate="male"), cbind(female_sks_breakdown_4, variate="female"),
                         cbind(pleasant_sks_breakdown_4, variate="pleasant"), cbind(unpleasant_sks_breakdown_4, variate="unpleasant"))

ggarrange(gglatticemap3(gm.k.lattice, gf.k.lattice, gp.k.lattice, gu.k.lattice,
                        c("male", "female", "pleasant", "unpleasant"), "Canonical subspace metric", 7, self=T),
          gglatticemap3(gm.c.lattice, gf.c.lattice, gp.c.lattice, gu.c.lattice,
                        c("male", "female", "pleasant", "unpleasant"), "Mean cosine similarity metric", 7, self=T)) +
    (ggplot() + theme_void()) +
    gginconsistency(inconsistencies) +
    plot_layout(ncol=3, widths=c(64, 1, 8))
```

![ ](/Users/akindel/code/cosine/figures/Fig1.png)

\pagebreak

```{r plot_subspace_lattices}
# observing the entire power set lattice makes it easy to see where the common subspace is
# we also see that the mean cosine similarity is invariant to the size of the comparison
summary_dists <- rbind(cbind(g1.lattice.summarystats, variate="male, pleasant"),
                       cbind(g3.lattice.summarystats, variate="female, pleasant"),
                       cbind(g2.lattice.summarystats, variate="male, unpleasant"),
                       cbind(g4.lattice.summarystats, variate="female, unpleasant"))

ggarrange(gglatticemap3(g1.ac.k.lattice, g3.ac.k.lattice, g2.ac.k.lattice, g4.ac.k.lattice,
                        c("male, pleasant", "female, pleasant", "male, unpleasant", "female, unpleasant"), "Canonical subspace metric", 7, legend=F),
          gglatticemap3(g1.ac.c.lattice, g3.ac.c.lattice, g2.ac.c.lattice, g4.ac.c.lattice,
                        c("male, pleasant", "female, pleasant", "male, unpleasant", "female, unpleasant"), "Mean cosine similarity metric", 7, legend=F),
          ncol=2) +
    (ggplot() + theme_void()) +
    gginconsummary(summary_dists) +
    plot_layout(ncol=3, widths=c(64, 1, 8))
```

![](/Users/akindel/code/cosine/figures/Fig2.png)

\pagebreak

```{r plot_canonical_congruences}
p1.ag <- krz_plot(g.ac, "male, pleasant") + coord_flip()
p1.bg <- krz_plot(g.ad, "male, unpleasant") + coord_flip()
p1.cg <- krz_plot(g.bc, "female, pleasant") + coord_flip()
p1.dg <- krz_plot(g.bd, "female, unpleasant") + coord_flip()

yl <- ggplot(data.frame(l = "Index", x = 1, y = 1)) +
      geom_text(aes(x, y, label = l), size=14, angle = 90) + 
      theme_void() +
      coord_cartesian(clip = "off")
xl <- ggplot(data.frame(l = "Canonical congruence", x = 1, y = 1)) +
      geom_text(aes(x, y, label = l), size=14) + 
      theme_void() +
      coord_cartesian(clip = "off")

# yl + ((p1.ag) / (p1.bg) / (p1.cg) / (p1.dg) / xl + plot_layout(heights=c(10, 10, 10, 10, 1))) + plot_layout(widths=c(4, 50))  # horizontal layout
yl + (((p1.ag) + (p1.bg)) / ((p1.cg) + (p1.dg)) / xl + plot_layout(heights=c(20, 20, 3))) + plot_layout(widths=c(4, 50))
```

![](/Users/akindel/code/cosine/figures/Fig3.png)
\pagebreak

## Tables

### Captions

#### Table 1.

Reanalysis of WEAT tests comparing MCS metric to canonical subspace metric.

```{r weat_reanalysis}
# Additional keyword lists
flowers <- str_split("aster, clover, hyacinth, marigold, poppy, azalea, crocus, iris, orchid, rose, bluebell, daffodil, lilac, pansy, tulip, buttercup, daisy, lily, peony, violet, carnation, gladiolus, magnolia, petunia, zinnia", ", ", Inf, T)  # "gladiola" -> "gladiolus"
insects <- str_split("ant, caterpillar, flea, locust, spider, bedbug, centipede, fly, maggot, tarantula, bee, cockroach, gnat, mosquito, termite, beetle, cricket, hornet, moth, wasp, blackfly, dragonfly, horsefly, roach, weevil", ", ", Inf, T) 
pleasant_a <- str_split("caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation", ", ", Inf, T)
unpleasant_a <- str_split("abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten, vomit, agony, prison", ", ", Inf, T)
instruments <- str_split("bagpipe, cello, guitar, lute, trombone, banjo, clarinet, harmonica, mandolin, trumpet, bassoon, drum, harp, oboe, tuba, bell, fiddle, harpsichord, piano, viola, bongo, flute, horn, saxophone, violin", ", ", Inf, T)
weapons <- str_split("arrow, club, gun, missile, spear, ax, dagger, harpoon, pistol, sword, blade, dynamite, hatchet, rifle, tank, bomb, firearm, knife, shotgun, teargas, cannon, grenade, mace, slingshot, whip", ", ", Inf, T)  # "axe" -> "ax"
career <- str_split("executive, management, professional, corporation, salary, office, business, career", ", ", Inf, T)
family <- str_split("home, parents, children, family, cousins, marriage, wedding, relatives", ", ", Inf, T)
temporary <- str_split("impermanent, unstable, variable, fleeting, short-term, brief, occasional", ", ", Inf, T)
permanent <- str_split("stable, always, constant, persistent, chronic, prolonged, forever", ", ", Inf, T)
math <- str_split("math, algebra, geometry, calculus, equations, computation, numbers, addition", ", ", Inf, T)
arts <- str_split("poetry, art, dance, literature, novel, symphony, drama, sculpture", ", ", Inf, T)
science <- str_split("science, technology, physics, chemistry, einstein, nasa, experiment, astronomy", ", ", Inf, T)  # lowercased "Einstein" and "NASA" (?)
arts2 <- str_split("poetry, art, shakespeare, dance, literature, novel, symphony, drama", ", ", Inf, T)  # lowercased "Shakespeare"
male_b <- str_split("brother, father, uncle, grandfather, son, he, his, him", ", ", Inf, T)
male_c <- str_split("men, man, boy, boys, he, him, his, himself", ", ", Inf, T)  # Nelson 2021
female_b <- str_split("sister, mother, aunt, grandmother, daughter, she, hers, her", ", ", Inf, T)
female_c <- str_split("women, woman, girl, girls, she, her, hers, herself", ", ", Inf, T)  # Nelson 2021
mental_illness <- str_split("sad, hopeless, gloomy, tearful, miserable, depressed", ", ", Inf, T)
physical_illness <- str_split("sick, illness, influenza, disease, virus, cancer", ", ", Inf, T)
male_names <- str_split(tolower("John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill"), ", ", Inf, T)  # all lowercased ("john"?)
female_names <- str_split(tolower("Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna"), ", ", Inf, T)  # all lowercased
white_names_a <- str_split(tolower("Brad, Brendan, Geoffrey, Greg, Brett, Jay, Matthew, Neil, Todd, Allison, Anne, Carrie, Emily, Jill, Kristen, Meredith, Sarah"), ", ", Inf, T)  # deleted "Laurie"
black_names_a <- str_split(tolower("Darnell, Hakim, Jermaine, Kareem, Jamal, Leroy, Rasheed, Tremayne, Tyrone, Aisha, Ebony, Keisha, Kenya, Latonya, Latoya, Tamika, Tanisha"), ", ", Inf, T)  # deleted "Lakisha"
young_names <- str_split(tolower("Tiffany, Michelle, Cindy, Kristy, Brad, Eric, Joey, Billy"), ", ", Inf, T)  # The female names here seem pretty outdated to me.
old_names <- str_split(tolower("Ethel, Bernice, Gertrude, Agnes, Cecil, Wilbert, Mortimer, Edgar"), ", ", Inf, T)

# dropped Bobbie-Sue and Sue-Ellen (no support)
# dropped Chip, Jed, Crystal, Amber, Peggy, Wendy
# to match below, needed to drop 2 more male and 4 more female names; CBN dropped these too
# also note that dropping Amber and Crystal helps with the polysemy issue!
white_names_b <- str_split(tolower("Adam, Harry, Josh, Roger, Alan, Frank, Ian, Justin, Ryan, Andrew, Fred, Jack, Matthew, Stephen, Brad, Greg, Paul, Todd, Brandon, Hank, Jonathan, Peter, Wilbur, Amanda, Courtney, Heather, Melanie, Sara, Katie, Meredith, Shannon, Betsy, Donna, Kristin, Nancy, Stephanie, Ellen, Lauren, Colleen, Emily, Megan, Rachel"), ", ", Inf, T)

# changes are as follows:
# delete Percell, Everol, Lashelle, Teretha, Tameisha, Lakisha, Shavonn, Tashika
# Rasaan -> Rashaan
# Terryl -> Terrell
# Aiesha -> Aisha
# Temeka -> Tamika
# Shanise -> Shanice
# Sharise -> Sharice
# Lashandra -> Lashawn
black_names_b <- str_split(tolower("Alonzo, Jamel, Lerone, Theo, Alphonse, Jerome, Leroy, Rashaan, Torrance, Darnell, Lamar, Lionel, Rashaun, Tyree, Deion, Lamont, Malik, Terrence, Tyrone, Lavon, Marcellus, Terrell, Wardell, Aisha, Nichelle, Shereen, Tamika, Ebony, Latisha, Shaniqua, Jasmine, Latonya, Shanice, Tanisha, Tia, Latoya, Sharice, Yolanda, Lashawn, Malika, Tawanda, Yvette"), ", ", Inf, T)

# Principal angles and mean cosines for each analysis

# WEAT 1
flowers_pleasant_k <- krz(embed.m, flowers, pleasant_a, 25)
insects_pleasant_k <- krz(embed.m, insects, pleasant_a, 25)
flowers_unpleasant_k <- krz(embed.m, flowers, unpleasant_a, 25)
insects_unpleasant_k <- krz(embed.m, insects, unpleasant_a, 25)
flowers_pleasant_c <- ss.cos(embed.m, flowers, pleasant_a)
insects_pleasant_c <- ss.cos(embed.m, insects, pleasant_a)
flowers_unpleasant_c <- ss.cos(embed.m, flowers, unpleasant_a)
insects_unpleasant_c <- ss.cos(embed.m, insects, unpleasant_a)
weat1 <- c(mean(flowers_pleasant_c), mean(flowers_unpleasant_c), mean(insects_unpleasant_c), mean(insects_pleasant_c))
canon1 <- c(mean(flowers_pleasant_k^2), mean(flowers_unpleasant_k^2), mean(insects_unpleasant_k^2), mean(insects_pleasant_k^2))

# WEAT 2
instruments_pleasant_k <- krz(embed.m, instruments, pleasant_a, 25)
weapons_pleasant_k <- krz(embed.m, weapons, pleasant_a, 25)
instruments_unpleasant_k <- krz(embed.m, instruments, unpleasant_a, 25)
weapons_unpleasant_k <- krz(embed.m, weapons, unpleasant_a, 25)
instruments_pleasant_c <- ss.cos(embed.m, instruments, pleasant_a)
weapons_pleasant_c <- ss.cos(embed.m, weapons, pleasant_a)
instruments_unpleasant_c <- ss.cos(embed.m, instruments, unpleasant_a)
weapons_unpleasant_c <- ss.cos(embed.m, weapons, unpleasant_a)
weat2 <- c(mean(instruments_pleasant_c), mean(instruments_unpleasant_c), mean(weapons_unpleasant_c), mean(weapons_pleasant_c))
canon2 <- c(mean(instruments_pleasant_k^2), mean(instruments_unpleasant_k^2), mean(weapons_unpleasant_k^2), mean(weapons_pleasant_k^2))

# WEAT 3
# see above -- this one is quite troubled...
white_names_b_pleasant_k <- krz(embed.m, white_names_b, pleasant_a, 25)
black_names_b_pleasant_k <- krz(embed.m, black_names_b, pleasant_a, 25)
white_names_b_unpleasant_k <- krz(embed.m, white_names_b, unpleasant_a, 25)
black_names_b_unpleasant_k <- krz(embed.m, black_names_b, unpleasant_a, 25)
white_names_b_pleasant_c <- ss.cos(embed.m, white_names_b, pleasant_a)
black_names_b_pleasant_c <- ss.cos(embed.m, black_names_b, pleasant_a)
white_names_b_unpleasant_c <- ss.cos(embed.m, white_names_b, unpleasant_a)
black_names_b_unpleasant_c <- ss.cos(embed.m, black_names_b, unpleasant_a)
weat3 <- c(mean(white_names_b_pleasant_c), mean(white_names_b_unpleasant_c), mean(black_names_b_unpleasant_c), mean(black_names_b_pleasant_c))
canon3 <- c(mean(white_names_b_pleasant_k^2), mean(white_names_b_unpleasant_k^2), mean(black_names_b_unpleasant_k^2), mean(black_names_b_pleasant_k^2))

# WEAT 4
white_names_a_pleasant_a_k <- krz(embed.m, white_names_a, pleasant_a, 17)
black_names_a_pleasant_a_k <- krz(embed.m, black_names_a, pleasant_a, 17)
white_names_a_unpleasant_a_k <- krz(embed.m, white_names_a, unpleasant_a, 17)
black_names_a_unpleasant_a_k <- krz(embed.m, black_names_a, unpleasant_a, 17)
white_names_a_pleasant_a_c <- ss.cos(embed.m, white_names_a, pleasant_a)
black_names_a_pleasant_a_c <- ss.cos(embed.m, black_names_a, pleasant_a)
white_names_a_unpleasant_a_c <- ss.cos(embed.m, white_names_a, unpleasant_a)
black_names_a_unpleasant_a_c <- ss.cos(embed.m, black_names_a, unpleasant_a)
weat4 <- c(mean(white_names_a_pleasant_a_c), mean(white_names_a_unpleasant_a_c), mean(black_names_a_unpleasant_a_c), mean(black_names_a_pleasant_a_c))
canon4 <- c(mean(white_names_a_pleasant_a_k^2), mean(white_names_a_unpleasant_a_k^2), mean(black_names_a_unpleasant_a_k^2), mean(black_names_a_pleasant_a_k^2))

# WEAT 5
white_names_a_pleasant_b_k <- krz(embed.m, white_names_a, pleasant_b, 8)
black_names_a_pleasant_b_k <- krz(embed.m, black_names_a, pleasant_b, 8)
white_names_a_unpleasant_b_k <- krz(embed.m, white_names_a, unpleasant_b, 8)
black_names_a_unpleasant_b_k <- krz(embed.m, black_names_a, unpleasant_b, 8)
white_names_a_pleasant_b_c <- ss.cos(embed.m, white_names_a, pleasant_b)
black_names_a_pleasant_b_c <- ss.cos(embed.m, black_names_a, pleasant_b)
white_names_a_unpleasant_b_c <- ss.cos(embed.m, white_names_a, unpleasant_b)
black_names_a_unpleasant_b_c <- ss.cos(embed.m, black_names_a, unpleasant_b)
weat5 <- c(mean(white_names_a_pleasant_b_c), mean(white_names_a_unpleasant_b_c), mean(black_names_a_unpleasant_b_c), mean(black_names_a_pleasant_b_c))
canon5 <- c(mean(white_names_a_pleasant_b_k^2), mean(white_names_a_unpleasant_b_k^2), mean(black_names_a_unpleasant_b_k^2), mean(black_names_a_pleasant_b_k^2))

# WEAT 6
male_names_career_k <- krz(embed.m, male_names, career, 8)
female_names_career_k <- krz(embed.m, female_names, career, 8)
male_names_family_k <- krz(embed.m, male_names, family, 8)
female_names_family_k <- krz(embed.m, female_names, family, 8)
male_names_career_c <- ss.cos(embed.m, male_names, career)
female_names_career_c <- ss.cos(embed.m, female_names, career)
male_names_family_c <- ss.cos(embed.m, male_names, family)
female_names_family_c <- ss.cos(embed.m, female_names, family)
weat6 <- c(mean(male_names_career_c), mean(male_names_family_c), mean(female_names_family_c), mean(female_names_career_c))
canon6 <- c(mean(male_names_career_k^2), mean(male_names_family_k^2), mean(female_names_family_k^2), mean(female_names_career_k^2))

# WEAT 7
math_male_k <- krz(embed.m, math, male_a, 8)
arts_male_k <- krz(embed.m, arts, male_a, 8)
math_female_k <- krz(embed.m, math, female_a, 8)
arts_female_k <- krz(embed.m, arts, female_a, 8)
math_male_c <- ss.cos(embed.m, math, male_a)
arts_male_c <- ss.cos(embed.m, arts, male_a)
math_female_c <- ss.cos(embed.m, math, female_a)
arts_female_c <- ss.cos(embed.m, arts, female_a)
weat7 <- c(mean(math_male_c), mean(math_female_c), mean(arts_female_c), mean(arts_male_c))
canon7 <- c(mean(math_male_k^2), mean(math_female_k^2), mean(arts_female_k^2), mean(arts_male_k^2))

# WEAT 8
science_male_k <- krz(embed.m, science, male_b, 8)
arts2_male_k <- krz(embed.m, arts2, male_b, 8)
science_female_k <- krz(embed.m, science, female_b, 8)
arts2_female_k <- krz(embed.m, arts2, female_b, 8)
science_male_c <- ss.cos(embed.m, science, male_b)
arts2_male_c <- ss.cos(embed.m, arts2, male_b)
science_female_c <- ss.cos(embed.m, science, female_b)
arts2_female_c <- ss.cos(embed.m, arts2, female_b)
weat8 <- c(mean(science_male_c), mean(science_female_c), mean(arts2_female_c), mean(arts2_male_c))
canon8 <- c(mean(science_male_k^2), mean(science_female_k^2), mean(arts2_female_k^2), mean(arts2_male_k^2))

# WEAT 9
mental_illness_temporary_k <- krz(embed.m, mental_illness, temporary, 7)
physical_illness_temporary_k <- krz(embed.m, physical_illness, temporary, 7)
mental_illness_permanent_k <- krz(embed.m, mental_illness, permanent, 7)
physical_illness_permanent_k <- krz(embed.m, physical_illness, permanent, 7)
mental_illness_temporary_c <- ss.cos(embed.m, mental_illness, temporary)
physical_illness_temporary_c <- ss.cos(embed.m, physical_illness, temporary)
mental_illness_permanent_c <- ss.cos(embed.m, mental_illness, permanent)
physical_illness_permanent_c <- ss.cos(embed.m, physical_illness, permanent)
weat9 <- c(mean(mental_illness_temporary_c), mean(mental_illness_permanent_c), mean(physical_illness_permanent_c), mean(physical_illness_temporary_c))
canon9 <- c(mean(mental_illness_temporary_k^2), mean(mental_illness_permanent_k^2), mean(physical_illness_permanent_k^2), mean(physical_illness_temporary_k^2))

# WEAT 10
young_names_pleasant_b_k <- krz(embed.m, young_names, pleasant_b, 8)
old_names_pleasant_b_k <- krz(embed.m, old_names, pleasant_b, 8)
young_names_unpleasant_b_k <- krz(embed.m, young_names, unpleasant_b, 8)
old_names_unpleasant_b_k <- krz(embed.m, old_names, unpleasant_b, 8)
young_names_pleasant_b_c <- ss.cos(embed.m, young_names, pleasant_b)
old_names_pleasant_b_c <- ss.cos(embed.m, old_names, pleasant_b)
young_names_unpleasant_b_c <- ss.cos(embed.m, young_names, unpleasant_b)
old_names_unpleasant_b_c <- ss.cos(embed.m, old_names, unpleasant_b)
weat10 <- c(mean(young_names_pleasant_b_c), mean(young_names_unpleasant_b_c), mean(old_names_unpleasant_b_c), mean(old_names_pleasant_b_c))
canon10 <- c(mean(young_names_pleasant_b_k^2), mean(young_names_unpleasant_b_k^2), mean(old_names_unpleasant_b_k^2), mean(old_names_pleasant_b_k^2))

rank_correlations <- rbind(data.frame(x=weat1, y=canon1, xr=paste0(rank(weat1), collapse=", "),yr=paste0(rank(canon1), collapse=", "),
                                      xm=sum(weat1 * c(1,-1,1,-1)), ym=sum(canon1 * c(1,-1,1,-1)), size=25,
                                      xyc=cor(rank(weat1), rank(canon1)), xy.ed=weat1-canon1, weat=1, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: flowers,\nB: insects,\nC: pleasant,\nD: unpleasant"))),
                           data.frame(x=weat2, y=canon2, xr=paste0(rank(weat2), collapse=", "),yr=paste0(rank(canon2), collapse=", "),
                                      xm=sum(weat2 * c(1,-1,1,-1)), ym=sum(canon2 * c(1,-1,1,-1)), size=25,
                                      xyc=cor(rank(weat2), rank(canon2)), xy.ed=weat2-canon2, weat=2, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: instruments,\nB: weapons,\nC: pleasant,\nD: unpleasant"))),
                           data.frame(x=weat3, y=canon3, xr=paste0(rank(weat3), collapse=", "),yr=paste0(rank(canon3), collapse=", "),
                                      xm=sum(weat3 * c(1,-1,1,-1)), ym=sum(canon3 * c(1,-1,1,-1)), size=25,
                                      xyc=cor(rank(weat3), rank(canon3)), xy.ed=weat3-canon3, weat=3, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: white names,\nB: Black names,\nC: pleasant,\nD: unpleasant"))),
                           data.frame(x=weat4, y=canon4, xr=paste0(rank(weat4), collapse=", "),yr=paste0(rank(canon4), collapse=", "),
                                      xm=sum(weat4 * c(1,-1,1,-1)), ym=sum(canon4 * c(1,-1,1,-1)), size=17,
                                      xyc=cor(rank(weat4), rank(canon4)), xy.ed=weat4-canon4, weat=4, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: white names,\nB: Black names,\nC: pleasant,\nD: unpleasant"))),
                           data.frame(x=weat5, y=canon5, xr=paste0(rank(weat5), collapse=", "),yr=paste0(rank(canon5), collapse=", "),
                                      xm=sum(weat5 * c(1,-1,1,-1)), ym=sum(canon5 * c(1,-1,1,-1)), size=8,
                                      xyc=cor(rank(weat5), rank(canon5)), xy.ed=weat5-canon5, weat=5, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: white names,\nB: Black names,\nC: pleasant,\nD: unpleasant"))),
                           data.frame(x=weat6, y=canon6, xr=paste0(rank(weat6), collapse=", "),yr=paste0(rank(canon6), collapse=", "),
                                      xm=sum(weat6 * c(1,-1,1,-1)), ym=sum(canon6 * c(1,-1,1,-1)), size=8,
                                      xyc=cor(rank(weat6), rank(canon6)), xy.ed=weat6-canon6, weat=6, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: male names,\nB: female names,\nC: career,\nD: family"))),
                           data.frame(x=weat7, y=canon7, xr=paste0(rank(weat7), collapse=", "),yr=paste0(rank(canon7), collapse=", "),
                                      xm=sum(weat7 * c(1,-1,1,-1)), ym=sum(canon7 * c(1,-1,1,-1)), size=8,
                                      xyc=cor(rank(weat7), rank(canon7)), xy.ed=weat7-canon7, weat=7, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: math,\nB: arts,\nC: male,\nD: female"))),
                           data.frame(x=weat8, y=canon8, xr=paste0(rank(weat8), collapse=", "),yr=paste0(rank(canon8), collapse=", "),
                                      xm=sum(weat8 * c(1,-1,1,-1)), ym=sum(canon8 * c(1,-1,1,-1)), size=8,
                                      xyc=cor(rank(weat8), rank(canon8)), xy.ed=weat8-canon8, weat=8, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: science,\nB: arts,\nC: male,\nD: female"))),
                           data.frame(x=weat9, y=canon9, xr=paste0(rank(weat9), collapse=", "),yr=paste0(rank(canon9), collapse=", "),
                                      xm=sum(weat9 * c(1,-1,1,-1)), ym=sum(canon9 * c(1,-1,1,-1)), size=7,
                                      xyc=cor(rank(weat9), rank(canon9)), xy.ed=weat9-canon9, weat=9, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: mental illness,\nB: physical illness,\nC: temporary,\nD: permanent"))),
                           data.frame(x=weat10, y=canon10, xr=paste0(rank(weat10), collapse=", "),yr=paste0(rank(canon10), collapse=", "),
                                      xm=sum(weat10 * c(1,-1,1,-1)), ym=sum(canon10 * c(1,-1,1,-1)), size=8,
                                      xyc=cor(rank(weat10), rank(canon10)), xy.ed=weat10-canon10, weat=10, component=c("AC", "AD", "BD", "BC"),
                                      labels=linebreak(c("A: young names,\nB: elderly names,\nC: pleasant,\nD: unpleasant")))) %>% mutate(xy.mr=xm/ym)

rank_correlations %>%
  select(labels, size, xm, ym, xyc, xy.mr, MCS=x, CCA=y, component) %>%
  pivot_wider(names_from=component, values_from=c(MCS, CCA)) ->
  rank_corr_table

#save(rank_corr_table, file=here("data", "weat_summary_table.rds"))
```

```{r show_comparison, eval=T, results="asis"}
load(here("data", "weat_summary_table.rds"))
rownames(rank_corr_table) <- paste0("WEAT", 1:10)
cn <- c("Keyword lists", "N", "WEAT$_{MCS}$", "WEAT$_{CCA}$", "$\\rho$", "Ratio",
        "A:C", "A:D", "B:D", "B:C",
        "A:C", "A:D", "B:D", "B:C")
knitr::kable(rank_corr_table, format="latex", digits=3, col.names=cn, escape=F, booktabs=T, linesep="\\hline", align=c("lccccccccccccc")) %>%
  add_header_above(c(" " = 7, "Mean cosine similarity (MCS)" = 4, "Canonical subspace metric (CCA)" = 4)) %>%
  kable_styling(font_size=10, latex_options=c("scale_down")) %>%
  column_spec(8, color="white", background=spec_color(unlist(rank_corr_table[,7]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(9, color="white", background=spec_color(unlist(rank_corr_table[,8]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(10, color="white", background=spec_color(unlist(rank_corr_table[,9]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(11, color="white", background=spec_color(unlist(rank_corr_table[,10]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(12, color="white", background=spec_color(unlist(rank_corr_table[,11]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(13, color="white", background=spec_color(unlist(rank_corr_table[,12]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(14, color="white", background=spec_color(unlist(rank_corr_table[,13]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  column_spec(15, color="white", background=spec_color(unlist(rank_corr_table[,14]), end=0.8, scale_from=c(min(rank_corr_table[,7:14]), max(rank_corr_table[,7:14])))) %>%
  landscape() ->
  out
cat(gsub('\\bNA\\b', '  ', out), sep='\n')
```
