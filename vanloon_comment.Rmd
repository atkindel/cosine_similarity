---
title: "The anisotropic geometry of high-dimensional cosine similarity"
author: "Alexander T. Kindel^[PhD Candidate, Department of Sociology, Princeton University. Contact: akindel@princeton.edu]"
abstract: |
 |  The cosine ratio is widely used to describe patterns of association in vector space models (e.g., word embeddings like word2vec or GloVe). It has recently been observed that some measures constructed in this manner exhibit a strong correlation with relative frequency. I show that the roots of this problem lie in the use of standard basis arithmetic to group and compare sets of cosine similarities, which do not imply a common basis. Means of sets of vectors selected arbitrarily from the underlying model imply a hyperelliptical subspace that is transformable to the unit ball; however, these hyperellipsoids are generally anisotropic (i.e., they have different shapes). When computed in the standard basis, numerical comparisons between these hyperelliptical measures implicitly upweight the contribution of comparisons involving relatively uneven group sizes. I emphasize that the problem is attributable to methodology---particularly the arithmetic comparison of correlation coefficients---and not to statistical properties of vector space models or inherent qualities of underlying processes.
 | 
 | **Keywords:** cosine similarity, vector space model, frequency bias, measurement, correlation, arithmetic
date: |
  | 22 February 2022
  | 
  | 
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: TRUE
    number_sections: true
header-includes:
  - \usepackage{setspace}
  - \setlength\parindent{24pt}
  - \usepackage[bottom]{footmisc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, cache.lazy= FALSE)
```

\pagebreak

A recent paper by van Loon and colleagues (2022) demonstrates frequency bias in the widely-cited Word Embedding Association Test (WEAT; Caliskan, Bryson & Narayanan 2017). WEAT adapts the "implicit association" methodology developed in social psychology (Greenwald, McGhee & Schwartz 1998) to measure differential association in large collections of text; for example, it has been used to measure the association between distinctively racialized or gendered names and positive or negative terms on social media.

In an empirical application of WEAT to geotagged Twitter posts, van Loon and colleagues find that although the WEAT statistic at first seems to exhibit a robust linear association with many other measures of racial animus, this effect disappears when the relative rarity of Black names is added into the model. The authors conclude that "WEAT appears to be a highly sophisticated way for detecting linguistic bias, but it is in fact just a noisy proxy measuring how rare Black names are in the data." The authors infer that the geometry of word embedding models is to blame because they "conflate the relative frequency of words prototypical of the groups with positivity" due to the relatively higher frequency of positive terms in the corpus compared to negative terms.

The purpose of this comment is to describe the source of the observed frequency bias analytically. I show that frequency bias originates in the quantitative design of the *comparison* rather than in the geometry of the embedding model itself.








# Introduction

Researchers frequently use the cosine ratio as a measure of similarity over vector space models of social phenomena.^[I focus on non-contextual word embeddings such as GloVe throughout the paper. I presume readers are familiar with basic setup of assigning real-valued vectors to terms as a representation of their meaning and the locally compositional properties of this semantic model. See Arseniev-Koehler [2022] for an introduction to word embedding models in cultural sociology.] The *cosine similarity* between two $p$-dimensional vectors $A$ and $B$ is defined as their dot product divided by the product of their magnitudes:

$$
cos(\theta_{AB}) = \frac {A \cdot B}{||A||\;||B||}
$$
Cosine similarity describes the amount that the two vectors point in the same, different, or opposite direction; it is bounded from -1 (pointing in opposite directions) to 0 (pointing in different directions) to 1 (pointing in the same direction). It is also equivalent to the Pearson product-moment correlation coefficient of the two centered vectors. The computational and interpretive simplicity of this method for quantitative comparison has led to a large number of applications using it as a measure of associative meaning among terms in large text corpora.

An important issue arises when comparing cosine similarities. It is sometimes observed that cosine similarity "does not satisfy" the triangle inequality; that is, it is not a function $d(A, B)$ satisfying $\forall C: d(A, B) \leq d(A, C) + d(B, C)$. The triangle inequality is a rule about what it means to add things that are measurements of distance (length, size). Consider the aphorism "detours are never shortcuts"; in the same way that making an extra stop on a road trip does not make the trip shorter, taking the distance between $A$ and $B$ by summing their distances with $C$ cannot result in a shorter overall distance in a metric space. So what does it mean for the cosine similarity to not obey this rule? One way of thinking about it is that the cosine ratio describes the amount that one can heuristically "shortcut" the triangle inequality by factoring out local relative frequency and using the resulting unit ball projection as a navigational guide to the underlying non-normalized vector space (Arseniev-Koehler 2022). Thinking about it this way leads to the observation that because cosine similarity quantities have been scaled by their magnitudes, they only have "units" relative to the relationship between the underlying vectors; $cos(\theta_{AB}) = 0.5$, say, can only be interpreted in terms of the geometric relationship between $A$ and $B$. The sense in which cosine similarity does not satisfy this inequality is also the sense in which we have factored out the relative magnitude of the two vectors in order to focus on their angular alignment.

Cosine provides a useful way of orienting ourselves to term associations, but its intentional disregard for scale renders it only locally meaningful. The amount that cosine similarity "shortcuts" is different in different subspaces; consequently, the underlying subspaces are *anisotropic*. That is, they vary when compared in terms of their principal directions; a step in the largest direction in subspace $\mathbf{A}$ is not generally the same as a step in the largest direction in subspace $\mathbf{B}$ (or $\mathbf{X}$ or $\mathbf{Y}$). This implies a non-standard basis for arithmetic. The degree to which adding cosine similarities between anisotropic subspaces of a vector space model approximates the kind of addition we are used to is dependent on the degree to which the corresponding sets of vectors under comparison lie in bases that are relatively similar in shape. More unbalanced comparisons receive more weight in measures that add cosine similarities in this way. This can lead to unanticipated scale-dependent surprises in seemingly well-behaved applications of cosine similarity.

# Example: Cosine similarity arithmetic in the Word Embedding Association Test

As a running example, I show how this issue produces frequency bias in the Word Embedding Association Test (WEAT) developed by Caliskan, Bryson and Narayanan (2017). WEAT is a widely-used measure of "stereotypes" and "implicit human biases" in language. The authors rely on comparisons of cosine similarities to measure broad associations between group membership and relative value or sentiment in public discourse. Specifically, the proposed measure takes the difference in the mean cosine similarities across pairs of group-identifying terms (e.g. Black or white American names) and sentiment-identifying terms (e.g. positive or negative terms) specified by the user. For some positive or negative sentiment word $W$, the statistic $s(W, A, B)$ is the difference in mean cosine similarity between that word and all of the words in each of $\mathbf{A}$ and $\mathbf{B}$:

$$s(W, A, B)=\operatorname{mean}_{a \in A} \cos (\vec{w}, \vec{a})-\operatorname{mean}_{b \in B} \cos (\vec{w}, \vec{b})$$  
The full WEAT statistic takes the absolute difference of the sum of these scores over pairs of words $W$ in $\mathbf{X}$ and $\mathbf{Y}$:

$$S_{A, B, X, Y}=\sum_{x \in X} s_{x, A, B}-\sum_{y \in Y} s_{y, A, B}$$

Several recent works in applied bias measurement have observed that WEAT estimates are sensitive to the frequency distribution of terms in the underlying corpus. This issue has been extensively demonstrated for WEAT measures of anti-Black discursive sentiment by van Loon and colleagues (2022), who show that the WEAT statistic seems to exhibit a robust linear association with many other measures of racial animus until the relative rarity of Black names is added into the model. The authors conclude that "WEAT appears to be a highly sophisticated way for detecting linguistic bias, but it is in fact just a noisy proxy measuring how rare Black names are in the data." The authors infer that the geometry of word embedding models is to blame; in particular, such models are said to "conflate the relative frequency of words prototypical of the groups with positivity" due to the relatively higher frequency of positive terms in the corpus compared to negative terms.

It is true that the shape of any vector space model of language is foremost describable in terms of term frequency. This is a common feature of the principal geometry of vector space embeddings of text (Mu \& Viswanath 2018). It is also not unique to statistical text analysis or applied social science. The first principal component in biometric data is typically strongly correlated with size (see e.g. Jolicouer & Mosimann 1960 on turtles; Carpenter et al. 1978 on cows). The first principal component of chemical electron microscopy data is proportional to the number of walks on the molecular adjacency graph (Cvetković & Gutman 1977; on topological indices in analytical chemistry, see Balaban et al. 1983). The problem has more to do with the fact that *cosine similarities cannot be added* --- at least not without taking on additional mathematical assumptions. The simplest way to see this is that cosine similarity is bounded on $[-1, 1]$. What does it mean to add two such numbers together? Concretely, if we compute $0.5 + 0.6 = 1.1$, what does this mean? Say we then additionally compute $0.9 + 0.4 = 1.3$. In what sense is 1.3 "more than" 1.1?

I demonstrate over the next two sections that the scale dependence in the WEAT statistic is attributable to its use of standard basis arithmetic to construct a cosine similarity comparison. The anisotropy of the resulting comparison can be demonstrated in a finite-sample setting using any word embedding model that supports a given WEAT lexicon (section 3), and it can be described mathematically in terms of "absolute" (with respect to the origin) and "relative" (with respect to the choice of terms) angular components of the statistic (section 4).

# The fixed geometry of WEAT in GloVe

WEAT is built on top of the fixed word lists in Greenwald et al. (1998) and Bertrand and Mullainathan (2004) (see Caliskan, Bryson & Narayanan 2017; Antoniak & Mimno 2021; van Loon et al. 2022). This implies that the problem has fixed geometry conditional on the choice of embedding. Here, I describe the shape of the "WEAT 3" test comparing examples of terms with pleasant and unpleasant connotations to examples of Black/white American first names:

* **Black names**: Alonzo, Jamel, Theo, Alphonse, Jerome, Leroy, Torrance, Darnell, Lamar, Lionel, Tyree, Deion, Lamont, Malik, Terrence, Tyrone, Lavon, Marcellus, Wardell, Nichelle, Shereen, Ebony, Latisha, Shaniqua, Jasmine, Tanisha, Tia, Lakisha, Latoya, Yolanda, Malika, Yvette.
* **White names**: Adam, Harry, Josh, Roger, Alan, Frank, Justin, Ryan, Andrew, Jack, Matthew, Stephen, Brad, Greg, Paul, Jonathan, Peter, Amanda, Courtney, Heather, Melanie, Katie, Betsy, Donna, Kristin, Nancy, Stephanie, Ellen, Lauren, Colleen, Emily, Megan, Rachel.
  
* **Pleasant terms**: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation.
* **Unpleasant terms**: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, bomb, divorce, jail, poverty, ugly, cancer, evil, kill, rotten, vomit.

To describe the distinct vector subspaces implied by each of these four sets of terms and the additive comparison architecture of the WEAT statistic, we can take the singular value decomposition of the matrices of name vectors $\textbf{A}$ and $\textbf{B}$, resulting in a set of eigenvectors of rank less than or equal to the number of terms in the set. The resulting eigenvalue functions describe the overall shape of the linear subspace spanned by each word list (see Bai & Silverstein 2010). We do the same thing for the attribute vectors $\textbf{X}$ and $\textbf{Y}$.  This procedure identifies four distinct hyperelliptical subspaces with different bases for arithmetic comparisons of cosine similarities. Each of these objects resembles a unit ball that has been stretched and compressed along its various dimensions, but in a different way in each of the four term groups. Each of these hyperellipsoids describes what it means to "add" in this particular region of the vector space relative to adding in the standard basis.

To understand how these bases relate to each other, we can compare the vector of eigenvalues of each subspace, which describes its overall shape.^[Note that the length of the word lists matters. There is a loss of information involved in transforming $\textbf{A}$ and $\textbf{B}$ onto $\textbf{X}$ and $\textbf{Y}$ when they are embedded in fewer dimensions. The Krzanowski (1979) statistic suggests that the linear subspaces created by the unequal term group sizes in WEAT 3 are only about 78% similar. I set this fact aside and work with word lists of the same length for the remainder of the paper; however, because nothing about the construction of the word lists guarantees their basis is full rank, it is important to note that this only minimizes the issue and does not solve it completely.] All of the spaces are isomorphic when the underlying word lists are the same length; this can be confirmed by checking that the common space embedding trace metric developed by Krzanowski (1979) attains its upper bound (i.e. the number of terms). However, they will in general have different eigenvalues, suggesting each has a subtly different overall shape.

Figures 1 and 2 provide a schematic geometric depiction of the problem. Each linear subspace is a hyperellipsoid that is similar to the unit ball (Figure 1), but not *directly* comparable to any of the other subspaces arithmetically without explicit transformation. A useful way of describing this geometry is in terms of the change of basis needed to treat a given hyperellipsoid as if it were the unit ball (Figure 2).

\pagebreak

```{r, echo=F, out.width="\\textwidth", fig.cap="Various ways of distorting the unit ball in $\\mathbb{R}^3$."}
knitr::include_graphics("./basis_distortion.png")
```

```{r, echo=F, out.width="\\textwidth", fig.cap="Each hyperellipsoid can be transformed into the unit ball using a different change of basis."}
knitr::include_graphics("./basis_change.png")
```

```{r, echo=F, message=F, fig.height=4, fig.width=4.5, fig.align="center"}
library(tidyverse)
library(textdata)
library(gridExtra)

theme_set(theme_bw())

glove_d <- embedding_glove6b(dimensions=200)
glove <- glove_d %>% pivot_longer(contains("d"), names_to="dim")
glove_tokens <- unique(glove$token)
glove_d %<>% select(-token) %>% as.matrix()
rownames(glove_d) <- glove_tokens

# WEAT black/white names and good/bad terms
weat_white <- c("adam", "harry", "josh", "roger", "alan", "frank", "justin", "ryan", "andrew", "jack",
                "matthew", "stephen", "brad", "greg", "paul", "jonathan", "peter", "amanda", "courtney",
                "heather", "melanie", "katie", "betsy", "kristin", "nancy", "stephanie", "ellen", "lauren",
                "colleen", "emily", "megan", "rachel")
weat_black <- c("alonzo", "jamel", "theo", "alphonse", "jerome", "leroy", "torrance", "darnell", "lamar",
                "lionel", "tyree", "deion", "lamont", "malik", "terrence", "tyrone", "lavon", "marcellus",
                "wardell", "nichelle", "shereen", "ebony", "latisha", "shaniqua", "jasmine", "tanisha",
                "tia", "lakisha", "latoya", "yolanda", "malika", "yvette")
weat_good <- c("caress", "freedom", "health", "love", "peace", "cheer", "friend", "heaven", "loyal", "pleasure",
               "diamond", "gentle", "honest", "lucky", "rainbow", "diploma", "gift", "honor", "miracle", "sunrise",
               "family", "happy", "laughter", "paradise", "vacation")
weat_bad <- c("abuse", "crash", "filth", "murder", "sickness", "accident", "death", "grief", "poison", "stink",
              "assault", "disaster", "hatred", "pollute", "tragedy", "bomb", "divorce", "jail", "poverty", "ugly",
              "cancer", "evil", "kill", "rotten", "vomit")

# Get GloVe matrices
weat_white_glove <- scale(glove_d[c(weat_white),])
weat_black_glove <- scale(glove_d[c(weat_black),])
weat_good_glove <- scale(glove_d[c(weat_good),])
weat_bad_glove <- scale(glove_d[c(weat_bad),])

# Randomize the order
weat_white_glove <- weat_white_glove[sample(1:nrow(weat_white_glove)),]
weat_black_glove <- weat_black_glove[sample(1:nrow(weat_black_glove)),]
weat_good_glove <- weat_good_glove[sample(1:nrow(weat_good_glove)),]
weat_bad_glove <- weat_white_glove[sample(1:nrow(weat_bad_glove)),]

# Compute SVD to get linear subspaces
# All 4 have different characteristic polynomials
# Scaling the vectors onto the unit ball first does not solve this problem!!
wwg_basis <- svd(weat_white_glove)
wbg_basis <- svd(weat_black_glove)
good_basis <- svd(weat_good_glove)
bad_basis <- svd(weat_bad_glove)

# Compute the matrix that rotates each of these hyperellipses onto the unit ball
wwg_vb <- varimax(wwg_basis$u)
wbg_vb <- varimax(wbg_basis$u)
good_vb <- varimax(good_basis$u)
bad_vb <- varimax(bad_basis$u)

# It can be shown that these share a linear basis, but the eigenvalues are different
# See Krzanowski (1979), "Between-groups comparison of principal components", JASA 74(367): 703-707.
# Compute S matrix (embed in common space); Tr(S) ranges from 0 (orthogonal spaces) to k (coincident spaces)
wg_wb_s <- wwg_basis$u %*% t(wbg_basis$u) %*% wbg_basis$u %*% t(wwg_basis$u)
wbs_basis <- svd(wg_wb_s)
# sum(wbs_basis$d)

# First look at the shape of the name space
# Note the white names more heavily weight the larger eigenvalues
name_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="White names"),
                 data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Black names"))
name_ev %>%
  ggplot(aes(x, y, color=group)) +
  geom_point() +
  geom_line() + 
  labs(x="Eigenvector",
       y="Eigenvalue",
       title="Shape of name subspaces")

# The ratio of eigenvalues in both groups scaled as a proportion of variance provides a sense for
#  how much addition in this domain is scaled by the contribution of each example to the structure of
#  the linear subspace spanned by each set of names (i.e. how differently the A and B subspaces weight each dimension)
# An intuitive way of understanding this is how much you have to stretch or compress your unit ball
# map in each direction to get from one group of words to the other set of words while preserving the meaning of "addition" 
# Both of these elliptical regions can be projected onto the same linear subspace, but they are different projections
d_prop_name <- (wwg_basis$d / sum(wwg_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))
data.frame(x=1:length(wwg_basis$d),
           y=(wwg_basis$d / sum(wwg_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
  ggplot(aes(x, y)) +
  geom_hline(yintercept = 1, linetype="dashed") +
  geom_point() +
  labs(x="Eigenvector",
       y="Eigenvector ratio",
       title="Ratio of variance explained by \n dimension between name spaces")

# The ratio of the maximum D_prop to the minimum D_prop describes the maximum distortion
# when this ratio is 0.5, the eigenvector corresponding to the least influential exemplary pair is half as long
# as the eigenvector for the most influential exemplary pair.
# The scale of the most elliptical 2D cross-section of the hyperellipse
# min(d_prop_name) / max(d_prop_name)

# Now look at the attribute space
# The attribute space is relatively less distorted/stretched than the higher-dimensional name space
attr_ev <- rbind(data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Good words"),
                 data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Bad words"))
attr_ev %>%
  ggplot(aes(x, y, color=group)) +
  geom_point() +
  geom_line() + 
  labs(x="Eigenvector",
       y="Eigenvalue",
       title="Shape of attribute subspaces")

# Plot ratio of variance explained by eigenvalue
d_prop_attr <- (good_basis$d / sum(good_basis$d)) / (bad_basis$d / sum(bad_basis$d))
data.frame(x=1:length(good_basis$d),
           y=(good_basis$d / sum(good_basis$d)) / (bad_basis$d / sum(bad_basis$d))) %>%
  ggplot(aes(x, y)) +
  geom_hline(yintercept = 1, linetype="dashed") +
  geom_point() +
  labs(x="Eigenvector",
       y="Eigenvalue ratio",
       title="Ratio of variance explained by \n dimension between attribute spaces")

# Maximum distortion factor
# min(d_prop_attr) / max(d_prop_attr)
```

```{r, echo=F, message=F, fig.height=4, fig.width=4.5, fig.align="center"}
# SIDEBAR: Uneven term set sizes as a source of bias in the WEAT
# Now let's look at the Krzanowski trace statistics across these comparisons
# To do this we take the top k dimensions of the larger hyperellipse
# The statistic is less than full rank (i.e. the subspaces are not in the same linear basis!)
# minrank <- 25
# ktest1 <- wwg_basis$u[1:minrank,1:minrank] %*% t(good_basis$u) %*% good_basis$u %*% t(wwg_basis$u[1:minrank,1:minrank])
# ktest2 <- wwg_basis$u[1:minrank,1:minrank] %*% t(bad_basis$u) %*% bad_basis$u %*% t(wwg_basis$u[1:minrank,1:minrank])
# ktest3 <- wbg_basis$u[1:minrank,1:minrank] %*% t(good_basis$u) %*% good_basis$u %*% t(wbg_basis$u[1:minrank,1:minrank])
# ktest4 <- wbg_basis$u[1:minrank,1:minrank] %*% t(bad_basis$u) %*% bad_basis$u %*% t(wbg_basis$u[1:minrank,1:minrank])
# sum(svd(ktest1)$d) / minrank
# sum(svd(ktest2)$d) / minrank
# sum(svd(ktest3)$d) / minrank
# sum(svd(ktest4)$d) / minrank

# The resulting addition metrics are only about 78% similar unless we equalize
# the size of the groups (net of randomization). So one way of screwing up is
# to use unequal term group sizes, i.e. to cram this into the lower-dimensional
# space defined by the attribute words.
```

```{r, echo=F, message=F, fig.height=6.5, fig.width=7, fig.align="center", fig.cap="The name and attribute subspaces do not have the same shape."}
# We now know how to get from one basis to another basis
# Now compare what happens when you subtract the hyperellipses with and without putting them in the same basis

# Swap out the white-black names for a subsample of the space in the same dimension as the good-bad terms
weat_white_glove_s <- weat_white_glove[sample(1:nrow(weat_white_glove), 25),]
weat_black_glove_s <- weat_black_glove[sample(1:nrow(weat_white_glove), 25),]
wwg_basis <- svd(weat_white_glove_s)
wbg_basis <- svd(weat_black_glove_s)
wwg_vb <- varimax(wwg_basis$u)
wbg_vb <- varimax(wbg_basis$u)

# Now compare the name-to-attribute spaces
white_good_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="Names (white)"),
                       data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Attributes (good)"))
white_bad_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="Names (white)"),
                       data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Attributes (bad)"))
black_good_ev <- rbind(data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Names (Black)"),
                       data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Attributes (good)"))
black_bad_ev <- rbind(data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Names (Black)"),
                       data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Attributes (bad)"))

grid.arrange(
  white_good_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"), 
  white_bad_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"), 
  black_good_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"),
  black_bad_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"),
  ncol=2
)

# white_good_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# white_bad_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# black_good_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# black_bad_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()

# Look at how these functions "cross"
black_good_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (Black)` > `Attributes (good)`) -> bgood_x
black_bad_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (Black)` > `Attributes (bad)`) -> bbad_x
white_good_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (white)` > `Attributes (good)`) -> wgood_x
white_bad_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (white)` > `Attributes (bad)`) -> wbad_x
```

```{r, echo=F, message=F, fig.height=6.5, fig.width=7, fig.align="center", fig.cap="Ratio of variance explained by eigenvector across paired name-attribute subspaces."}
# Plot ratio of variance explained by eigenvalues in each pair of subspaces
# These are the implicit weights on each dimension of "addition" we perform by adding across
#  these spaces without performing the necessary change of basis
# Observe that the key differentiating factor is the first two principal components (the size dimension)
grid.arrange(data.frame(x=1:length(good_basis$d),
                       y=(good_basis$d / sum(good_basis$d)) / (wwg_basis$d / sum(wwg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Good/White"),
            data.frame(x=1:length(bad_basis$d),
                       y=(bad_basis$d / sum(bad_basis$d)) / (wwg_basis$d / sum(wwg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Bad/White"),
            data.frame(x=1:length(good_basis$d),
                       y=(good_basis$d / sum(good_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Good/Black"),
            data.frame(x=1:length(bad_basis$d),
                       y=(bad_basis$d / sum(bad_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                     y="Bad/Black"), ncol=2)
```

```{r, eval=F, echo=F, message=F, fig.height=4, fig.width=4.5, fig.align="center"}
# This leads to a bilinear form (each dimension is the difference of two vectors)
# The resulting object has bilinear principal components (principal "planes")
# The transformation from one to the other will tend to upweight the comparisons that are more elliptical
good_bad_bl <- good_basis$u %*% good_vb$rotmat - bad_basis$u %*% bad_vb$rotmat
black_white_bl <- wwg_basis$u %*% wwg_vb$rotmat - wbg_basis$u %*% wbg_vb$rotmat

gb_bl_basis <- svd(good_bad_bl)
bw_bl_basis <- svd(black_white_bl)

gb_bl_vb <- varimax(gb_bl_basis$u)
bw_bl_vb <- varimax(bw_bl_basis$u)

# Without scale
good_bad_nosc <- good_basis$u - bad_basis$u
black_white_nosc <- wwg_basis$u - wbg_basis$u

gb_nosc_basis <- svd(good_bad_nosc)
bw_nosc_basis <- svd(black_white_nosc)

gb_nosc_vb <- varimax(gb_nosc_basis$u)
bw_nosc_vb <- varimax(bw_nosc_basis$u)

# The problem with the subtraction operator is that it imposes a pairwise one-to-one comparison on the data
# This smuggles in information about the relative magnitude of the terms!

# This matrix describes how you get B into the coordinates of A through the standard basis:
# gb_bl_vb$rotmat %*% t(bw_bl_vb$rotmat)

# Rotate them using each other's transformation matrix -- then how similar are these?
gb_bwrot <- gb_bl_basis$u %*% gb_bl_vb$rotmat %*% t(bw_bl_vb$rotmat)
bw_gbrot <- bw_bl_basis$u %*% bw_bl_vb$rotmat %*% t(gb_bl_vb$rotmat)

A <- gb_bl_basis$u
B <- svd(gb_bwrot)$u
C <- bw_bl_basis$u
D <- bw_gbrot

# The relationship of the coordinate-transformed matrix to the original matrix can be described
#  almost entirely in terms of a single principal plane in the BW space.
reshape2::melt(A) %>%
  left_join(reshape2::melt(B), by=c("Var1", "Var2")) %>%
  mutate(vdiff = value.x - value.y,
         vratio = value.x / value.y) %>%
  ggplot(aes(x=Var1, y=Var2, fill=vratio)) +
  geom_tile() + scale_fill_viridis_c()
data.frame(x=1:25, y=svd(A/B)$d) %>% ggplot(aes(x, y)) + geom_point() + geom_line()

```

\hfill\break

# How standard basis arithmetic with cosine similarities induces frequency bias

The problem lies in the use of standard basis arithmetic as a global surrogate for local comparisons in a series of distorted linear subspaces of the vector space model, which have different bases for addition. As previously discussed, this means that there is an implicit change of basis involved in subtracting means of cosine similarities. The key issue is the absolute difference operation $`-`$ at the heart of the statistic. I show next that taking this difference implies a factorization of the resulting quantity into a ratio of two angular components: an absolute angular component $\theta_{AB}$ and a relative angular component $\theta_{WD}$. The WEAT statistic can be written as a particular function of these.

For analytic convenience, begin by considering the $||W|| = 1$ case. The key quantity in the WEAT statistic is the standard basis difference in cosine similarity between W and A and W and B:

$$cos(\theta_{WA}) - cos(\theta_{WB})$$  
This function is bounded on $[-2, 2]$, is larger when $cos(\theta_{WA}) > cos(\theta_{WB})$, and reaches its greatest variance at 0:

```{r, echo=F, message=F, fig.height=4, fig.width=4, fig.align="center"}
library(tidyverse)
library(latex2exp)
theme_set(theme_bw())
test <- data.frame(x=seq(-1, 1, by=0.005), y=seq(-1, 1, by=0.005)) %>% expand(x, y) %>% mutate(z=x - y)
test %>% ggplot(aes(x, y, fill=z)) + geom_tile() + labs(x=TeX("$cos(\\theta_{AB})$"), y=TeX("$cos(\\theta_{AC})$"), fill=TeX("$cos(\\theta_{AB}) - cos(\\theta_{AC})$")) + theme(legend.position='bottom')
```

The difference in cosine similarities between two terms and a third word can be written as the scalar projection of the difference vector between A and B, $D = \frac{A}{||A||} - \frac{B}{||B||}$ onto to the attribute vector $W$:

$$
\begin{aligned}
cos(\theta_{WA}) - cos(\theta_{WB}) &= \frac {W \cdot A}{||W||\;||A||} - \frac {W \cdot B}{||W||\;||B||}  \\
&= \frac{W}{||W||} \cdot \left( \frac{A}{||A||} - \frac{B}{||B||} \right) \\
&= \frac{W \cdot D}{||W||} \\
&= \frac{||W||\;||D||\; cos(\theta_{WD})}{||W||} \\
&= ||D||\;cos(\theta_{WD})
\end{aligned}
$$

Note that because the underlying vectors are normalized, the magnitude of the difference vector ($||D||$) is independent of the relative scale of $A$ and $B$, so it is entirely determined by their "absolute" angle to each other (i.e. relative to the origin and regardless of the choice of $W$). $||D||$ can be written as a function of the cosine similarity of this angle:

$$
\begin{aligned}
||D|| &= ||\frac{A}{||A||} - \frac{B}{||B||}|| \\
&= \sqrt{||A|| + ||B|| - 2(\frac{A}{||A||} \cdot \frac{B}{||B||})} \\
&= \sqrt{2 - 2\cos(\theta_{AB}))} \\
&= \sqrt{2(1 - \cos(\theta_{AB}))}. \\
\end{aligned}
$$

This implies that the scalar projection (the difference in cosine similarities) can be parameterized entirely in angular coordinates, one "absolute" component computed with respect to the origin and one "relative" component computed with respect to $W$:

$$||D||\;cos(\theta_{WD}) = \sqrt{2(1 - \cos(\theta_{AB}))}\;\cos(\theta_{WD})$$

For convenience, let $\phi(\cos(\theta_{\alpha})) = ||D|| = \sqrt{2(1 - \cos(\theta_{AB}))}$. I next show that the WEAT statistic can be written in terms of a sum of these scalar projections. WEAT is the difference between the means of $s(W, A, B)$ over pairs of names selected from lists $\mathbf{A}$ and $\mathbf{B}$ and across pairs of attribute terms selected from lists $\mathbf{X}$ and $\mathbf{Y}$ Assuming (as in the original paper) that these lists are of length $n$ and $m$ respectively:

$$
\begin{aligned}
s(W, A, B)  &= \frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WA_i})-\frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WB_i})\\
            &= \frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WA_i}) - \cos(\theta_{WB_i}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} ||D_i||\;\cos(\theta_{WD_i}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} \sqrt{2(1 - \cos(\theta_{A_iB_i}))}\;\cos(\theta_{WD_i}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} \phi(\cos(\theta_{A_iB_i}))\cos(\theta_{WD_i}) \\
\end{aligned}
$$

Then, this can be extended over $W$ to the full set of paired difference comparisons in $X$ and $Y$. The same bilinear transformation performed above can be done in the $X - Y$ space; let $T$ be these difference vectors. Then, factoring:

$$
\begin{aligned}
S_{A, B, X, Y} &= \sum_{x \in X} s(x, A, B)-\sum_{y \in Y} s(y, A, B) \\
  &= \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} \phi(\cos(\theta_{A_iB_i}))\cos(\theta_{xD_i}) -
     \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} \phi(\cos(\theta_{A_iB_i}))\cos(\theta_{yD_i}) \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\cos(\theta_{A_iB_i}))\cos(\theta_{x_jD_i}) - \phi(\cos(\theta_{A_iB_i}))\cos(\theta_{y_jD_i}) \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\cos(\theta_{A_iB_i}))(\cos(\theta_{x_jD_i})\;-\;\cos(\theta_{y_jD_i}))  \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\cos(\theta_{A_iB_i}))(||T_i||\cos(\theta_{T_iD_i}))  \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\cos(\theta_{A_iB_i}))\cdot\phi(\cos(\theta_{X_jY_j}))\cdot\cos(\theta_{T_iD_i}))  \right]
\end{aligned}
$$

Observe that this representation of the WEAT statistic factors it into a sum of a function of three angles: an "absolute" angular component $\theta_{A_iB_i}$ and two "relative" angular components $\theta_{x_jD_i}$, $\theta_{y_jD_i}$ (line 3). This reflects the fact that the comparison is being computed in a weighted average of the differenced subspaces described previously. Note that these coordinates could also be expressed from the perspective of the $X$ and $Y$ term spaces. An equivalent parameterization decomposes the statistic into two "absolute" angular components $\theta_{A_iB_i}$, $\theta_{X_jY_j}$ (i.e. the angles between matched pairs of names/attributes) and one "relative" angular component between the two corresponding difference vectors $\theta_{T_iD_i}$ (last line).

Following this last way of thinking about the statistic, you can think of WEAT as constructing a sum of cosine similarities between (e.g.) $\texttt{Harry} - \texttt{Jamel}$ and $\texttt{peace} - \texttt{sickness}$. The sum we get is weighted by the relative magnitudes of these difference vectors; it will tend to upweight comparisons where the ratio of the absolute angular components $\theta_{A_iB_i}$ and $\theta_{X_jY_j}$ is larger (i.e. comparisons when one of the pairs is more similar than the other pair and thus has a shorter difference vector) even when the underlying "purely relative" or analogical component $\theta_{T_iD_i}$ is the same. Conversely, different values of $\theta_{T_iD_i}$ can sum to the same WEAT value given the right combination of relative difference vector magnitudes. Dually, you can view the relative component as the weight. From this perspective, the component of interest is a function of the ratio of $\theta_{A_iB_i}$ and $\theta_{X_jY_j}$. This quantity is weighted by the relative angle at which the two difference vectors $T_i$ and $D_i$ intersect.

# Conclusion: Measuring culture in word embeddings

The angular representation of the WEAT statistic describes the sense in which it is not independent of scale. This property of the statistic is not dependent on the model used to generate the word vectors, but originates from the choice of comparison operator and the way it arranges the data comparatively. This scheme derives proportionally more of its information from comparisons that are more uneven. This upweights comparisons involving rarer terms (i.e. terms with longer word vectors). This suggests a methodological reason why the estimated linguistic bias in van Loon et al. (2022) disappears when a measure of term frequency is added to the other side of the regression model: the comparison has been assembled in a direction-dependent way.

An important but relatively uninteresting takeaway is that the WEAT measure is very sensitive to the relative lengths of the word lists. This is uninteresting in the sense that researchers can for the most part address this problem by using four word lists that are the same length. A slightly more interesting takeaway (which is, alas, uninteresting for the same reason) is that randomizing the order of the lists is important, because the original ordering of the lists can smuggle scale information into the paired comparisons WEAT operates on. But most interesting is that equalizing the lengths of the word lists and randomizing their order does not completely solve the problem, because the lists imply different relative term frequency distributions in different parts of the vector space.

In effect, what is happening is something like the following fanciful anecdote. I have been teleported to Middle Earth and am standing next to a giant and a dwarf who wish to know how tall I am. Each of them is holding what they consider to be a standard 12-inch ruler on their respective scales, such that the average adult in each group measures around 5-foot-8 (in giant-inches or dwarf-inches). I mark the point on my waist that is exactly half of my height. Then, the dwarf measures my height up to my waist from my feet, and the giant measures my height down to my waist from my head. After some deliberation, they report the sum of their results to me, and to my awe they have me exactly right at 73 inches! But are these the inches hypothetical me thinks they are? To be sure, the rulers are good for measuring giants and dwarves independently, and it is possible for various differently weighted combinations of giant-inches and dwarf-inches to give me approximately the right answer. But the unknown contributions of the two rulers suggests that this is a generally challenging way to measure height.

\pagebreak

## Works referenced

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent

Antoniak, M. and D. Mimno. 2021. "Bad Seeds: Evaluating Lexical Methods for Bias Measurement." *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing*: 1889-1904.

Arseniev-Koehler, A. 2022. "Theoretical foundations and limits of word embeddings: what types of meaning can they capture?" SocArXiv preprint: https://osf.io/preprints/socarxiv/vrwk3/.

Bai, Z. and J.W. Silverstein. 2010. *Spectral Analysis of Large Dimensional Random Matrices.* New York: Springer.

Balaban, A.T., I. Motoc, D. Bonchev, and O. Mekenyan. 1983. "Topological indices for structure-activity correlations." In V. Austel, A. T. Balaban, D. Bonchev, M. Charton, T. Fujita, H. Iwamura, O. Mekenyan, and I. Motoc (eds.), *Steric Effects in Drug Design*: 21-55.

Bertrand, M. and S. Mullainathan. 2004. "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination." *American Economic Review*  94: 991–1013.

Caliskan, A, J.J. Bryson and A. Narayanan. 2017. "Semantics derived automatically from language corpora contain human-like biases." *Science* 356(6334): 183-186.

Carpenter, Jr., J. A., H. A. Fitzhugh, T. C. Cartwright, R. C. Thomas, and A. A. Melton. 1978. "Principal Components for Cow Size and Shape." *Journal of Animal Science* 46(2): 370-375.

Cvetković, D. M., and I. Gutman. 1977. "Note on branching." *Croatica Chemica Acta* 49(1): 115-121.

Greenwald, A.G, D. E. McGhee, and J. L. Schwartz. 1998. "Measuring individual differences in implicit cognition: The implicit association test." *Journal of Personality and Social Psychology* 74: 1464–1480.

Jolicoeur, P. and J.E. Mosimann. 1960. "Size and shape variation in the painted turtle: A principal component analysis." *Growth* 24(4): 339-354.

Krzanowski, W.J. 1979. "Between-groups comparison of principal components." *Journal of the American Statistical Association* 74(367): 703-707.

Mu, J. and P. Viswanath. 2018 "All-but-the-Top: Simple and Effective Postprocessing for Word Representations." *Proceedings of the 6th International Conference on Learning Representations (2018)*: 1-25.

van Loon, A., S. Giorgi, R. Willer, and J. Eichstaedt. 2022. "Negative Associations in Word Embeddings Predict anti-Black Bias Across Regions -- but only via Name Frequency." arXiv preprint: 2201.08451.
