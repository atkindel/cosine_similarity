---
title: "cosine_v4_extra"
output: pdf_document
---

## Appendix A: Cosine similarity as a hyperbolic projection of the inner product space

```{r nelson_lld, echo=F, message=F}
# Cosine similarity as a locally linear perspective on the inner product space
# The factorization implies you can look at the manifold from three "sides"
local_linear_decomp <- function(rangs) {
  # Plot some inner product distribution by components. Larger objects in the
  # space have more differentiated IPs, but there are also way fewer of them.
  rangs %>%
    ggplot(aes(y=ab_cs, color=ab_ip, x=nprod, size=ab_ip)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v1
  
  # Cosine similarity differentiates three regions on the inner product space
  # that we can more easily see if we look from the cosine similarity into the
  # manifold. Looking from this perspective helps you see the "twist" structure
  # in A by rotating to the position in which the "flat" structure has a minimal
  # profile and the twist has a maximal profile. The extremal region of the twist
  # (by local normalization weight) is generally more "flat" than it was from the other perspective.
  # The linear relationship between local normalization weight and the IP is controlled by cosine similarity.
  rangs %>%
    ggplot(aes(color=ab_cs, y=ab_ip, x=nprod, size=ab_cs)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v2
  
  # Cosine similarity has a linear relationship to the inner product, but only
  # conditional on the local normalization weight, which defines a class of vector pairs
  # with the same CS-IP linear relationship. The distribution of cosine similarities in
  # any given class is generally skewed and tends to have non-zero mean. The variance of
  # the cosine similarity distribution is non-constant and varies systematically with the
  # local normalization weight. One way of thinking about this is that the cosine similarity
  # is a maximally linear perspective on the manifold.
  rangs %>%
    ggplot(aes(x=ab_cs, y=ab_ip, color=nprod, size=nprod)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v3
  
  ggarrange(v1, v2, v3, ncol=3)
}
```

```{r nelson_lld_print, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", include=T}
local_linear_decomp(ra_nemb)
```

Readers may find it helpful to develop a geometric intuition for the relationship between the inner product and cosine similarity. The cosine similarity between two points is a ratio of three inner products of the form $f(a,b,c)=\frac{a}{bc}$. Consequently, the projecting transformation has the shape of a hyperbolic paraboloid.^[Readers may find this shape surprisingly familiar. The "saddle roof" used to cover large buildings like churches and stadiums is a hyperbolic paraboloid; this is also the shape of each chip in a tube of Pringles.] A more precise way of characterizing the non-Euclidean shape of this surface is in terms of its pointwise Gaussian curvature:  
$$\text{K}(A, B) = -(1 + ||A||^2||B||^{2} + \cos^{2}(A, B))^{-2}$$  
This quantity is non-constant and negative. The relationship between the Gaussian curvature and the local normalization weight is helpful for understanding intuitively what cosine similarity is doing to create semantically meaningful lists of words. In particular, it suggests that cosine differentiates short-norm, low-frequency vector pairs proportionally more from each other than word pairs involving long word vectors. 

This equation can be rearranged to parameterize the cosine similarity between $A$ and $B$ as a function of the discrepancy between the local normalization weight and the Gaussian curvature at this point:

$$\cos(A, B) = \sqrt{\sqrt{\frac{-1}{K}} - ||A||^2||B||^2 - 1}$$  
This representation of the quantity bears an interesting resemblance to the subsampling weight function used to downsample frequent terms in word2vec prior to forming the context window, $1-\sqrt{t/p(w)}$ (Mikolov et al. 2013). Goldberg and Levy (2014) comment that this subsampling procedure considerably changes the geometry of the resulting vector space. Because very common words are down-sampled, training samples are more likely to see more words that tend to co-occur with common words in the wider *effective* contexts where these common words would have been, and this effect is proportionally larger for rarer words because this change is additive. The variance in norms among low-frequency terms in SGNS is lower than it "should be" because some of this variation is taken up by the high-frequency terms, which consequently occupy a less locally scale-biased region of the inner product space. In Figure 5 (top panel) note that there is a smooth downward distortion of the distribution of squared vector norms after a certain point in the frequency distribution, resulting in a region where the mean vector norm is closer to uniform conditional on frequency rather than intensifying this relationship in the opposite direction (compare Figure 2 in Arora et al. 2016). 

Computing this quantity exactly overlooks that there is uncertainty associated with performing the non-Euclidean transformation. This can be worsened by inducing dependencies prior to the projection; this commonly occurs when we make other transformations to the inner product space, such as adding vectors together.

```{r nelson_gaussiancurvature, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Let's look at the Gaussian curvature next
# This shows that the decomposition is non-Euclidean
# It also helps us understand what cosine similarity does to the inner product space
# Mainly what it does is it bends the low-frequency space
# If you scale the LPNW the relative curvature in the rest of the manifold becomes more pronounced
ra_nemb %<>% mutate(gK = -1 * (1 + nprod^2 + ab_cs^2)^-2)

ggarrange(ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=nprod, y=ab_cs, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=nprod, y=ab_ip, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=ab_cs, y=ab_ip, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ncol=3, common.legend = T, legend="bottom")

# Show this in terms of frequency
ra_nemb %>% 
  ggplot(aes(x=log(a$frequency), y=gK, color=ab_cs)) +
  geom_point(size=3) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> gkp1
ra_nemb %>% 
  ggplot(aes(x=log(b$frequency), y=gK, color=ab_cs)) +
  geom_point(size=3) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> gkp2
ggarrange(gkp1, gkp2, ncol=2, common.legend = T, legend="bottom")

# Better plot of curvature-frequency relationship
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=log(a$frequency), y=log(b$frequency), color=gK)) +
  geom_point() +
  scale_color_viridis_c(direction=-1)
```


## Appendix B: Scale and frequency in neural word embeddings

Log-bilinear word embedding models like CBOW, SGNS, and GloVe can be interpreted as a family of low-rank approximations to latent semantic analysis (LSA; Deerwester et al. 1990; Mnih & Hinton 2008; Levy & Goldberg 2014; Arora et al. 2016). LSA involves taking the singular value decomposition of a large, sparse word frequency matrix; a key advantage of modern neural word embedding models is that they enable researchers to efficiently find dense low-rank approximations to this matrix with better bounds on memory and time requirements, facilitating data analysis with much larger amounts of information.^[This is an oversimplification; in practice there can be important differences in the way that each algorithm constructs word-word associations for a given problem, and the approximation error can be important.] The output of a word embedding model is a set of vectors embedded in a $p$-dimensional inner product space. This space is so-named because it supports the inner product operation $\left<v_w, v_c\right>$, which gives a measure of distance between any two word vectors. The inner product space occupied by classic word embedding vectors is a Euclidean vector space on $\mathbb{R}^p$, where the conventional inner product and norm are the dot product and the Euclidean norm respectively. The vectors estimated by classic word embedding models vary in scale (some are longer than others), so the inner product space is associated with a distribution of Euclidean norms.

Arora, Li, Liang, Ma, and Risteski (2016) propose a generative log-bilinear model for semantic vector spaces that models an inner product space on relative (log) word frequencies conditional on a random walk vector in the context word space (Theorem 2.2; also see Mnih \& Hinton 2008). Optimizing the proposed model implies finding a vector space that sets distances between word vectors in a way that best represents the conditional random walk frequency distribution for every position in the vocabulary. The model provides a number of clarifying insights regarding the behavior of word embedding models with respect to their word frequency distributions. I focus on the following implication. Formally, we have a set of $p$-dimensional word vectors estimated with window size $q$, and $w$ is any word observed in the corpus. We consider a set of randomly generated and smoothly varying context positions in the semantic vector space: $c \in C$. Then the following linear relationship holds (Corollary 2.3):  
$$
\begin{aligned}
\log p(w) &= \frac{||v_w||^2_2}{2p} - 2\ \text{LogSumExp}_{C,w}\left<v_w, c \right> + \log\left(\frac{q(q-1)}{2}\right) + \epsilon \\
\end{aligned}
$$
Schematically, this equation says that the log word frequency for any given word is a linear function of three quantities: its squared Euclidean norm scaled by the dimensionality of the vector space ($\frac{||v_w||^2_2}{2p}$); the approximate maximum distance^[For some set of scalars $X$, $\text{LogSumExp}(X)$ is a smooth approximation to $\max(X)$.] between this word vector and nearby contexts ($-2\ \text{LogSumExp}_{C,w}\left<v_w, c \right>$); and a constant reflecting the global scale of the frequency distribution induced by the window size ($\log\left(\frac{q(q-1)}{2}\right)$), plus an error term.
