---
title: "Scale distortion and frequency bias in cosine similarity regression"
author: "Alexander T. Kindel^[PhD Candidate, Department of Sociology, Princeton University. Contact: akindel@princeton.edu. I am grateful to Anna Berg, Dahyun Choi, Nicolás Torres-Echevarry, Benjamin Rohr, Brandon Stewart, and members of the Stewart Lab for critical feedback and conversation on earlier drafts of this paper; and to Bobray Bordelon and David Jenkins at Princeton University Library for assistance with dataset acquisition.]"
abstract: |
 |  The cosine of the angle between two *p*-dimensional vectors is widely used as a measure of semantic association in statistical text analysis. In applications of neural word embedding models (e.g. word2vec, GloVe) to problems in the social sciences, a common extension to this methodology involves estimating a regression with cosines of linear combinations of word vectors as the dependent variable. However, because cosine similarity is a ratio with correlated components, regression-based comparisons of this type are susceptible to scale distortion. In the word embedding analysis setting, cosine similarity regression tends to introduce distorted comparisons between heterogeneous locations in the underlying word frequency distribution, resulting in models that suffer from multiple distinct frequency-related biases. I identify key model assumptions for a number of common semantic measurement procedures, discuss informative bias decompositions, and suggest related alternative models. The results are applied to a reanalysis of two frequency-distorted word embedding analyses in cultural sociology.
 | 
 | **Keywords:** cosine similarity, frequency bias, subspace projection, omitted variable bias, word embeddings, measurement
date: |
  | 10 May 2022
  | 
  | 
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: TRUE
    number_sections: true
    toc: FALSE
header-includes:
  - \usepackage{setspace}
  - \usepackage{enumitem}
  - \usepackage{epigraph}
  - \usepackage{cancel}
  - \usepackage{bbm}
  - \usepackage{rotating}
  - \usepackage{caption}
  - \captionsetup[figure]{labelfont=bf}
  - \setlength\epigraphwidth{0.9\textwidth}
  - \setlength\epigraphrule{0pt}
  - \renewcommand{\textflush}{flushepinormal}
  - \renewcommand{\epigraphflush}{center}
  - \setlist{listparindent=\parindent, parsep=0pt}
  - \doublespacing
  - \setlength\parindent{24pt}
  - \setlength{\parskip}{0.5mm}
  - \usepackage[bottom]{footmisc}
  - \renewcommand{\footnotelayout}{\setstretch{1.05}}
---

\pagebreak

```{=latex}
\renewcommand{\baselinestretch}{1}\normalsize
\tableofcontents
\renewcommand{\baselinestretch}{1.8}\normalsize
```

```{r setup, echo=F, message=F}
library(tidyverse)
library(magrittr)
library(readtext)
library(quanteda)
library(quanteda.textstats)
library(lmtest)
library(sandwich)
library(text2vec)
library(word2vec)
library(ggpubr)
library(here)

# Document settings
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, cache.lazy=FALSE)
set.seed(271828)

# Load COCA word frequency file
words_219k_m2138 <- read_delim(here("data", "words_219k_m2138.txt"),
                               "\t", escape_double = FALSE, trim_ws = TRUE, skip = 8)
```

\pagebreak

# Introduction

Word embedding models are a popular tool for computational research on language use in the social sciences. Researchers apply word embeddings to measure entities widely theorized in the study of culture previously considered difficult to quantify, such as discourse, ideology, sentiment, cognition, emotion, bias, stereotype, association, identity, experience, subjectivity, and diversity (among many other concepts; see Kozlowski, Taddy \& Evans 2019; Mohr et al. 2020; Arseniev-Koehler \& Foster 2020). Researchers previously struggled to measure these qualities in text quantitatively because selecting keywords requires making strong assumptions about the link between specific word choices and more general notions of meaning. It has been argued that word embeddings address this problem by assigning similar representations to words that share similar contexts. This inherently contextualizing quality of the representation has made word embeddings a key site of recent activity in the longer-term reorientation of quantitative cultural sociology away from a nominal analysis of separable classifications and toward a comparative analysis of related distinctions (White 2008; Monk 2022). Researchers compose lists of seed words related to concepts of interest (e.g. class, gender, race, emotion) and use the corresponding sets of word vectors to represent them, taking advantage of the distributed representation implied by the vector to avoid overfitting to the inducing keywords. Phrasing and answering research questions about differences in discourse and linguistic association then becomes a matter of constructing a summary quantity over pairs of vectors implied by comparisons within and between the word sets.

A common way to measure pairwise word association in this setting is to take the cosine similarity between the two corresponding word vectors: $$\cos(A_i, B_i) = \frac{\left<A_i, B_i\right>}{||A_i||||B_i||}$$
The popularity of this metric in the social sciences is a mark of the expanding influence of predictive analysis and machine learning on sociological explanation (Watts 2014; Molina \& Garip 2019; Hofman et al. 2021). It originally became popular in information retrieval, where it offers superior performance as a similarity metric for nearest-neighbor indexing and classification of documents, particularly when the comparison of interest involves documents of varying lengths (Salton & Buckley 1988). Mathematically, cosine similarity is the inner product corresponding to the non-Euclidean projection of the estimated semantic vector space onto a unit ball in $\mathbb{R}^p$ (Caillez \& Kuntz 1996; Dominich 2001). This adjustment is traditionally computed exactly over the entire vector space at once by constructing the normalization weight function $||A_i||||B_i||$ from the estimated inner product space. The projection encodes an assumption that from some arbitrary choice of vector and fixed distance, shorter vectors are "really" closer than they appear to be in the original vector space, relative to longer vectors. In other words, if two vectors are equally close to a third vector in the original vector space $(A_i \cdot B_i = A_i \cdot C_i)$, the shorter one will have a higher cosine similarity.^[Two vectors that are approximately the same length $(||B_i|| = ||C_i||)$ can only have the same cosine similarity with $X$ if they are the same distance from it.] An important implication is that the distribution of cosine similarity is systematically higher mean for comparisons involving one or two short vectors. When searching for nearby vectors, a cosine similarity-maximizing procedure tends to exhibit a bias toward shorter neighbors compared to the inner product.

The mathematical properties of the cosine ratio make it a difficult target for inference in the social sciences, where researchers generally phrase questions in terms of conditional expectation functions estimated through regression analysis (Duncan 1984; Lundberg, Johnson & Stewart 2021). The scale dependence of cosine similarity becomes problematic when the research goal changes to predicting similarity or assessing differences in distributions of similarities in this way. One particularly widely used model for this type of analysis is the *cosine similarity regression*, which models the expected cosine similarity over pairs of word vectors or paired linear combinations of word vectors, conditional on some covariates such as a grouping of the words, a characteristic of the author/speaker, or another set of related pairwise word similarities.

$$
\mathbb{E}[\cos\left(\frac{1}{|A|}\sum_{|A|}A_i, \frac{1}{|B|}\sum_{|B|}B_i\right)|\textbf{X}_i]
$$

There are two interacting challenges facing this model: a bundle of statistical issues pertaining to the use of a ratio as the dependent variable, and the relationship between the components of this ratio and the underlying word frequency distribution of the dataset at hand. An extensive methodological literature covers the scale distortions implied by $Y/Z \sim f(X)$ and related models (Fleiss & Zubin 1969; Cormack 1971; Firebaugh \& Gibbs 1985, 1986; Kronmal 1993; Bartlett \& Partnoy 2020). This paper extends these results to the word embedding analysis regime. In this setting, the distribution of the ratio variable is commonly affected by further transformations of the vector space implied by the research design, which imply potentially informative alternative models. The lengths of word vectors estimated by modern neural embeddings are proportional to their marginal frequencies in the underlying corpus (Levy & Goldberg 2014; Arora et al. 2016), so in practice employing a scale distorted model implies the estimated semantic association will be biased in proportion to the word frequency distribution. Including a measure of relative word frequency as a covariate will tend to substantially improve the model fit and can change the interpretation of the estimated relationship. The overall direction of the distortion (the "frequency bias") depends on the particular comparative design as well as the relative location and variance of each word in the dataset within the overall word frequency distribution.

It is increasingly widely known that word embedding analyses are sensitive to this type of estimation bias, but the broad field of models used to summarize semantic vector spaces in the social sciences has eluded direct mathematical characterization. Consequently there is little prior synthetic work that directly focuses on biases and distortions associated with research designs over word embedding models, and researchers have tended to look elsewhere for explanations of frequency distortions in applications of embedding models. For example, the Word Embedding Association Test (WEAT; Caliskan, Bryson & Narayanan 2017), a model of this type widely used to measure implicit bias (e.g. racism, sexism) in collections of text, has been shown to be frequency biased (Ethayarajh, Duvenaud \& Hirst 2018; van Loon et al. 2022; also see Zhou, Ethayarajh \& Jurafsky 2021). Existing explanations for the bias in WEAT attribute it to the choice of word lists or the underlying embedding model. I argue the source of the frequency bias is essentially model misspecification, which results from an overly confident choice of similarity projection prior to conducting statistical analysis. The consequence is an unmodeled discrepancy between a global similarity projection of the entire vector space and a local *effective* similarity projection implied by the design-specific, potentially reprojected analytic vector subspace.

Specifying the discrepancy in similarity comparisons introduced by frequency distortion helps to more clearly characterize the the role of cosine similarity in making judgments of comparative association. In particular, it suggests viewing similarity as an *estimated* quantity that can be the target of statistical inference rather than a mechanically fixed projection used to generate predictions. Estimating similarity requires accounting for uncertainty about the components of the ratio as well as their multiplicative interaction. This principle extends to the use of vector compositions to stand in for the concept spanned by a set of word vectors; this practice also results in a weighted projection of the inner product space. Computing cosine similarities with these composed vectors makes a number of additional strong assumptions about the internal covariance structure among the set of vectors in the composition when standard statistical analyses are run.

I adopt a model-centric perspective to discuss these issues and their impact on data analysis in applied settings. I focus on the parametric OLS setting as this is the most commonly employed tool for summarizing cosine similarities in this style. The advantage of approaching the problem from the perspective of regression analysis is stressing the connection to applied data analysis and motivating alternative approaches to modeling word associations. I write down bias decompositions for common comparisons and apply them to two applications of word embeddings in cultural sociology (Kozlowski et al. 2019; Nelson 2021). Having a particular application in mind is useful for demonstrating the distortion because it helps to show how different common analytic decisions affect the distortion. That said, an important shortcoming of this approach is that it focuses a lot on the particular mechanics of this type of model-based comparison, which may or may not be the most preferable model given the original research question. In particular, the linear composition of vectors prior to analysis is difficult to justify given the extensive potential for frequency distortion as a result of the implied vector space projection. In some cases, the desired model cannot be estimated exactly due to the design of the comparison. Disaggregated models can be estimated more robustly, but other embedding-based models of semantic differentiation in text are available that may offer a more robust route to answering research questions (e.g. Rodriguez, Spirling \& Stewart n.d.). Alternative models making use of the disaggregated underlying word vector sets may also offer qualitatively superior disaggregations of the research question at hand in any particular case, particularly if they contribute to sharpening the research question (Nelson 2020). I return to this point at the end of the paper.

The paper is organized as follows. Section 2 introduces the basic cosine similarity regression model and shows how frequency distortion arises from the application of ratio regression to a semantic vector space. I also discuss two informative decompositions of frequency bias that describe the estimation error in similarity coefficients and the variable sources of distortion that contribute to these estimation issues. In section 3, I extend results from the basic case to models involving linear combinations of word vectors and models having cosines as independent variables. Section 4 applies these results to a discussion of frequency distortion in two applications of word embeddings in cultural sociology. I conclude by noting a few additional known conditions on the cosine similarity distribution and discussing some additional challenges for applying word embeddings in the social sciences. Additional details on the reanalysis are available in the appendix.^[I do not systematically discuss the implications of different embedding approaches (model architecture, training objectives, hyperparameter tuning, etc.) for answering questions in the social sciences. In general, applied researchers use one of three “classic” word embedding algorithms, either GloVe (Pennington, Socher & Manning 2014) or one of the word2vec models (CBOW and SGNS; Mikolov et al. 2013). Model choice may imply different notions of meaning that are important relative to social science applications. I discuss these issues only briefly and selectively as they arise; readers are encouraged to consult a more comprehensive introduction to semantic vector space models (e.g. Jurafsky & Martin 2021, ch. 6; Grimmer, Roberts & Stewart 2022, ch. 7-8; Rodriguez & Spirling 2022; Turney & Pantel 2010) for a more detailed discussion of these issues.]

# Frequency bias in cosine similarity regression

Cosine similarity measures the alignment of two $p$-dimensional vectors $A$ and $B$ by taking the ratio of their inner product to the product of their norms:

$$\cos(\theta_{AB}) = \frac {\left<A, B\right>}{||A||\;||B||} = \frac{\sum_i a_ib_i}{\sqrt{\sum_i a_i^2} \sqrt{\sum_i b_i^2}}$$  
The ratio describes the coplanar angle between the two vectors, and is usually interpreted as the amount that their directions are the same, different, or opposite. Due to the mathematical relationship between the inner product and the norm defined by the Cauchy-Schwarz inequality, $\left|\left<A, B\right>\right| \leq ||A||||B||$, the ratio is nominally bounded from -1 (pointing in opposite directions) to 0 (pointing in different/orthogonal directions) to 1 (pointing in the same direction). In practice the distribution of cosine similarity is application-dependent because its range depends on how isotropically the vectors are distributed throughout the estimated vector space. In the modern log-bilinear word embeddings setting, the inner product is positively skewed (Mimno & Thompson 2017; Mu & Viswanath 2018) and in general it cannot be negative when all of the vector components are positive.^[Throughout this section I assume the paired word vector sets $A$ and $B$ are independently and identically distributed. This is a critical simplification. In practice, researchers typically construct comparisons that imply non-independent similarity structures. The following section discusses this aspect of frequency bias in more depth.]

An important implication of the design of modern neural word embedding models is that the estimated semantic vector space norms are strongly related to the corresponding word frequencies in the underlying corpus (Mnih & Hinton 2008; Arora et al. 2016; Ethayarajh, Duvenaud \& Hirst 2018). This implies that the cosine similarity between any two word vectors is partially a function of a ratio of their frequencies:  
$$
\begin{aligned}
\frac{\left<A, B\right>}{||A||\ ||B||} \propto \frac{\left<A, B\right>}{\sqrt{\log p(A)} \ \sqrt{\log p(B)}} \\
\end{aligned}
$$
The close relationship between cosine similarity and the word frequency distribution is paradoxical when we consider that the entire motivation for using cosine similarity in the first place is that it is typically understood (heuristically) to be a *solution* to the problem of scale dependence in vector space comparison. The confusion revolves around the role of the product of the Euclidean vector norms corresponding to a given pair of words in constructing this quantity; I refer to this quantity as the *(local) normalization weight* (LNW) function throughout the paper. It is a local weight in the sense that $1/\text{LNW}(A,B)$ adjusts each point in the vector space in a way that is specific to the pair $\{A, B\}$. $\text{LNW}(A,B)$ describes how the transformation from the original inner product space inhabited by the word vectors to the normalized inner product space performed when applying cosine similarity varies at different locations in the underlying word frequency distribution; consequently it plays an important role in frequency bias.

## Binary cosine similarity regression

Researchers are often in the following situation. There is a set of cosine similarities and there is a need to summarize them concisely, ideally in a single quantity. Typically this is done as a way of making a comparison in the data (e.g., asking whether the similarities in set A are larger on average than in set B). Questions of this type are often estimated using the binary *cosine similarity regression*, $\cos(A_i, B_i) = \beta^*_0 + \beta^*_1X_i + \epsilon_i$, where $X_i$ is a binary variable indicating a grouping of the vector pairs (as in e.g. the difference in means). The estimated coefficient $\beta^*_1$ is interpreted as a potentially significant difference in the mean cosine similarity, indicating a substantively meaningful difference in linguistic association between the two groups.

Estimating this model makes two strong parametric assumptions about the relationship between the inner product and the normalization weight $\text{LNW}_i$ with respect to the grouping variable. These assumptions can be seen more transparently by multiplying both sides of the model equation by the normalization weight:
$$
\begin{aligned}
\cos(A_i, B_i) &= \alpha^*_0 + \alpha^*_1X_i + \epsilon_i \\
\frac{\left<A_i,B_i\right>}{\text{LNW}_i} &= \alpha^*_0 + \alpha^*_1X_i + \epsilon_i \\
\left<A_i,B_i\right> &= \alpha^*_0\text{LNW}_i + \alpha^*_1X_i\text{LNW}_i + \epsilon_i\text{LNW}_i \\
\left<A_i,B_i\right> &= \alpha^*_0\text{LNW}_i + \alpha^*_1X_i\text{LNW}_i + \epsilon_i\text{LNW}_i \\
&+ \color{red}{\alpha^*_2 + \alpha^*_3X_i} \\
\end{aligned}
$$  
The model omits the two terms highlighted in red: an intercept term and a term for the main effect of $X$; estimating the original model implicitly assumes $\alpha^*_2 = \alpha^*_3 = 0$.

There are a few interconnected problems with the model that can be characterized from an omitted variable bias perspective (Firebaugh \& Gibbs 1985, 1986; Kronmal 1993; Bartlett \& Partnoy 2020). First, the missing intercept term $\alpha^*_2$ forces the line of best fit to pass through the origin. This leads to poor model fit in most practical data analysis settings. When working with word vectors, the distribution of normalization weights (i.e. norm products) is bounded away from zero. Additionally, the inner product is only small/negative when the weight is increasing, meaning the implicit frequency adjustment estimated by cosine similarity regression tends to have the wrong sign. The variance of the model is also likely to be high due to its strong dependence on the observations with large outlying errors on the tails of the normalization weight distribution.

Second, the missing main effect term $\alpha^*_3X_i$ distorts the estimated difference in means by requiring the trends in each group to converge at the intercept. This implies a conditional difference in means that is maximally distinct when the normalization weight increases. This tends to be the opposite of what we see in semantic vector spaces; we observe a wider range of inner product values when the normalization weight is low, and the scale-conditional difference tends to be driven by the low-norm-weight vectors.

In practice, the difference in the marginal distribution of the inner product of a set of points with two focal vectors tends to be close to zero, particularly if the number of vectors in the analysis is small. It is possible to use more vectors to estimate the difference, but this involves a tradeoff with the generality of the resulting conceptual neighborhood. In this case generality can be a *bad* thing because we would like the set of points to be a maximally crisp representation of a theoretical concept of interest, and adding more terms heightens the polysemy of the corresponding concept. A better model will avoid moving this tradeoff in a bad direction for the sake of the test. Yet our interest is rarely in whether the set of distances is significantly larger in one group. Despite the specification error, there is a strong intuition behind the original coefficient: what researchers need is an estimate of the difference in the scale-distance relationship between the groups.

An alternative model, the *inner product regression*, includes the missing intercept and main effect terms in a regression with the inner product as the dependent variable:  
$$
\begin{aligned}
\left<A_i,B_i\right> &= \beta_0 + \beta_1X_i + \beta_2\text{LNW}_i + \beta_3X_i\text{LNW}_i + v_i \\
\end{aligned}
$$  
Note that in this model the focus is no longer on the coefficient on $X_i$, as in the cosine similarity regression model. In practice there are a large range of values of $\beta_1$, but the estimate tends to be high-variance.^[For example, when estimating the comparisons of interest in Nelson (2021) using inner product regression, in no case can we confidently state that any of the sets of social institution vectors *of similar length* are closer or further away than any of the social identity vectors on average; see appendix.] Instead, the target of inference is the interaction effect $\beta_3$, which tells us how different the distance-size relationship is between the two groups. The coefficients $\beta_2$ and $(\beta_2 + \beta_3)$ can be interpreted as an estimate of the mean "similarity" (i.e. the mean distance-size association) in the two groups, and $\beta_3$ yields a measure of the difference in the size of this relationship.

A closely related specification adds an interaction with the inverse normalization weight into the original model with cosine similarity as the dependent variable:
$$
\begin{aligned}
\cos(A_i,B_i) &= \beta_2 + \beta_3X_i + \beta_0\frac{1}{\text{LNW}_i} + \beta_1\frac{X_i}{\text{LNW}_i} + \eta_i \\
\end{aligned}
$$
The numbering of the coefficients reflects the relationship between this model and the inner product regression coefficients; both equations estimate the same model, although each model imposes a different interpretation on the coefficients that correspond to each other: $\beta_1$ and $\beta_3$ swap interpretations as the coefficient on the grouping variable $X$ and the coefficient on the group-scale interaction term, while $\beta_0$ and $\beta_2$ swap interpretations as the intercept and the coefficient on the scaling variable. The key difference in the models is whether the estimation focuses on the normalization weight or its reciprocal, equivalent to deciding whether the analysis should be performed on the original vector space or its canonical cosine-normalized projection.

An advantage of this corrected ratio model is the error term $\eta_i$. The error term of the inner product regression is by assumption a function of the normalization weight ($v_i = \epsilon_i\text{LNW}_i$) whereas the corrected model need not make this assumption (Firebaugh & Gibbs 1986). This means that the inner product regression estimate is typically heteroskedastic. In general the two models give similar answers, so the practical implications of this difference are small, although the fit of one model tends to be better. When the inner product regression is preferable, a heteroskedasticity correction can be employed to compute standard errors. Researchers can also directly examine the change in error variance over the normalization weight distribution using the typical Lagrange multiplier test or a related analysis (Breusch \& Pagan 1979; also see Cook \& Weisberg 1983; Western & Bloome 2009).

A central question that this model raises is whether to include lower-order terms for the vector norms that make up the local normalization weight. In general one wants to do this, but depending on the research design this may introduce exact multicollinearities into the corrected inner product regression.  A common issue is that the researcher has chosen sets of word vectors that imply different relative word frequency distributions (Antoniak \& Mimno 2018; Ethayarajh, Duvenaud \& Hirst 2018). In practice the similarity structure determines how this frequency heterogeneity impacts the estimate of interest. This is because the design of the comparison may lead to word vectors and their norms reoccurring in multiple model terms. I will discuss this issue further as it arises in the context of other models in the cosine similarity regression family.

## Some useful decompositions

*Total scale distortion.* The frequency bias between $\beta_1$ and $\alpha^*_1$ can be decomposed into scale distortion terms reflecting the contribution of the omitted intercept and the omitted main effect of the grouping variable, modified by the partial effect on the outcome of, respectively, the normalization weight function and the group-conditional normalization weight function (Bartlett and Partnoy 2020):
$$
\alpha^*_1 = \beta_1 + \beta_2\left(\frac{cov(X_i, \frac{1}{\text{LNW}_i})}{var(X_i)}\right) + \beta_3\left(\frac{cov(X_i, \frac{X_i}{\text{LNW}_i})}{var(X_i)}\right)
$$  
The decomposition shows why it is usually helpful to think of the regression bias due to frequency in terms of distortion rather than as a bias in the estimate per se. Even in the simplest case, there are multiple sources of distortion that can cancel out or amplify depending on the particular dataset we happen to be working with. This means that the biased coefficient $\alpha^*_1$ can be close to $\beta_1$ even when the amount of distortion due to each omission is comparatively large. Typically $\beta_2$ and the lower-order partial effect are positive, so this situation arises when the signs of $\beta_3$ and $cov(X_i, \frac{1}{\text{LNW}_i})$ conflict. The absolute amount of distortion is usually a better measure of total "bias" than the equation above suggests due to this property of the uncorrected ratio model.

*Inner product partition.* The inner product regression model factors out a subspace-specific proportion of the inner product that is (by assumption) unrelated to the normalization weight distribution. This results in a modeled quantity that is similar in interpretation to cosine similarity, as well as an estimate of the amount each group of inner products should be discounted prior to applying a similarity interpretation because it is uncorrelated with the subspace-specific normalization weight function associated with this dataset.^[Word vector subspaces of the size typically used for word embedding analysis in the social sciences are rarely full rank with respect to the global embedding, even when the number of sampled vectors is large.] To see this, consider the inner product regression model when $X_i = 0$: $\left<A_i,B_i\right> = \beta_0 + \beta_2\text{LNW}_i + v_i$. This model implies that up to some error and minus an estimated adjustment $\beta_0$, $\left<A_i,B_i\right> \propto \beta_2\text{LNW}_i$. ($\beta_1$ and $\beta_3$ provide the additive intercept and slope changes for the $X_i = 1$ case.) In other words, by moving the normalization weight function to the other side of the model, the analysis pivots to estimating two different similarity distributions over the local inner product subspaces implied by each group of vectors, rather than assuming that these distributions are known in advance.

*Local similarity estimates.* This partition of the inner product suggests a model of similarity that discounts the estimated inner product by some group-specific amount and applies the normalizing projection only to the proportion of the variation in the inner product that we have estimated we can attribute to the variation in normalization weights. An observation-wise similarity coefficient can be produced by subtracting the estimated group-specific intercept of the inner product regression from each observation's inner product, then dividing this quantity by the normalization weight for this observation:  
$$
\text{sim}_{local}(A_i, B_i) = \frac{\left<A_i, B_i\right> - (\beta_0 + \beta_1X_i)}{||A_i||||B_i||}
$$  
The discrepancy between this quantity and $\cos(A_i, B_i)$ is a curvilinear function of the normalization weight (recall the dual interpretation of $\beta_0$ as the lower-order normalization weight coefficient in the ratio model), and the lower-order coefficient on the normalization weight in the component model $\beta_2$ is an estimate of the mean of this distribution. Notice that $\text{sim}_{global}(A_i, B_i) = (\beta_0 + \beta_1X_i)/||A_i||||B_i||$ is also a similarity coefficient. Another way of writing the cosine similarity when there is a known grouping is in terms of a decomposition into a local similarity component and a global discrepancy component:  
$$
\cos(A_i, B_i) = \text{sim}_{local}(A_i, B_i) + \delta(A_i, B_i)
$$  
Assuming that $\beta_0 = \beta_1 = 0$ in the uncorrected cosine similarity regression forces the second similarity term to zero. This amounts to an assumption that all of the similarity between A and B must be related to the grouping variable. The main consequence of this assumption is that $\cos(A_i, B_i)$ is always inflated with respect to the grouping if the ratio is computed in the global basis. Employing the corrected model makes it possible to recover an estimate of $\text{sim}_{local}(A_i, B_i)$ that allows some of the global similarity to be unrelated to the local comparison the model is meant to test.

# Common cosine similarity regressions

Researchers often employ few closely related cosine similarity regression models that I discuss in more depth in this section. The expanded family of models often involves two choices: (1) a *bivariate* cosine similarity regression incorporating a cosine ratio-valued independent variable, potentially involving the same set of reference vectors as the outcome cosine; and (2) a structured pre-processing of the vector sets to produce *composed* vectors prior to inducing a set of similarities. Researchers sometimes combine these analytic decisions, resulting in a variety of linked models.

Two linked themes will emerge over the course of this section. First, a significant part of the challenge of this family of methods is that the underlying data structure is a similarity structure over the set of vectors (Cormack 1971). Consequently, the similarities tend to have a complex clustering structure that should be taken into account when attempting to model differences in similarity. Second, applying exact semantic arithmetic operations prior to regression analysis significantly affects the geometry of the resulting estimation problem. Linear combinations of vectors exacerbate the frequency distortion problems associated with cosine regression because they imply additional reprojections of the vector space that are unrealistically strictly homogeneous given the underlying choice of vectors in the composition. These pre-aggregated models imply disaggregated models that tend to yield more robust estimates of associations of interest, at the cost of increasing the complexity of the modeling side of explanations of sociocultural lexical variation.

## Bivariate cosine similarity regression

Researchers sometimes estimate a linear relationship between two sets of cosine similarities directly, leading to an analogous set of omitted variable errors in the resulting model:  
$$
\begin{aligned}
\cos(A_i, B_i) &= \gamma^*_0 + \gamma^*_1\cos(C_i, D_i) + \epsilon_i \\
\left<A_i, B_i\right>&= \gamma_0\text{LNW}(A_i, B_i) + \gamma_1\cos(C_i, D_i)\text{LNW}(A_i, B_i) + v_i \\
&+ \color{red}{\gamma_2 + \gamma_3\cos(C_i, D_i) + \gamma_4\text{LNW}^{-1}(C_i, D_i) + \gamma_5\left<C_i, D_i\right>} \\
&+ \color{red}{\gamma_6\left<C_i, D_i\right> \text{LNW}(A_i, B_i) + \gamma_7\frac{\text{LNW}(A_i, B_i)}{\text{LNW}(C_i, D_i)}}
\end{aligned}
$$
In addition to the zeroed $\gamma_2$ and $\gamma_3$ coefficients, the bivariate cosine similarity regression omits terms for the lower-order components of the right cosine ($\gamma_4$, $\gamma_5$). Additionally, $\gamma_6$ captures the potential interaction of the right inner product with the left normalization weight; intuitively, this is an estimate of how different we think the distance association is at different locations in the left frequency distribution. Analogously, $\gamma_7$ assesses whether there is a difference in the scale association over the distribution of left normalization weights. These terms are particularly important to include if the dependent and independent cosines share a component word vector (i.e. if the incorrect specification is of the form $\cos(A_i, B_i) = \gamma^*_0 + \gamma^*_1\cos(A_i, C_i) + \epsilon_i$). This occurs often in applied settings. I discuss this model in more detail below.

## Cosine similarity regression with composed vectors

So far we have explored the cosine similarity regression $\mathbb{E}[\cos\left(\frac{1}{|A|}\sum_{|A|}A_i, \frac{1}{|B|}\sum_{|B|}B_i\right)|X_i]$ only for the case where $|A| = |B| = 1$. In applied settings, researchers tend to operationalize discursive concepts by using fixed word lists to induce a set of vectors related to a concept of interest. To represent the concept, researchers aggregate the set of vectors that results from a word list into a composed vector (e.g. the mean vector) and use the cosine similarity to these fixed points as a quantity of interest. This feature of the model space is important to characterize because it appears commonly in applied settings. This strategy tends to intensify the frequency bias relative to a direct analysis of the component vectors.

Comparisons of this type can be factored into a set of inner products between the $j, k$ selected sets of component vectors in the original vector space. A particularly common version of this model arises when the collection of cosine similarities is derived from comparisons of mean vectors composed of $j$ word vectors to members of a set of $n$ word vectors, where $|A| > |B| = 1$. (The paper by Nelson [2021] is an example of this type of model.) The case with one additional vector, $\cos(A+B, C)$, provides a useful basic case to show how this analytic choice contributes to frequency bias. First, observe that the normalization weight function implied by adding the vectors includes the pairwise component vector inner products:  
$$
\begin{aligned}
\cos(A+B, C) &= \frac{\sum_{i}(A_i+B_i)(C_i)}{\sqrt{\sum_i(A_i+B_i)^2}\sqrt{\sum_i(C_i)^2}} \\
&= \frac{\sum_{i}(A_i+B_i)(C_i)}{\text{LNW}(A+B, C)}\\
&= \frac{\sum_{i}(A_iC_i+B_iC_i)}{\text{LNW}(A+B, C)}\\
&= \frac{\sum_{i}A_iC_i} {\text{LNW}(A+B, C)}+\frac{\sum_{i}B_iC_i}{\text{LNW}(A+B, C)}.\\
\end{aligned}
$$  
$$
\begin{aligned}
\text{LNW}(A+B, C) &= \sqrt{\sum_i(A^2_i+B^2_i + 2A_iB_i)}\sqrt{\sum_iC^2_i}\\
&= \sqrt{\sum_iA^2_i+\sum_iB^2_i + 2\sum_iA_iB_i}\sqrt{\sum_iC^2_i} \\
&= \sqrt{||A||^2_2+||B||^2_2 + 2\left<A, B\right>}||C|| \\
&= g(A_i, B_i)\ ||C_i||
\end{aligned}
$$  
Note that $g(A_i, B_i)$ is a function of the inner product between $A$ and $B$ in addition to their respective norms. This leads to a different expansion of the uncorrected cosine model into the inner product regression:  
$$
\begin{aligned}
\cos(A_i+B_i, C_i) &= \alpha^*_0 + \alpha^*_1X_i + \epsilon_i \\
\frac{\left<A_i+B_i, C_i\right>}{\text{LNW}(A+B, C)} &= \alpha^*_0 + \alpha^*_1X_i + \epsilon_i \\
\left<A_i+B_i, C_i\right> &= \alpha^*_0\text{LNW}(A+B, C) + \alpha^*_1X_i\text{LNW}(A+B, C) + \epsilon_i\text{LNW}(A+B, C) \\
\left<A_i+B_i, C_i\right> &= \alpha^*_0g(A_i, B_i)\ ||C_i|| + \alpha^*_1X_ig(A_i, B_i)\ ||C_i|| + v_i \\
&+ \color{red}{\alpha^*_2 + \alpha^*_3X_i + \alpha^*_4g(A_i, B_i) + \alpha^*_5||C_i||} \\
&+ \color{red}{\alpha^*_6X_ig_{A,B}(A, B) + \alpha^*_7X_i||C_i||} \\
\end{aligned}
$$  
Including an additional vector on one side of the cosine ratio outcome implies a model that omits four additional variables by fixing their coefficients to zero. In addition to the missing intercept and lower-order grouping variable term as in the vector-wise analysis, a key issue is determining whether the components of the normalization weight have distinct (conditional) associations with the transformed inner product.

Extending to $j$ vectors:  
$$
\begin{aligned}
\cos(A_1+...+A_j, X) &= \cos(\Omega, X) = \frac{\sum_{j}\sum_{i}(\Omega_{ji}X_i)}{\sqrt{\sum_i(\sum_j\Omega_{ji})^2}\sqrt{\sum_iX_i^2}} \\
&\left(= \frac{\sum_{i}(A_1X_i+...+A_jX_i)}{\sqrt{\sum_i(A_1+...+A_j)^2}\sqrt{\sum_iX_i^2}} \right)\\
\sqrt{\sum_i(\sum_j\Omega_{ji})^2} &= \sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}.\\
\text{LNW}(\Omega, X) &= \sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}\sqrt{\sum_iX_i^2}
\end{aligned}
$$  
This cosine similarity is a weighted average of the inner products between the target vector $X$ and each of the component vectors $\Omega_j$. The normalization weight is a function of the squared norms of each of the component vectors and their pairwise inner products with each other. Observe two facts about this weight. First, in each inner product the familiar normalization weight $||\Omega_{j}||||\Omega_{j*}||$ reappears again. Second, adding the $j$th word vector to $\Omega$ implies adding $j - 1$ more comparisons to the weight function. Pre-composing the word vectors in $A$ to facilitate the comparison to each $X_i$ thus implies computing all of the pairwise inner products $\left<A_i, A_j\right>$, but without accounting for the uncertainty associated with this measure. The corresponding inner product regression is as follows:
$$
\begin{aligned}
\cos(\Omega_i, X_i) &= \gamma^*_0 + \gamma^*_1D_i + \epsilon_i \\
\left<\Omega_i, X_i\right>&= \gamma_0\text{LNW}(\Omega_i, X_i) + \gamma_1D_i\text{LNW}(\Omega_i, X_i)  + v_i \color{red}{+ \gamma_2 + \gamma_3D_i} \\
\sum_j\left<\Omega_{ji}, X_i\right>&= \gamma_0 \sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}\sqrt{\sum_iX_i^2} \\
&+ \gamma_1D_i\sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}\sqrt{\sum_iX_i^2} + v_i \color{red}{+ \gamma_2 + \gamma_3D_i} \\
\end{aligned}
$$
The left hand side of the model decomposes into a sum of the component inner products. The implicit model for a given comparison can be constructed by moving the $j-1$ alternative comparisons to the right hand side of the regression equation:
$$
\begin{aligned}
\left<\Omega_{ji}, X_i\right>&= \gamma_0 \sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}\sqrt{\sum_iX_i^2} \\
&+ \gamma_1D_i\sqrt{\sum_j||\Omega_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega_{j}, \Omega_{j*}\right>}\sqrt{\sum_iX_i^2} \\
&+ \gamma_2 + \gamma_3D_i + v_i \\
&+ \color{purple}{\xi_1\left<\Omega_{1,i}, X_i\right> + \cdots + \xi_{j-1}\left<\Omega_{(j-1),i}, X_i\right>}
\end{aligned}
$$
The inner product regression and the constrained/uncorrected model alike assume that the linked inner product coefficients $\xi_j = -1$. Additionally, the zero conditional mean error assumption in this model requires the errors of these distances to be mutually uncorrelated. This strict homogeneity assumption is *very* demanding in light of the empirical observation that semantic vector spaces exhibit frequency anisotropy (Mu \& Viswanath 2018). Note that the bias in cosine similarity induced by pooling the $A$ vectors and constructing normalization weights in this way is not addressed by correcting the normalization weight adjustment.

An additional problem with this model is that the grouping variable is (by design) not independent of the normalization weight distribution. $||\Omega^{X_i}||$ and $D_i$ are exactly collinear, implying the comparison as constructed cannot differentiate whether the coefficient $\gamma_3$ reflects the grouping of vectors $X_i$ corresponding to $\Omega^{X_i}$ or the length of the composed vector. This relates to the question of whether a model of similarity should allow $||A||$ and $||B||$ to have separable associations with the inner product. I will discuss this issue in depth in the context of a more complex bivariate model.

## Composed cosines are biased estimates of the subspace mean cosine similarity

How should researchers think about the homogeneity assumption implicit in comparing quantities like $\cos(\Omega, X_i)$? One way to break down this model is to think of it as a specific *estimate* of the subspace-specific (i.e. conditional) mean cosine similarity $\mathbb{E}[\cos(\Omega_j, X_i)|J=j]$ implied by the word vectors that make up the composed vector, or alternatively $\mathbb{E}[\left<\Omega_j, X_i\right>|LNW_{ij}, J=j]$. The pre-computed plug-in estimate obtained by taking the cosine of a reference vector $X_i$ with the mean vector $\Omega$ is a biased estimate of this conditional expectation:
$$
\begin{aligned}
\cos\left(\frac{1}{N}\sum_j\Omega_{ji}, X_i\right) &= \frac{1}{N}\sum_j\cos(\Omega_{ji}, X_i) + \delta_{ji} \\
\frac{\left<\frac{1}{N}\sum_j\Omega_{ji}, X_i\right>}{||N^{-1}\sum_j\Omega_{ji}||||X_i||} &= \frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>}{||\Omega_{ji}||||X_i||} + \delta_{ji} \\
\frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>}{||N^{-1}\sum_j\Omega_{ji}||||X_i||} &= \frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>}{||\Omega_{ji}||||X_i||} + \delta_{ji} \\
\frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>}{||N^{-1}\sum_j\Omega_{ji}||} &= \frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>}{||\Omega_{ji}||} + ||X_i||\delta_{ji} \\
\frac{1}{N}\sum_j\left<\Omega_{ji}, X_i\right> &= \frac{1}{N}\sum_j\frac{\left<\Omega_{ji}, X_i\right>||N^{-1}\sum_j\Omega_{ji}||}{||\Omega_{ji}||} + \frac{1}{N}\sum_j\delta_{ji}||X_i||||N^{-1}\sum_j\Omega_{ji}|| \\
\end{aligned}
$$
This subspace-specific scale bias can be expressed in terms of a decomposition of the mean component-reference inner product into a weighted mean of these inner products plus an additive comparison-specific external scale biasing term. The elementwise scale weighting function $\psi^{I}_{ij} = \frac{||N^{-1}\sum_j\Omega_{ji}||}{||\Omega_{ji}||}$ must be distributed with mean one and the overall bias with respect to the reference vector $X_i$ $\psi^{II}_{ij} = \delta_{ji}||X_i||||N^{-1}\sum_j\Omega_{ji}||$ must be mean zero for this model to be undistorted. The strictly homogeneous case is $\delta_{ij} = 0$ with norm ratios $\frac{||\Omega_i||}{||\Omega_{ij}||}$ uniformly equal to one; this is the zero-variance case we assume by using the plug-in similarity estimate. Because the scale distortion is complex, a large number of situations may result in approximately zero net frequency bias. On average the bias of the estimator increases with the number of components in the mean vector; mean vectors are mechanically shortened by the number of component vectors, resulting in a lower scale ratio due to the researcher's choice of subspace size (i.e. number of component vectors). So, the composed cosine tends to overstate the mean subspace cosine. Often the "worse" biasing component of the two is the reference-specific bias term; the distribution of this term over the entire reference set $X$ is of interest. The homogeneous estimator can be a reasonable assumption within a given subspace depending on how the construction of the vector affects the correlation structure of its components. But, the implied projection often underperforms with respect to multiple external reference comparisons because the bias can vary considerably over different choices of $X$.

A minimal alternative procedure is to estimate a separate model for each of the $j$ variables in the composed vector, so that the model parameters can vary from comparison to comparison rather than presuming a single fixed parameter across the entire set. This has the advantage of considerably simplifying the interpretation of the normalization weighting procedure. The inclusion of the $\xi_j$ terms is also suggestive of the bivariate cosine regression (i.e. on average, how mutually predictable are the pairwise distances in $\Omega$ to $X$, conditional on frequency?). Alternatively, researchers could estimate a multivariate linear model with the set of inner products as the outcome vector, which combines both approaches.

```{r kte, eval=F, include=F}
# Show 2-7 composed vectors
j <- 2000
ak <- kemb[sample(rownames(kemb), j),]
bk <- kemb[sample(rownames(kemb), j),]
ck <- kemb[sample(rownames(kemb), j),]
dk1 <- kemb[sample(rownames(kemb), j),]
dk2 <- kemb[sample(rownames(kemb), j),]
dk3 <- kemb[sample(rownames(kemb), j),]
dk4 <- kemb[sample(rownames(kemb), j),]
dk5 <- kemb[sample(rownames(kemb), j),]

data.frame(i=1:2000) %>%
  rowwise() %>%
  mutate(an = norm(ak[i,], "2"),
         bn = norm(bk[i,],"2"),
         abn = norm(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]))), "2"),
         cn = norm(ck[i,], "2"),
         ip = as.numeric(ak[i,]) %*% as.numeric(bk[i,]),
         scs = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]))), as.numeric(ck[i,])),
         acs = lsa::cosine(as.numeric(ak[i,]), as.numeric(ck[i,])),
         bcs = lsa::cosine(as.numeric(bk[i,]), as.numeric(ck[i,])),
         mcs = mean(c(acs, bcs)),
         d1cs = lsa::cosine(as.numeric(dk1[i,]), as.numeric(ck[i,])),
         d2cs = lsa::cosine(as.numeric(dk2[i,]), as.numeric(ck[i,])),
         d3cs = lsa::cosine(as.numeric(dk3[i,]), as.numeric(ck[i,])),
         d4cs = lsa::cosine(as.numeric(dk4[i,]), as.numeric(ck[i,])),
         d5cs = lsa::cosine(as.numeric(dk5[i,]), as.numeric(ck[i,])),
         scs2 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]))), as.numeric(ck[i,])),
         scs3 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]))), as.numeric(ck[i,])),
         scs4 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]))), as.numeric(ck[i,])),
         scs5 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]))), as.numeric(ck[i,])),
         scs6 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]), as.numeric(dk5[i,]))), as.numeric(ck[i,])),
         abn6 = norm(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]), as.numeric(dk5[i,]))), "2"),
         mcs2 = mean(c(acs, bcs, d1cs)),
         mcs3 = mean(c(acs, bcs, d1cs, d2cs)),
         mcs4 = mean(c(acs, bcs, d1cs, d2cs, d3cs)),
         mcs5 = mean(c(acs, bcs, d1cs, d2cs, d3cs, d4cs)),
         mcs6 = mean(c(acs, bcs, d1cs, d2cs, d3cs, d4cs, d5cs))) ->
  omegah

# Plot bias in cos(mean(X), Y) as an estimator of mean(cos(X, Y))
omegah %>%
  ggplot(aes(x=scs, y=mcs, color=abn)) +
  geom_point() +
  scale_color_viridis_c() +
  geom_smooth(method="lm", alpha=0.5) +
  geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
  labs(x=latex2exp::TeX("$\\cos(\\Omega(2), X_i)$"),
       y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))
omegah %>%
  ggplot(aes(x=scs6, y=mcs6, color=abn6)) +
  geom_point() +
  scale_color_viridis_c() +
  geom_smooth(method="lm", alpha=0.5) +
  geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
  labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
       y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))

# Color points by mean composed/component norm ratio
# The bias goes the other way when this value is high (?)
omegah %>%
    rowwise() %>% mutate(ascn=abn/an, bscn=abn/bn, cl = mean(c(ascn, bscn))) %>%
    ggplot(aes(x=scs, y=mcs, color=cl)) +
    geom_point() +
    scale_color_viridis_c() +
    geom_smooth(method="lm", alpha=0.5) +
    geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
    labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
         y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))

# Color points by external composed normalization weight ||X_i||,||Omega(n)||
omegah %>%
    rowwise() %>% mutate(ascn=abn/an, bscn=abn/bn, cl = mean(c(ascn, bscn)), elnw=abn*cn) %>%
    ggplot(aes(x=scs, y=mcs, color=cl, size=elnw)) +
    geom_point() +
    scale_color_viridis_c() +
    geom_smooth(method="lm", alpha=0.5) +
    geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
    labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
         y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))

# Joint distribution of ratio abn/an, abn/bn
omegah %>%
  rowwise() %>%
  mutate(cl = mean(c(ascn, bscn))) %>%
  ggplot(aes(x=log(ascn), y=log(bscn), color=cl)) +
  geom_point() +
  scale_color_viridis_c()
```

## Bivariate composed cosine similarity regression

The composed cosines $\Omega^{\circ}$ can be substituted into the bivariate cosine regression to yield a combined model:

$$
\begin{aligned}
\cos(\Omega^{A}, Y_i) &= \gamma^*_0 + \gamma^*_1\cos(\Omega^{B}, X_i) + \epsilon_i \\
(\cos(A_1+A_2+..., Y_i) &= \gamma^*_0 + \gamma^*_1\cos(B_1+B_2+..., X_i) + \epsilon_i) \\
\left<\Omega^{A}, Y_i\right>&= \gamma_0\text{LNW}(\Omega^{A}, Y_i) + \gamma_1\cos(\Omega^{B}, X_i)\text{LNW}(\Omega^{A}, Y_i) + v_i \\
&+ \color{red}{\gamma_2 + \gamma_3\cos(\Omega^{B}, X_i) + \gamma_4\text{LNW}^{-1}(\Omega^{B}, X_i) + \gamma_5\left<\Omega^{B}, X_i\right>} \\
&+ \color{red}{\gamma_6\left<\Omega^{B}, X_i\right> \text{LNW}(\Omega^{A}, Y_i) + \gamma_7\frac{\text{LNW}(\Omega^{A}, Y_i)}{\text{LNW}(\Omega^{B}, X_i)}}
\end{aligned}
$$
Each of the right inner product terms ($\gamma_1, \gamma_3, \gamma_5, \gamma_6$) decomposes further into a sum of componentwise inner products (teal terms below). In the uncorrected model all but $\gamma_1$ are zeroed out, but otherwise each of these coefficients is forced to be equal across the marginal and joint inner product and left normalization weight distributions. Additionally, the model incorporates a left-hand mean vector, so the $\xi_j = -1$ constraint applies here as well (purple).
$$
\begin{aligned}
\left<\Omega^{A}_j, Y_i\right>&= \gamma_0\text{LNW}(\Omega^{A}, Y_i) + v_i \\
&+ \color{red}{\gamma_2 + \gamma_4\text{LNW}^{-1}(\Omega^{B}, X_i) + \gamma_7\frac{\text{LNW}(\Omega^{A}, Y_i)}{\text{LNW}(\Omega^{B}, X_i)}} \\
&+ \color{teal}{\gamma_1\cos(B_1, X_i)\text{LNW}(\Omega^{A}, Y_i) + \gamma_1\cos(B_2, X_i)\text{LNW}(\Omega^{A}, Y_i) + ...} \\
&+ \color{teal}{\gamma_3\cos(B_1, X_i) + \gamma_3\cos(B_2, X_i) + ...} \\
&+ \color{teal}{\gamma_5\left<B_1, X_i\right> + \gamma_5\left<B_2, X_i\right> + ...}  \\
&+ \color{teal}{\gamma_6\left<B_1, X_i\right> \text{LNW}(\Omega^{A}, Y_i) + \gamma_6\left<B_2, X_i\right> \text{LNW}(\Omega^{A}, Y_i) + ...} \\
&+ \color{purple}{\xi_1\left<\Omega^A_{1,i}, X_i\right> + \cdots + \xi_{j-1}\left<\Omega^A_{(j-1),i}, X_i\right>}
\end{aligned}
$$


## Correlated bivariate composed cosine similarity regression 

Up to this point I have assumed that the underlying word vectors are not mechanically associated. This assumption is often violated in practice. The behavior of the model when $Y_i = X_i$ is of special interest. This is a common setup when the interest is in comparing how two concepts (represented by a set of vectors or a composed vector) relate to a common set of words. I focus on the composed case. Continuing the previous example, we are often comparing two mean vectors $\Omega^A$ and $\Omega^B$ to the same set of vectors $Y$. The data for this comparison consist of a set of pairwise correlated cosines, $\{\cos(\Omega^A, Y_i), \cos(\Omega^B, Y_i)\}_i$, and the corresponding normalization weights are $\{||\Omega^A||||Y_i||, ||\Omega^B||||Y_i||\}_i$. The key phenomenon to observe in the model is that the normalization weight factor for the vector $X_i$ cancels from the scale ratio terms with coefficients $\gamma_1$ and $\gamma_7$ (orange):^[For clarity of notation I have omitted the expansion for $\gamma_1$ in the specification, but it should not be overlooked that this term is an estimate of the mean inner product over $B$. The dependence on $\Omega^B$ implies a similar strict homogeneity assumption to the LHS cosine.]
$$
\begin{aligned}
\cos(\Omega^{A}, X_i) &= \gamma^*_0 + \gamma^*_1\cos(\Omega^{B}, X_i) + \epsilon_i \\
\left<\Omega^{A}_j, X_i\right>&= \gamma_0\text{LNW}(\Omega^{A}, X_i) + v_i \\
&+ \color{red}{\gamma_2 + \gamma_4\text{LNW}^{-1}(\Omega^{B}, X_i)} \\
&+ \color{teal}{\gamma_3\cos(B_1, X_i) + \gamma_3\cos(B_2, X_i) + ...} \\
&+ \color{teal}{\gamma_5\left<B_1, X_i\right> + \gamma_5\left<B_2, X_i\right> + ...}  \\
&+ \color{teal}{\gamma_6\left<B_1, X_i\right> \text{LNW}(\Omega^{A}, Y_i) + \gamma_6\left<B_2, X_i\right> \text{LNW}(\Omega^{A}, Y_i) + ...} \\
&+ \color{purple}{\xi_1\left<\Omega^A_{1,i}, X_i\right> + \cdots + \xi_{j-1}\left<\Omega^A_{(j-1),i}, X_i\right>} \\
&+ \color{orange}{\gamma_1\frac{\left<\Omega^{B}, X_i\right> \sqrt{\sum_j||\Omega^{A}_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega^{A}_{j}, \Omega^{A}_{j*}\right>}\cancel{\sqrt{\sum_iX_i^2}}}{ \sqrt{\sum_j||\Omega^{B}_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega^{B}_{j}, \Omega^{B}_{j*}\right>}\cancel{\sqrt{\sum_iX_i^2}}}} \\
&+ \color{orange}{\gamma_7\frac{\sqrt{\sum_j||\Omega^{A}_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega^{A}_{j}, \Omega^{A}_{j*}\right>}\cancel{\sqrt{\sum_iX_i^2}}}{\sqrt{\sum_j||\Omega^{B}_{j}||^2_2 + 2\sum_{j,j*}\left<\Omega^{B}_{j}, \Omega^{B}_{j*}\right>}\cancel{\sqrt{\sum_iX_i^2}}}}
\end{aligned}
$$
The cancellation induced by this design implies that the model fails to identify the desired right cosine-left scale interaction. The term $\gamma_1$ averages the three-way interactions of the right inner product with the scale ratio ($\gamma_5, \gamma_7$), the interaction of the right cosine with the left normalization weight ($\gamma_0, \gamma_3$), *or* the second-order interaction of the right (reciprocal) normalization weight and the interaction of the right inner product and the left normalization weight ($\gamma_4, \gamma_6$). But the interpretation of the variable this coefficient refers to is not the same across the lower-order terms.

Kozlowski et al. (2019) employ a model of this type with an additional correlated cosine similarity regressor sharing the reference vector set $X$, leading to a trivariate (additive) composed cosine regression. An alternative trivariate inner product regression is discussed in more detail below.


## Correlated binary cosine similarity regression 

Returning to a simpler model may help to clarify why the correlated regression generates frequency distortion. The score component $S(w_i, A, B) = \mathbb{E}[cos(w_i, X_i)|X_i \in \{A, B\}]$ of the Word Embedding Association Test (Caliskan, Bryson \& Narayanan 2017) is a limiting case of the binary cosine similarity regression model where the comparison vectors $X_i$ are separated by $D_i$ and the reference vectors $w_i$ are not. This known dependence structure in the local normalization weight function implies that the lower-order interactions $D_i||w_i||$ and $D_i||X_i||$ and the main norm terms $||w_i||$ and $||X_i||$ are of interest. In addition the model assumes that the difference in distances is completely controlled by the distance-scale association and that this association passes through the origin. As in the simple case,


$$
\begin{aligned}
\cos(w_i, X_i) &= \alpha^*_0 + \alpha^*_1D_i + \epsilon_i \\
\left<w_i, X_i\right> &= \alpha^*_0\text{LNW}_i + \alpha^*_1D_i\text{LNW}_i + \epsilon_i\text{LNW}_i \\
\left<w_i, X_i\right> &= \alpha^*_0\text{LNW}_i + \epsilon_i\text{LNW}_i \\
&+ \color{blue}{\alpha^*_1D_i||w_i||||X_i||} \\
&+ \color{red}{\alpha^*_2 + \alpha^*_3D_i} \\
\end{aligned}
$$
A key limitation of the computation of the score is that it implies a separate model for each $w_i$, implying the term $||w_i||$ is zero variance; this makes the normalization weight terms of each individual difficult to interpret on their own, because the lack of variance in the vector set implies we cannot estimate the uncertainty associated with applying the norm weight in each model. The WEAT statistic $S(X, Y, A, B)$ compares the difference in means of the vectors of uncorrected interaction coefficients $\alpha^*_1(i), \alpha^*_1(j)$ across the word sets $X(i)$ and $Y(j)$, resulting in a nested linear model. The sampling distributions of these coefficients are bilinearly correlated, and each of the word sets generally imply different frequency distributions. This complex dependency structure in the resulting nested model propagates the bias in the component scores.


## Weight function measurement error

A recurrent feature of this type of analysis is that the local normalization weight function is estimated from the same vectors as the inner product. One could reasonably object to this procedure. In the context of regression analysis, when the local normalization weight is introduced into the other side of the model, the model fit will usually increase (depending on sample size, etc.). Some of this added explanatory power is due to the fact that the normalization projection errors are correlated.^[A closely related issue is that word embedding analyses are sensitive to the intrinsic frequency bias of the word count sampling procedure. The large number of low-frequency words implies an unobserved set of related words that could have been included, and there is a steep bias-variance tradeoff involved in selecting the minimum frequency window. Both issues motivate considering an external word frequency estimate. Researchers usually use the observed word count above a threshold in a corpus of documents as a plug-in estimate of the probability of observing a word, but this estimate is frequency biased because there is heightened measurement error in the low frequency region and a sharp discontinuity at the threshold. The threshold is the part of the remaining vector space that is the most affected by the size-biased sampling problem. An implication of this idea is that cutting off the model at a fixed threshold can seriously impact the qualitative interpretation of the model, because this region of terms is also more specific on average. When the research goal is to measure concepts by identifying subspaces of the vector space, it is important to consider how qualitative interpretations of this space are affected by perturbations to this threshold. Additionally, denominator measurement error results in biased estimates, particularly when it is associated with $X_i$ (Casson 1973; Bartlett \& Partnoy 2020). This situation also creates difficulties for frequency adjustment because we do not usually have an external *group-specific* frequency estimate. In general researchers should prefer the heteroskedastic estimator because this model expresses an expectation that a similarity measure over word vectors "should" have frequency-dependent projection errors.] In the appendix, I use an external estimate of word frequency derived from the Corpus of Contemporary American English^[There is a variable temporal and geographic fit between this dataset and the corpora underlying any given application. The corresponding model is informative about what happens to regression analysis with word embeddings when a covariate is added that is variably correlated to the local normalization weight with non-random measurement error. However, the implications of using an external frequency reference that is more or less relevant to the target corpus should be considered.] to show that using external word frequency information results in a qualitatively similar estimate of the inner product regression model. Researchers will tend to see a considerable jump in $\text{R}^2$ and changes in the substantive and statistical significance of estimated quantities of interest when an estimate of word frequency is added. The variance of this model depends to some extent on how exactly the frequency information enters into the model and the degree of fidelity to the original word frequency distribution estimated from the corpus. The error in frequency estimates between the corpus count and the external count may also be of substantive interest.


# Two examples of frequency bias

This section demonstrates frequency bias in a few applications of word embeddings to problems in the social sciences. The studies were selected because (1) they construct an aggregate cosine similarity quantity from locally estimated classic word embeddings; (2) they span a wide range of disciplinary perspectives; and (3) they have publicly accessible and comprehensively documented replication materials. Frequency bias is a property of this general approach and is not unique to these papers or research areas. The body of work discussed in this paper is part of a long-term effort to adapt tools designed for making optimal predictions to the task of developing theoretical explanations (Hofman et al. 2021). If our goal is to improve our ability to observe and record social variation (Xie 2013), aggregation errors may be inevitable. The findings reported in this paper are indebted to the willingness of a wide community of researchers to take risks and experiment with unfamiliar methods.

It is important for researchers to notice that frequency bias can lead to results that *simultaneously understate and overstate* the substantive degree of association in term use. It is not the case that estimates will generally go to zero when frequency bias is accounted for. Rather, the word frequency distribution creates *heterogeneous* groupings of similarities in the data that cannot be ignored by taking the mean. Comparative methodologies of this type that do not account for this heterogeneity explicitly are merely averaging over it implicitly. Conversely, researchers can expect substantial improvements in the fit of models of discursive variation by accounting for frequency explicitly, but the interpretability of the resulting model is unclear. I will discuss this in greater detail with respect to the empirical applications below.

## “Leveraging the alignment between machine learning and intersectionality: Using word embeddings to measure intersectional experiences of the nineteenth century U.S. South.” (Nelson 2021)

```{r nelson_setup, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center"}
# Replication of "Measuring Intersectionality" (Nelson 2021 Poetics)
# Describes frequency bias in the canonical SGNS model in the paper
# See https://github.com/lknelson/measuring_intersectionality

# Load canonical embedding
nemb.d <- read_table2(here("data", "nelson2021", "word2vec_all_clean.txt"), col_names = FALSE, skip = 1)
# nemb.d <- read_table2(here("data", "nelson2021", "w2v_adjusted", "word2vec_w5_rs01.txt"), col_names = FALSE, skip = 1)
# nemb.d <- read_table2(here("data", "nelson2021", "w2v_adjusted", "word2vec_w5_rs1.txt"), col_names = FALSE, skip = 1)
nemb.tok <- nemb.d$X1
nemb <- nemb.d[,-1]  # 31362 x 100 embedding matrix
rownames(nemb) <- nemb.tok
sN <- nrow(nemb)
sp <- ncol(nemb)

# Fixed word lists
fwl_male <- c("men", "man", "boy", "boys", "he", "him", "his", "himself")
fwl_female <- c("women", "woman", "girl", "girls", "she", "her", "hers", "herself")
fwl_white <- c("white", "caucasian", "anglosaxon")
fwl_black <- c("black", "colored", "coloured", "negro", "negress", "negros", "afroamerican")

# Load corpus; get lexicon
n1 <- readtext(here("data", "nelson2021", "first-person-narratives-american-south", "data", "texts"))
n2 <- readtext(here("data", "nelson2021", "na-slave-narratives", "data", "texts"))
nc <- corpus(rbind(n1 %>% mutate(doc_id = paste0("a_", doc_id)),
                   n2 %>% mutate(doc_id = paste0("b_", doc_id))))
nt <- tokens(nc, remove_punct=T)  # Drop punctuation

# Form DFM and term frequency matrix
nd <- dfm_trim(dfm(nt), min_termfreq=10)
nt_freq <- textstat_frequency(nd)

# Word-word cofrequency matrix
ndf <- fcm(nd) 

# Also useful to have the norms laying around
nt_freq %>%
  filter(feature %in% nemb.tok) %>%
  rowwise() %>%
  mutate(snorm = norm(nemb[feature,], "2")) ->
  tfn
```

```{r nelson_angles_setup, echo=F, message=F}
# Now let's examine a sample of the inner product space created by this model
# Function to plot inner product manifold given a set of angles/frequencies
plot_angle_manifold <- function(r_angles) {
  r_angles %>%
    ungroup() %>%
    mutate(lfr = log(a$frequency) + log(b$frequency)) %>%
    arrange(lfr) %>%
    ggplot(aes(x=nprod, y=ab_cs)) +
    geom_point(aes(color=lfr, size=lfr), alpha=0.9) +
    geom_hline(yintercept=mean(r_angles$ab_cs), linetype="dashed") +
    scale_color_viridis_c() +
    geom_smooth(method="lm") +
    geom_smooth(color="tomato") +
    theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) +
    labs(x=latex2exp::TeX("$||A||*||B||$"), y=latex2exp::TeX("$cos(A, B)$"),
         color=latex2exp::TeX("$log(p(A)) * log(p(B))$"), size=latex2exp::TeX("$log(p(A)) * log(p(B))$"))
}

# Look at a uniformly random (wrt. term frequency) sample of term pairs
# Default is 5k term pairs (with replacement)
# aZ and bZ are something like LogSumExp<w,c> (maximum expected inner product to a context)
make_angles <- function(embmat, k=5000) {
  # sm <- sample(tfn$feature, k*2)
  sm <- sample(tfn$feature, k*2, replace=T)
  sA <- sm[1:k]
  sB <- sm[(k+1):(k*2)]
  data.frame(aterm=sA, bterm=sB) %>%
    # rowwise() %>%
    # mutate(anorm = norm(embmat[aterm,], "2"),
    #        bnorm = norm(embmat[bterm,], "2"),
    #        a = tfn[which(tfn$feature == aterm),"frequency"],
    #        b = tfn[which(tfn$feature == bterm),"frequency"],
    #        ab_cs = lsa::cosine(as.numeric(embmat[aterm,]), as.numeric(embmat[bterm,]))[1],
    #        ab_ip = as.numeric(embmat[aterm,]) %*% as.numeric(embmat[bterm,]),
    #        ppmcc = cor(as.numeric(embmat[aterm,]), as.numeric(embmat[bterm,])),
    #        nprod = anorm * bnorm,
    #        aZ = word2vec_similarity(as.numeric(embmat[aterm,]),
    #                                 as.matrix(embmat[-which(rownames(embmat) == aterm),]),
    #                                           top_n=1, type="cosine") %$% similarity * nprod,
    #        bZ = word2vec_similarity(as.numeric(embmat[bterm,]),
    #                                 as.matrix(embmat[-which(rownames(embmat) == bterm),]),
    #                                           top_n=1, type="cosine") %$% similarity * nprod,
    #        gK = -1 * (1 + nprod^2 + ab_cs^2)^-2) ->
    rowwise() %>%
    mutate(anorm = norm(embmat[aterm,], "2"),
           bnorm = norm(embmat[bterm,], "2"),
           a = tfn[which(tfn$feature == aterm),"frequency"],
           b = tfn[which(tfn$feature == bterm),"frequency"],
           ab_ip = as.numeric(embmat[aterm,]) %*% as.numeric(embmat[bterm,]),
           nprod = anorm * bnorm,
           ab_cs = ab_ip / nprod,
           pmi = log(as.numeric(ndf[aterm, bterm]))/(log(a$frequency)*log(b$frequency))) ->
    random_angles
  
  return(random_angles)
}

# Get term pair sample
ra_nemb <- make_angles(nemb, k=5000)
```

```{r nelson_fcomp_setup, echo=F, message=F}
# The analysis is based on arithmetic composites across the term lists
# Let's inspect these directly
# First, modify above functions to work on a composite vector we provide
view_from_composite <- function(embmat, fv) {
  embmat_ <- embmat
  if(is.data.frame(embmat)) {
    embmat_ <- as.matrix(embmat)
  }
  
  # dotp <- word2vec_similarity(fv, embmat, top_n=nrow(embmat))

  word2vec_similarity(fv, embmat_, top_n=nrow(embmat_), type="cosine") %>%
    mutate(angle = acos(similarity) * (180/pi)) %>%
    left_join(tfn %>% as.data.frame() %>% select(feature, frequency, snorm), by=c("term2"="feature")) %>%
    mutate(nprod = snorm * norm(fv, "2"),
           fprod = sqrt(log(frequency)) * snorm) ->
    test_1term
  return(test_1term)
}

add_composite_ip <- function(comp, embmat, fv) {
  comp %>%
    rowwise() %>%
    mutate(inner_product = fv %*% as.numeric(embmat[term2,]))
}

plot_composite_view <- function(csims, fv_label, topsel=NA, mv=F) {
  spflab <- ifelse(mv,
                   "$\\textit{%s}$ (mean vector)",
                   "$\\textit{%s}$")
  # Commented lines use sqrt(log(p(w))) * norm(focal vector) 
  csims %>%
    filter(similarity < 1) %>%  # Drop self-comparison if in data
    ggplot(aes(x=log(frequency), y=similarity, color=snorm)) +
    # ggplot(aes(x=fprod, y=similarity, color=snorm)) +
    geom_point() +
    geom_vline(xintercept=median(log(csims$frequency), na.rm=T), linetype="dotted", alpha=0.8) +
    # geom_vline(xintercept=median(csims$fprod, na.rm=T), linetype="dotted", alpha=0.8) +
    geom_hline(yintercept=mean(csims$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
    geom_smooth(method="gam", color="tomato") +
    geom_smooth(method="lm") +
    scale_color_viridis_c() +
    ggtitle(latex2exp::TeX(sprintf(spflab, fv_label))) +
    labs(x="Word frequency (log scale)",
         y="Cosine similarity",
         color=latex2exp::TeX("$||w_j||$")) -> plt
  geom_smooth(method="lm") +if(is.na(topsel)) {
    return(plt)
  } else {
    return(plt +
             geom_hline(yintercept=csims[topsel,"similarity"],
                        linetype="dotted", color="violetred2", size=2) +
             geom_smooth(method="lm", data=csims %>% arrange(desc(similarity)) %>% slice_head(n=50)))
  }
}

# For two fixed word lists, get mean of all paired sum vectors
# See Nelson (2021) p. 5
compute_fwl_meanvector <- function(embmat, fwl1, fwl2) {
  expand.grid(fwl1, fwl2) %>%
    rowwise() %>%
    summarize(embmat[which(rownames(embmat) == Var1),] + embmat[which(rownames(embmat) == Var2),]) %>%
    colMeans() ->
    mv_fwl
  return(mv_fwl)
}

# Compute the mean social identity vectors
mv_male_black <- compute_fwl_meanvector(nemb, fwl_male, fwl_black)
mv_female_black <- compute_fwl_meanvector(nemb, fwl_female, fwl_black)
mv_male_white <- compute_fwl_meanvector(nemb, fwl_male, fwl_white)
mv_female_white <- compute_fwl_meanvector(nemb, fwl_female, fwl_white)

# Compute the social institution inducing vectors
v_polity <- as.matrix(nemb["nation",] + nemb["state",])
v_economy <- as.matrix(nemb["money",])
v_culture <- as.matrix(nemb["culture",])
v_domestic <- as.matrix(nemb["housework",] + nemb["children",])
v_authority <- as.matrix(nemb["authority",])
```


Nelson (2021) presents a word embedding-based analysis of autobiographical narratives describing intersectional life experiences in the 19th century U.S. South. The study builds from a corpus of English "diaries, autobiographies, memoirs, travel accounts, and ex-slave narratives" as well as a number of "autobiographical narratives," "biographies," and "fictionalized" accounts by fugitive and former slaves. The word embedding analysis in the paper is built on a word2vec SGNS model with $p=100$ dimensions. The model employs a context window of size 5 and a minimum word frequency threshold of 10, and it is trained directly on the target corpus from scratch.

The key quantitative measures in the paper are based on cosine similarity between composed vectors. A set of \textit{social category vectors} is constructed by averaging pairs of vectors corresponding to a small number of fixed terms in lists of "women/men/Black/white synonyms" (see "Appendix: List of Words Used to Create Four Averaged Social Category Vectors"). A set of \textit{social institution vectors} is constructed by identifying the 50 word vectors most closely aligned with the vectors \texttt{nation + state} (polity), \texttt{money} (economy), \texttt{culture} (culture), and \texttt{housework + children} (domestic). Figure 3 (original) reports differences in mean cosine similarity between each pair of social category vectors with respect to each social institution vector, resulting in 24 total aggregate comparisons. After qualitative analysis, an additional set of cross-categorical differences in means is computed with respect to the vector \texttt{authority}, reported in Figure 5 (original).

The qualitative dimension of Nelson's methodology apart from the use of cosine similarity regression is relevant to the analysis of word frequencies. Three interlocking analytic moves are particularly interesting from the perspective of frequency bias. First, the analysis is not based solely on quantities derived from word embeddings; Nelson develops substantive insights through a dynamic engagement between model results and close reading of the underlying texts (also see Nelson 2020). Second, this portion of the analysis employs cosine similarity as an indexing measure to surface words closely related to terms of interest. Third, it is through the combination of these two approaches---more dense interpretive engagement with the text \textit{and} the use of the cosine ratio for information retrieval---that Nelson is able to observe rich and meaningful differences in diction across the texts included in the corpus. Notably, the analysis in this portion of the paper focuses a great deal on relative word frequencies; I return to this point at the end of the paper.

The measures of intersectional discursive space in the paper are affected by frequency bias in ways that considerably affect their interpretation. In particular, the identity-pairwise differences in mean cosine similarity summarized in Figures 3 and 5 of the paper are skewed by the implicit omission of word frequency from each model. To show this, I compare each original estimated difference in means to a corrected ratio model including a multiplicative interaction with the inverse local normalization weight $\text{LNW}^{-1}(w_i, \Omega_{X(i)})$ for each comparison and the corresponding lower-order term:  
$$
\begin{aligned}
\cos(w_i, \Omega_{X(i)}) &= \beta^*_0 + \beta^*_1X_i + \epsilon_i\\
\cos(w_i, \Omega_{X(i)}) &= \beta_0 + \beta_1X_i + \color{blue}{\beta_2\text{LNW}^{-1}(w_i, \Omega_{X(i)}) + \beta_3X_i\text{LNW}^{-1}(w_i, \Omega_{X(i)})} + v_i\\
\end{aligned}
$$  
An important feature of this model is the dependence between the grouping variable $X_i$ and the composed vector norm $||\Omega_{X(i)}||$ in the normalization weight terms. This increases correlation between $X_i$ and $\text{LNW}^{-1}(w_i, \Omega_{X(i)})$ relative to the independent model. Because all of the variance in the normalization weight in this model comes from the frequency distribution of $||w_i||$ conditional on $X_i$, the interpretation of the normalization weight terms in the resulting model becomes unclear. The variance of this term tends to increase considerably because it is strongly associated with the grouping variable, or the lower order norm weight term if this is included.

The use of the composed vectors $\Omega^{\circ}_{X}$ additionally implies the model estimates the set of groupwise individual distance-scale associations subject to the compositional homogeneity constraint $\color{purple}{\xi_1\left<\Omega_{1,i}, X_i\right> + \cdots + \xi_{j-1}\left<\Omega_{(j-1),i}, X_i\right>; \xi_j = -1}$ (omitted in above specification). This implies a model and frequency bias decomposition for every word vector that composes $\Omega^{\circ}_{X}$. I focus on describing the frequency distortion at the level of the analysis as conducted; related respecifications of the analysis are possible, but exploring the space of alternative models for any particular application is beyond the scope of this paper.

Figure \ref{fig:nelson_diffinmeans} compares the results of the original analysis to the corrected cosine ratio regression; the full results are reported in tabular form in the appendix. Several aspects of this reanalysis are worth noting. First, adding the local normalization weight to the model increases fit considerably; the change in $\text{R}^2$ varies between 0.179 and 0.688. Second, because the variance in the proposed corrected cosine similarity model now accounts for uncertainty relating to the local normalization weight, the estimated standard error on the identity coefficient also changes across every comparison. Third, the direction of the bias is not always the same, and in particular it does not always move the point estimate of interest toward zero.

Figure \ref{fig:nelson_errordecomp} decomposes the estimated frequency bias in each comparison. The estimated distortion is largest for the between-race between-gender comparisons, then the within-race between-gender comparisons, and least for the within-gender between-race comparisons. The total (net) frequency bias is by far the highest in the \texttt{domestic} subspace, and there is some positive distortion in the \texttt{polity} and \texttt{economy} subspaces as well. On balance the `culture` comparisons are unbiased, but this masks some variability in the underlying distortion decomposition; the intercept bias in this subspace is comparable, but the lower-order term bias goes in the opposite direction. The \texttt{authority} terms stand out as the only subspace where the estimated frequency bias is attenuating on average. The between-race between-gender comparisons and the within-race between-gender comparisons exhibit a strong negative mean lower-order term bias.

```{r nelson_diffinmeans, echo=F, message=F, fig.height=8.8, out.width="0.9\\textwidth", fig.align="center", fig.cap="Change in estimated difference in mean cosine similarity between paired social category mean vectors and social institution vectors (Nelson 2021). Original model estimate in blue and local normalization weight-adjusted linear model estimate in red with parametric 95\\% confidence intervals superimposed. Labels indicate $\\text{R}^2$ in the adjusted model. The change in the difference in means reflects the relative difference between the implicit word cofrequency distributions $P(Category(A), Institution)$ and $P(Category(B), Institution)$."}
# How frequency bias affects regression: omitted variable bias in Fig. 3
# Predict difference in means from log term frequency
# Each DiM is a linear regression of the form cos(identity vector, institution vector) ~ identity category
# The frequency bias can be captured by adding various frequency measures to the design matrix

# Construct all 24 comparisons shown in Fig. 3
# Also include the "authority" comparison in Fig. 5
# k=50 is the original setting in the paper
k <- 50
polity_vmat <- view_from_composite(nemb, v_polity) %>%
  filter(!term2 %in% c("nation", "state") & !is.na(frequency)) %>%
  slice_head(n=k)
polity_words <- polity_vmat$term2
polity_snorm <- polity_vmat$snorm
polity_vmat <- as.matrix(nemb[polity_vmat$term2,])
economy_vmat <- view_from_composite(nemb, v_economy) %>%
  filter(!term2 %in% c("money") & !is.na(frequency)) %>%
  slice_head(n=k)
economy_words <- economy_vmat$term2
economy_snorm <- economy_vmat$snorm
economy_vmat <- as.matrix(nemb[economy_vmat$term2,])
culture_vmat <- view_from_composite(nemb, v_culture) %>%
  filter(!term2 %in% c("culture") & !is.na(frequency)) %>%
  slice_head(n=k)
culture_words <- culture_vmat$term2
culture_snorm <- culture_vmat$snorm
culture_vmat <- as.matrix(nemb[culture_vmat$term2,])
domestic_vmat <- view_from_composite(nemb, v_domestic) %>%
  filter(!term2 %in% c("housework", "children") & !is.na(frequency)) %>%
  slice_head(n=k)
domestic_words <- domestic_vmat$term2
domestic_snorm <- domestic_vmat$snorm
domestic_vmat <- as.matrix(nemb[domestic_vmat$term2,])
authority_vmat <- view_from_composite(nemb, v_authority) %>%
  filter(!term2 %in% c("authority") & !is.na(frequency)) %>%
  slice_head(n=k)
authority_words <- authority_vmat$term2
authority_snorm <- authority_vmat$snorm
authority_vmat <- as.matrix(nemb[authority_vmat$term2,])

cs.male_black_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_male_black))
cs.male_black_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_male_black))
cs.male_black_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_male_black))
cs.male_black_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_male_black))
cs.male_black_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_male_black))

cs.female_black_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_female_black))
cs.female_black_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_female_black))
cs.female_black_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_female_black))
cs.female_black_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_female_black))
cs.female_black_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_female_black))

cs.male_white_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_male_white))
cs.male_white_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_male_white))
cs.male_white_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_male_white))
cs.male_white_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_male_white))
cs.male_white_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_male_white))

cs.female_white_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_female_white))
cs.female_white_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_female_white))
cs.female_white_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_female_white))
cs.female_white_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_female_white))
cs.female_white_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_female_white))

# Function to construct comparison
make_dim_df <- function(a, b, alab, blab, av, bv, flex, ext_freq=words_219k_m2138) {
  an <- norm(av, "2")
  bn <- norm(bv, "2")
  rbind.data.frame(data.frame(cs=a, identity=alab, word=flex) %>% 
                   left_join(tfn, by=c("word"="feature")) %>%
                   left_join(ext_freq %>% select(word, ext.freq=freq), by="word") %>%
                   mutate(nprod = snorm * an,
                          bprod = sqrt(log(frequency)) * bn,
                          fprod = sqrt(log(ext.freq)) * an,
                          ip = cs * nprod),
                 data.frame(cs=b, identity=blab, word=flex) %>%
                   left_join(tfn, by=c("word"="feature")) %>%
                   left_join(ext_freq %>% select(word, ext.freq=freq), by="word") %>%
                   mutate(nprod = snorm * bn,
                          bprod = sqrt(log(frequency)) * bn,
                          fprod = sqrt(log(ext.freq)) * bn,
                          ip = cs * nprod)) ->
    ddf
  return(ddf)
}

stargz <- function(pval) {
  ifelse(pval < 0.001, "***",
         ifelse(pval < 0.01, "**",
                ifelse(pval < 0.05, "*", "")))
}

# Show linear model frequency bias 
linear_frequency_bias <- function(dimdf, dimtitle, use.scale=F, use.nprod=T, robust=F, ftest=F, stargazer=T, focal.p=F, only.dim=F, fullsumm=F, delta.rsq=F) {
  if(use.scale) {
    dimdf$fbmeasure <- dimdf$snorm^2
  } else if(use.nprod) {
    dimdf$fbmeasure <- 1/dimdf$nprod  
  } else {
    dimdf$fbmeasure <- log(dimdf$frequency)
  }
  ols <- lm(cs ~ identity, data=dimdf)
  ols.freqbias <- lm(cs ~ identity * fbmeasure, data=dimdf)
  ols.fb2 <- lm(cs ~ identity * fprod, data=dimdf %>% mutate(fprod=1/fprod))
  # ols.fb2 <- lm(ip ~ identity * snorm, data=dimdf)
  # ols.fb2 <- lm(cs ~ identity * fbmeasure + snorm, data=dimdf %>% mutate(snorm=1/snorm))
  ols.freqbias2 <- lm(ip ~ identity * nprod, data=dimdf)
  hsk <- c(bptest(ols)$statistic,
           bptest(ols.freqbias)$statistic,
           bptest(ols.fb2)$statistic,
           bptest(ols.freqbias2)$statistic)
  hskp <- c(stargz(bptest(ols)$p.value),
            stargz(bptest(ols.freqbias)$p.value),
            stargz(bptest(ols.fb2)$p.value),
            stargz(bptest(ols.freqbias2)$p.value))
  chsk <- c("LM Statistic", paste0(paste0(round(hsk, 3), sep=""), hskp, sep=""))
  if(ftest) {
    anova(ols, ols.freqbias)
  } else if(stargazer) {
    stargazer::stargazer(ols, ols.freqbias, ols.fb2, ols.freqbias2,
                         title=dimtitle,
                         # dep.var.labels=c("Cosine similarity"),
                         dep.var.labels=c("Cosine similarity", "Inner product"),
                         dep.var.caption="",
                         covariate.labels=c("Social category vector",
                                            "Local normalization weight",
                                            "Identity \\times \\ LNW",
                                            # "||A||",
                                            # "Identity \\times \\ ||A||",
                                            "Scaled log COCA frequency",
                                            "Identity \\times \\ log(freq)",
                                            "Norm product",
                                            "Identity \\times \\ NP",
                                            "Constant"),
                         se=list(sqrt(diag(vcovHC(ols, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.freqbias, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.fb2, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.freqbias2, type="HC3")))),
                         add.lines=list(chsk),
                         star.cutoffs=c(0.05, 0.01, 0.001),
                         omit.stat=c("adj.rsq"),
                         single.row = T,
                         column.sep.width = "1pt",
                         font.size = "footnotesize",
                         header=F)
  } else if(focal.p) {
    summary(polity_mb_fb.freqbias)$coefficients[2,4]
  } else if(only.dim) {
    which.mod <- ols.freqbias  # Default (ratio model)
    # which.mod <- ols.freqbias2  # Component model
    # which.mod <- ols.fb2  # Shows the figure with the COCA frequency estimate instead
    
    # Bias decomposition
    dimdf %<>% mutate(idv = as.numeric(as.factor(identity)))
    bc <- coef(ols.freqbias)[3] * cov(dimdf$idv, dimdf$fbmeasure) / var(dimdf$idv)
    bm <- coef(ols.freqbias)[4] * cov(dimdf$idv, dimdf$idv * dimdf$fbmeasure) / var(dimdf$idv)
    
    # Heteroskedasticity test between component and ratio models
    m1.h <- bptest(ols.freqbias)
    m2.h <- bptest(ols.freqbias2)
    
    return(c(comparison = paste0(unique(dimdf$identity)),
             # r2=summary(which.mod)$r.squared,
             r2=sprintf("R²: %s, %s", round(summary(ols)$r.squared, 3), round(summary(which.mod)$r.squared, 3)),
             coef(summary(which.mod))[2,1:2],
             bias.const = bc,
             bias.main = bm,
             bias = bc + bm,
             bchk = coef(ols)[2],
             hsk = m1.h$statistic,
             hsk.p = m1.h$p.value,
             hsk.alt = m2.h$statistic,
             hsk.alt.p = m2.h$p.value,
             b2 = coef(ols.freqbias)[3],
             b2.cr = cov(dimdf$idv, dimdf$fbmeasure) / var(dimdf$idv),
             b3 = coef(ols.freqbias)[4],
             b3.cr = cov(dimdf$idv, dimdf$idv * dimdf$fbmeasure) / var(dimdf$idv),
             foc.rsq = summary(ols.freqbias)$r.squared,
             alt.rsq = summary(ols.freqbias2)$r.squared))
  } else if(fullsumm) {
    if(robust) {
      print(coeftest(ols, vcov = vcovHC(ols, type="HC1")))
      print(coeftest(ols.freqbias, vcov = vcovHC(ols.freqbias, type="HC1")))
    } else {
      print(summary(ols))
      print(summary(ols.freqbias))
      summarize_ovb(dimdf)
    }
  } else if(delta.rsq) {
    data.frame(rsq=c(summary(ols)$r.squared, summary(ols.freqbias)$r.squared),
               model=c("original model", "frequency interaction"))
  } else {
    rbind.data.frame(data.frame(summary(ols)$coefficients, model="original model"),
                     data.frame(summary(ols.freqbias)$coefficients, model="frequency interaction")) %>%
      mutate(sig=tsig(`Pr...t..`))
  }
}

# Frequency bias tends to create omitted variable bias because frequency is correlated
#  with the cosine similarity and the relative frequency distributions differ by group
# Adding a measure of frequency into the model tends to increase the standard error on the
#  grouping variable in the difference in mean, and the model fit goes up a lot
# Note that this doesn't always make the effect "go away" per se; the problem is that
#  frequency has a somewhat unpredictable relationship to this problem because it depends
#  on the exact difference in mean log frequency by group and the linear correlation of 
#  the log frequency with the cosine similarity
# In general the reciprocal local normalization weight is the best frequency measure to use
#  because it's a component of cosine similarity, but any functional form will work; the 
#  key thing here is that the LPNW tends to vary by group (frequency bias is relative)
rbind(

# Black male <-> Black female
data.frame(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), comparison="Black, Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), comparison="Black, Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), comparison="Black, Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), comparison="Black, Male x Black, Female (domestic)"),

# White male <-> White female
data.frame(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), comparison="White Male x White Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), comparison="White Male x White Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), comparison="White Male x White Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), comparison="White Male x White Female (domestic)"),

# White female <-> Black female
data.frame(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), comparison="White Female x Black, Female (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), comparison="White Female x Black, Female (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), comparison="White Female x Black, Female (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), comparison="White Female x Black, Female (domestic)"),

# Black male <-> White male
data.frame(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), comparison="Black, Male x White Male (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), comparison="Black, Male x White Male (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), comparison="Black, Male x White Male (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), comparison="Black, Male x White Male (domestic)"),

# White male <-> Black female
data.frame(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), comparison="White Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), comparison="White Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), comparison="White Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), comparison="White Male x Black, Female (domestic)"),

# White female <-> Black male
data.frame(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), comparison="White Female x Black, Male (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), comparison="White Female x Black, Male (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), comparison="White Female x Black, Male (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), comparison="White Female x Black, Male (domestic)"),

# Authority (Fig. 5)
data.frame(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), comparison="Black, Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), comparison="White Male x White Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), comparison="White Female x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), comparison="Black, Male x White Male (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), comparison="White Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), comparison="White Female x Black, Male (authority)")) ->
  all_comparisons

# Get all of the regression results in one place
# This is unspeakably ugly but oh well
rbind(

# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)", only.dim=T, stargazer=F),

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)", only.dim=T, stargazer=F),

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)", only.dim=T, stargazer=F),

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)", only.dim=T, stargazer=F)) ->
  fblm_results_f

institutions <- c("polity", "economy", "culture", "domestic", "authority")
data.frame(fblm_results_f) %>%
  rowwise() %>%
  mutate(institution = institutions[which(str_detect(comparison1, institutions))],
         comparison = str_replace_all(paste0(str_remove(comparison2, institution), "- ",
                                             str_remove(comparison1, institution)), "_", " "),
         comparison2 = str_replace_all(paste0(str_remove(comparison1, institution), "- ",
                                              str_remove(comparison2, institution)), "_", " ")) %>%
  rename(lm.est.dim = Estimate, lm.est.se = Std..Error) ->
  fblm_results

# Plot all comparisons
all_comparisons %>%
  mutate(institution=str_remove_all(str_extract(comparison, "\\([a-z]+\\)"), "[()]")) %>%
  group_by(institution, identity) %>%
  summarize(mean_cs = mean(cs), sd_cs = sd(cs)) %>%
  ungroup() ->
  ac_summ

ac_summ %>%
  left_join(ac_summ, by="institution") %>%
  filter(!duplicated(paste0(pmin(identity.x, identity.y),
                            pmax(identity.x, identity.y))) &
           identity.x != identity.y) %>%
  mutate(diffm_cs = mean_cs.y - mean_cs.x,
         diff.se_cs = sqrt(sd_cs.x^2/k + sd_cs.y^2/k),
         comparison = str_replace_all(paste0(str_remove(identity.x, institution), "- ",
                                             str_remove(identity.y, institution)), "_", " ")) %>%
  left_join(fblm_results, by=c("comparison", "institution")) %>%
  left_join(fblm_results, by=c("comparison"="comparison2", "institution")) %>%
  mutate(lm.est.dim = as.numeric(coalesce(lm.est.dim.x, lm.est.dim.y)),
         lm.est.se = as.numeric(coalesce(lm.est.se.x, lm.est.se.y)),
         # r2 = as.numeric(coalesce(r2.x, r2.y))) ->
         r2 = coalesce(r2.x, r2.y)) ->
  nemb_dim_estimates

# Plot faceted by institution
# nemb_dim_estimates %>%
#   rowwise() %>%
#   mutate(label.pin.x = max(lm.est.dim+(1.96*lm.est.se), diffm_cs+(1.96*diff.se_cs)) + 0.035,
#          # label.text = sprintf("R² = %s", round(r2, 3))) %>%
#          label.text = r2) %>%
#   ggplot() +
#   geom_vline(xintercept=0, linetype="dashed") +
#   geom_point(aes(y=comparison, x=diffm_cs), color="steelblue2",
#              position=position_nudge(y=0.1)) +
#   geom_errorbarh(aes(y=comparison,
#                      xmin=diffm_cs - 1.96*diff.se_cs,
#                      xmax=diffm_cs + 1.96*diff.se_cs),
#                  height=0.2, color="steelblue2", position=position_nudge(y=0.1)) +
#   geom_point(aes(y=comparison, x=lm.est.dim), color="tomato",
#              position=position_nudge(y=-0.1)) +
#   geom_errorbarh(aes(y=comparison,
#                    xmin=lm.est.dim - 1.96*lm.est.se,
#                    xmax=lm.est.dim + 1.96*lm.est.se),
#                height=0.2, color="tomato", position=position_nudge(y=-0.1)) +
#   geom_label(aes(label=label.text, y=comparison, x=label.pin.x), size=3, color="tomato") +
#   scale_x_continuous(limits=c(NA,0.23), n.breaks=10) +
#   facet_wrap(~institution, ncol=1) +
#   labs(x="Difference in mean cosine similarity", y="")

# Show scatterplots of means
# all_comparisons %>%
#   group_by(identity, comparison) %>%
#   mutate(lgmean = mean(cs), lgsd = sd(cs)) %>%
#   ungroup() %>%
#   arrange(nprod) %>%
#   ggplot(aes(y=identity, x=cs, color=nprod)) +
#   geom_jitter(width=0.2, alpha=0.3) +
#   geom_point(aes(y=identity, x=lgmean), color="black") +
#   geom_errorbarh(aes(y=identity, xmin=lgmean-1.96*lgsd, xmax=lgmean+1.96*lgsd), height=0.2, color="black") +
#   scale_color_viridis_c() +
#   theme(legend.position="bottom") +
#   facet_wrap(~comparison, scales = "free_y", ncol=5) +
#   labs(x="Focal social category vector",
#        y="Cosine similarity",
#        color="local normalization weight")

# Swap institution and identity (plot as in paper)
nemb_dim_estimates %>%
  rowwise() %>%
  mutate(label.pin.x = max(lm.est.dim+(1.96*lm.est.se), diffm_cs+(1.96*diff.se_cs)) + 0.07,  #+ 0.035,
         # label.text = sprintf("R² = %s", round(r2, 3))) %>%
         label.text = r2) %>%
  ggplot() +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_point(aes(y=institution, x=diffm_cs), color="steelblue2",
             position=position_nudge(y=0.1)) +
  geom_errorbarh(aes(y=institution,
                     xmin=diffm_cs - 1.96*diff.se_cs,
                     xmax=diffm_cs + 1.96*diff.se_cs),
                 height=0.2, color="steelblue2", position=position_nudge(y=0.1)) +
  geom_point(aes(y=institution, x=lm.est.dim), color="tomato",
             position=position_nudge(y=-0.1)) +
  geom_errorbarh(aes(y=institution,
                   xmin=lm.est.dim - 1.96*lm.est.se,
                   xmax=lm.est.dim + 1.96*lm.est.se),
               height=0.2, color="tomato", position=position_nudge(y=-0.1)) +
  geom_label(aes(label=label.text, y=institution, x=label.pin.x), size=3, color="tomato") +
  scale_x_continuous(limits=c(NA,0.3), n.breaks=10) +
  facet_wrap(~comparison, ncol=1) +
  labs(x="Difference in mean cosine similarity", y="")
```

```{r nelson_regcomp, echo=F, message=F, fig.height=8.8, out.width="0.9\\textwidth", fig.align="center", fig.cap="", include=F, eval=F}
plot_reg_comparison <- function(focal) {
  all_comparisons %>%
  filter(str_detect(identity, focal)) %>%
  mutate(fbmeasure = 1/nprod) %>%
  ggplot(aes(x=fbmeasure, y=ip, color=identity, label=word)) +
  geom_text(size=3) +
  geom_line(aes(group=word), color="black", alpha=0.75, linetype="dashed") +
  theme(legend.position="bottom") +
  facet_wrap(~comparison, scales = "free", ncol=3) +
  labs(x="Local normalization weight",
       y="Inner product",
       color="Group",
       title="Conditional word size-distance distribution") -> acf1

  all_comparisons %>%
    filter(str_detect(identity, focal)) %>%
    mutate(fbmeasure = 1/nprod) %>%
    ggplot(aes(x=fbmeasure, y=ip, color=identity, label=word)) +
    geom_point() +
    geom_smooth(aes(group=identity), method="lm", alpha=0.3) +
    # geom_smooth(aes(group=identity), method="loess", alpha=0.3) +
    geom_line(aes(group=word), color="black", alpha=0.2, linetype="dashed") +
    theme(legend.position="bottom") +
    facet_wrap(~comparison, scales = "free", ncol=3) +
    labs(x="Local normalization weight",
         y="Inner product",
         color="Group",
         title="Size-distance separation regression") -> acf2
  
  all_comparisons %>%
    filter(str_detect(identity, focal)) %>%
    group_by(comparison) %>%
    mutate(idrank=as.numeric(as.factor(identity))) %>%
    ungroup() %>%
    mutate(fbmeasure = 1/nprod) %>%
    ggplot(aes(x=fbmeasure, y=ip, color=identity, label=word)) +
    geom_point() +
    geom_smooth(aes(group=identity), method="lm", formula=y~-1+x, alpha=0.3) +
    geom_line(aes(group=word), color="black", alpha=0.2, linetype="dashed") +
    theme(legend.position="bottom") +
    facet_wrap(~comparison, scales = "free", ncol=3) +
    labs(x="Local normalization weight",
         y="Inner product",
         color="Group",
         title="Cosine similarity regression") -> acf3
  
  ggarrange(acf1, acf2, acf3, common.legend = T, legend="bottom", ncol=1)
}

plot_reg_comparison("domestic")
plot_reg_comparison("authority")
plot_reg_comparison("economy")
```

```{r nelson_errordecomp, echo=F, message=F, fig.height=7, out.width="0.9\\textwidth", fig.align="center", fig.cap="Frequency distortion decomposition, Nelson (2021). Top row compares estimated frequency bias in the uncorrected cosine similarity regression relative to the corrected ratio model for each social insitutition by paired social identity comparison. Bottom row shows estimated bias decomposition due to omitted intercepts (left) and omitted lower-order grouping term (right) corresponding to each comparison in the top row. The rows of each plot correspond to the same social identity comparison, and the vertical dashed lines indicate the institution-wise mean bias (zero bias in black). The scale of the lower-order term bias generally dominates the intercept bias; however, they also have a tendency to cancel each other (for example, compare mean authority bias in each decomposition), obscuring the total distortion in the model."}
# Total bias, comparison X institution
# Distortion is largest for the intersectional comparisons, then the within-race gender comparisons, then the within-gender race comparisons
fblm_results %>% mutate(bias=as.numeric(bias.fbmeasure)) %>% group_by(institution) %>% mutate(gm=mean(bias)) %>% ggplot(aes(x=bias, y=comparison, color=institution, label=institution)) + geom_vline(aes(xintercept=gm, color=institution), linetype="dashed") + geom_text(angle=30) + geom_vline(xintercept=0, linetype="dashed") + theme(legend.position="none") + labs(x="Total frequency bias", y="Comparison") -> tbx1

# Flipped
# Distortion order, perhaps: domestic, polity, economy, authority, culture
fblm_results %>% mutate(bias=as.numeric(bias.fbmeasure)) %>% group_by(comparison) %>% mutate(gm=mean(bias)) %>% ggplot(aes(x=bias, y=institution, color=comparison, label=comparison)) + geom_vline(aes(xintercept=gm, color=comparison), linetype="dashed") + geom_text(angle=30) + geom_vline(xintercept=0, linetype="dashed") + theme(legend.position="none") + labs(x="Total frequency bias", y="Institution") -> tbx2

# Decomposition
# Omitted intercept bias contribution
fblm_results %>% mutate(bias=as.numeric(bias.const.fbmeasure)) %>% group_by(institution) %>% mutate(gm=mean(bias)) %>%
  ggplot(aes(x=bias, y=comparison, color=institution, label=institution)) +
  geom_vline(aes(xintercept=gm, color=institution), linetype="dashed") +
  # geom_text(size=2, angle=30) +
  geom_point() +
  geom_vline(xintercept=0, linetype="dashed") +
  theme(legend.position="none", axis.text.y=element_blank()) +
  labs(x="Omitted intercept bias", y="") -> bdc1

# Omitted lower-order term bias contribution (tends to be larger)
fblm_results %>% mutate(bias=as.numeric(bias.main.identitymale_black_polity.fbmeasure)) %>% group_by(institution) %>% mutate(gm=mean(bias)) %>%
  ggplot(aes(x=bias, y=comparison, color=institution, label=institution)) +
  geom_vline(aes(xintercept=gm, color=institution), linetype="dashed") +
  # geom_text(size=2, angle=30) +
  geom_point() +
  geom_vline(xintercept=0, linetype="dashed") +
  theme(legend.position="none", axis.text.y=element_blank()) +
  labs(x="Omitted lower-order term bias", y="") -> bdc2

# ggarrange(tbx1, tbx2, bdc1, bdc2, ncol=2, nrow=2)

library(patchwork)
tbx1 / (bdc1 | bdc2)
```

```{r nbias2, include=F, eval=F}
# Compare bias terms
# Main effect bias tends to dominate
# The biases are generally negatively associated (r = -0.461)
fblm_results %>% ggplot(aes(x=bias.const, y=bias.main)) + geom_point() + coord_equal() + xlim(-0.1, 0.1) + ylim(-0.1, 0.1) + geom_smooth(method="lm") + geom_abline(slope=1, intercept=0, linetype="dashed") + geom_abline(slope=-1, intercept=0, linetype="dashed")

# Which model tends to perform better for which comparison
fblm_results %>% ggplot(aes(x=as.numeric(foc.rsq), y=as.numeric(alt.rsq), color=comparison, label=institution)) + geom_text(angle=30) + geom_abline(slope=1, intercept=0, linetype="dashed") + labs(x="Ratio model", y="Component model")
```

\pagebreak

## "The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings." (Kozlowski, Taddy and Evans 2019)

Kozlowski, Taddy and Evans (2019; hereafter KTE) present an analysis of a set of word vectors designed to measure the associative meanings of social class in language over time. To accomplish this, KTE leverage the Google Ngram 5-gram corpus to produce embeddings of small snippets of language derived from a large body of books and other archived texts.^[Some researchers have raised concerns about the generalizability of the corpus underlying Ngram, particularly its heavy representation of scientific texts and general bias toward library-like collections of text over other comparably important samples of language; see Pechenick, Danford \& Dodds (2015).] The authors estimate word vectors using an SGNS model with 300 dimensions, a minimum term frequency threshold of 25, and the implicitly dynamic context window implied by the algorithm's default behavior and a user-determined maximum post-subsampling window size of $w=5$.^[The standard SGNS implementation selects the threshold at each sample on $[1, w]$ and subsamples word occurrences over a left-truncated word frequency window function, such that contexts involving more common words are effectively larger on average. See Levy and Goldberg (2014) for a more detailed discussion of the behavior of the context window hyperparameter, and see Schütze and Pedersen (1999) for additional discussion of context range semantics.] Models are estimated over approximately 10-year windows in the data; my reanalysis focuses on the publicly available vectors estimated for the time period 2000-2012.^[I do not currently re-estimate the temporally varying models; it would be interesting to examine the continuous(?) $X_i=t_i$ case.]

*Trivariate cosine similarity regression.* Figure 6 (KTE) describes the results of the following trivariate cosine regression, where the vectors $\{A, C, E\}$ are estimated mean vectors representing affluence, cultivation, and education, and $X$ comprises the top 50K word vectors by frequency in the corpus:
$$
\begin{aligned}
\cos(A, X_i) &= \beta^*_0 + \beta^*_1\cos(C, X_i) + \beta^*_2\cos(E, X_i) + \epsilon_i \\
\end{aligned}
$$  
Extending the bivariate results discussed previously, the model omits a number of terms reflecting potentially separable scale and distance variability in the comparison with respect to the normalization weight functions:  
$$
\begin{aligned}
\left<A, X_i\right> &= \beta_0\text{LNW}(A, X_i) + \beta_1\cos(C, X_i)\text{LNW}(A, X_i) + \beta_2\cos(E, X_i)\text{LNW}(A, X_i) + \epsilon_i \\
&+ \color{red}{\beta_3 + \beta_4\cos(C, X_i) + \beta_5\cos(E, X_i)} \\
&+ \color{red}{\beta_6\left<C, X_i\right> + \beta_7\text{LNW}^{-1}(C, X_i) + \beta_8\left<E, X_i\right> + \beta_9\text{LNW}^{-1}(E, X_i)} \\
&+ \color{red}{\beta_{10}\left<C, X_i\right>\text{LNW}(A, X_i) + \beta_{11}\left<E, X_i\right>\text{LNW}(A, X_i)} \\
\end{aligned}
$$
The alternative model with lower-order terms included can be written as an inner product regression, optionally including an interaction between the RHS inner products. This may be of interest given the shared $X_i$ term in each distance.
$$
\begin{aligned}
\left<A, X_i\right> &= \beta_0 + \beta_1\left<C, X_i\right> +\beta_2\left<E, X_i\right> \\
&+ \beta_3\text{LNW}(A, X_i) + \beta_4\left<C, X_i\right>\text{LNW}(A, X_i) + \beta_5\left<E, X_i\right>\text{LNW}(A, X_i) \\
\textit{optionally: } &+ \beta_6 \left<C, X_i\right>\left<E, X_i\right> + \beta_7\left<C, X_i\right>\left<E, X_i\right>\text{LNW}(A, X_i) \\
&+ \epsilon_i
\end{aligned}
$$  
Table 1 reports the results of the original model extended to the 2000-2012 Ngram embeddings (1), along with the ratio and component specifications of a proposed alternative additive similarity model (2 and 3) and an interactive component model (4). The extension of the original model into the 2000-2012 period shows a significant difference between the coefficients for the cultivation cosines and the education cosines because the latter has decreased, perhaps back toward its long-term trend (p. 924). However, adjusting this model by including an interaction with the normalization weight suggests this mean association varies conditional on the word frequencies implied by the comparison. This suggests the estimated distance associations are scale-dependent. The rank order of the affluence-cultivation and affluence-education distance associations swap as we move from one end of the word frequency distribution to the other, which also implies that there is a segment of middle co-frequencies for which the difference is statistically indistinguishable. The interactive model suggests that the cultivation-education inner product ratio is independently associated with the affluence-$X_i$ inner product distribution conditional on the marginal norm distribution of the reference vectors.

Note that the model implies three (separable) strict homogeneity constraints over the vector subspaces $A, C, E$, each of which is composed of an additional layered projection linking two sets of terms representing semantic oppositions (see KTE tables D1, D3). I omit these from the above specifications for clarity.

\pagebreak

```{r kte_setup, echo=F, message=F}
# Load canonical embedding
kemb.d <- read_csv(here("data", "kte2019", "US_Ngrams_2000_12.csv"), col_names = FALSE, skip = 1)
kemb.tok <- kemb.d$X1
kemb <- kemb.d[,-1]  # 1.5M X 300d embedding matrix
rownames(kemb) <- kemb.tok
sN <- nrow(kemb)
sp <- ncol(kemb)

# Fixed word lists
affluence <- read_csv(here("data", "kte2019", "word_pairs", "affluence_pairs.csv"), col_names = c("RICH", "POOR"))
gender <- read_csv(here("data", "kte2019", "word_pairs", "gender_pairs.csv"), col_names = c("MASCULINE", "FEMININE"))
race <- read_csv(here("data", "kte2019", "word_pairs", "race_pairs.csv"), col_names = c("BLACK", "WHITE"))

# Class dimensions
cultivation <- read_csv(here("data", "kte2019", "word_pairs", "cultivation.csv"))
education <- read_csv(here("data", "kte2019", "word_pairs", "education.csv"))
employment <- read_csv(here("data", "kte2019", "word_pairs", "employment.csv"))
morality <- read_csv(here("data", "kte2019", "word_pairs", "morality.csv"))
status <- read_csv(here("data", "kte2019", "word_pairs", "status.csv"))

# Load pre-joined word frequency file
# Corpus is Google Ngram files for year in [2000, 2012]
ktfn <- read_csv(here("data", "kte2019", "vocabulary_norms_freqs.csv"))

# Load COCA word frequency file
words_219k_m2138 <- read_delim(here("data", "words_219k_m2138.txt"),
                               "\t", escape_double = FALSE, trim_ws = TRUE, skip = 8)

# Construct comparison data
affluence %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("RICH"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("POOR"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[RICH,]), as.numeric(kemb[POOR,])),
         ip = as.numeric(kemb[RICH,]) %*% as.numeric(kemb[POOR,])) ->
  affl_f

cultivation %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("CIVILIZED"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("UNCIVILIZED"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[CIVILIZED,]), as.numeric(kemb[UNCIVILIZED,])),
         ip = as.numeric(kemb[CIVILIZED,]) %*% as.numeric(kemb[UNCIVILIZED,])) ->
  cult_f

education %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("EDUCATED"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("UNEDUCATED"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[EDUCATED,]), as.numeric(kemb[UNEDUCATED,])),
         ip = as.numeric(kemb[EDUCATED,]) %*% as.numeric(kemb[UNEDUCATED,])) ->
  educ_f

employment %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("BOSS"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("WORKER"="feature"))  %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[BOSS,]), as.numeric(kemb[WORKER,])),
         ip = as.numeric(kemb[BOSS,]) %*% as.numeric(kemb[WORKER,])) ->
  empl_f

morality %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("GOOD"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("EVIL"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[GOOD,]), as.numeric(kemb[EVIL,])),
         ip = as.numeric(kemb[GOOD,]) %*% as.numeric(kemb[EVIL,])) ->
  good_f

status %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("STATUS_HIGH"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("STATUS_LOW"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[STATUS_HIGH,]), as.numeric(kemb[STATUS_LOW,])),
         ip = as.numeric(kemb[STATUS_HIGH,]) %*% as.numeric(kemb[STATUS_LOW,])) ->
  stat_f

gender %>%
  left_join(ktfn %>% select(feature, norm.dominant=snorm,
                           freq.ext.dominant=frequency.coca, freq.dominant=frequency),
            by=c("MASCULINE"="feature")) %>%
  left_join(ktfn %>% select(feature, norm.subordinate=snorm,
                           freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
            by=c("FEMININE"="feature")) %>%
  rowwise() %>%
  mutate(cs = lsa::cosine(as.numeric(kemb[MASCULINE,]), as.numeric(kemb[FEMININE,])),
         ip = as.numeric(kemb[MASCULINE,]) %*% as.numeric(kemb[FEMININE,])) ->
  gend_f

# race %>%
#   left_join(ktfn %>% select(feature, norm.dominant=snorm,
#                            freq.ext.dominant=frequency.coca, freq.dominant=frequency),
#             by=c("WHITE"="feature")) %>%
#   left_join(ktfn %>% select(feature, norm.subordinate=snorm,
#                            freq.ext.subordinate=frequency.coca, freq.subordinate=frequency),
#             by=c("BLACK"="feature")) ->
#   race_f

# Construct mean vectors (some of these are not in the embedding?)
affl_mv <- colMeans(kemb[affl_f$RICH,]-kemb[affl_f$POOR,], na.rm=T)
cult_mv <- colMeans(kemb[cult_f$CIVILIZED,]-kemb[cult_f$UNCIVILIZED,], na.rm=T)
educ_mv <- colMeans(kemb[educ_f$EDUCATED,]-kemb[educ_f$UNEDUCATED,])
empl_mv <- colMeans(kemb[empl_f$BOSS,]-kemb[empl_f$WORKER,])
good_mv <- colMeans(kemb[good_f$GOOD,]-kemb[good_f$EVIL,])
stat_mv <- colMeans(kemb[stat_f$STATUS_HIGH,]-kemb[stat_f$STATUS_LOW,], na.rm=T)
gend_mv <- colMeans(kemb[gend_f$MASCULINE,]-kemb[gend_f$FEMININE,])

# Compute pairwise cosines
class_angles <- lsa::cosine(t(rbind(affl_mv, cult_mv, educ_mv, empl_mv, good_mv, stat_mv, gend_mv)))

# Note that these have different lengths
affl_mv_n <- norm(affl_mv, "2")
cult_mv_n <- norm(cult_mv, "2")
educ_mv_n <- norm(educ_mv, "2")
empl_mv_n <- norm(empl_mv, "2")
good_mv_n <- norm(good_mv, "2")
stat_mv_n <- norm(stat_mv, "2")
gend_mv_n <- norm(gend_mv, "2")
```

```{r kte_reg6_prep, echo=F, message=F}
# Set up Figure 6 regression matrix (TODO: is this exact?)
n.s <- 50000
ktfn %>%
  filter(!is.na(frequency)) %>%
  arrange(desc(frequency)) %>%
  slice_head(n=n.s) ->
  tfnf

tfnf %>%
  rowwise() %>%
  mutate(ip.affl = as.numeric(kemb[feature,]) %*% affl_mv,
         np.affl = snorm * affl_mv_n,
         lnw.affl = 1/np.affl,
         cs.affl = ip.affl / np.affl,
         nref.affl = affl_mv_n, 
         ip.educ = as.numeric(kemb[feature,]) %*% educ_mv,
         np.educ = snorm * educ_mv_n,
         lnw.educ = 1/np.educ,
         cs.educ = ip.educ / np.educ,
         nref.educ = educ_mv_n, 
         ip.cult = as.numeric(kemb[feature,]) %*% cult_mv,
         np.cult = snorm * cult_mv_n,
         lnw.cult = 1/np.cult,
         cs.cult = ip.cult / np.cult,
         nref.cult = cult_mv_n) ->
         # ip.empl = as.numeric(kemb[feature,]) %*% empl_mv,
         # np.empl = snorm * empl_mv_n,
         # lnw.empl = 1/np.empl,
         # cs.empl = ip.empl / np.empl,
         # nref.empl = empl_mv_n,
         # ip.good = as.numeric(kemb[feature,]) %*% good_mv,
         # np.good = snorm * good_mv_n,
         # lnw.good = 1/np.good,
         # cs.good = ip.good / np.good,
         # nref.good = good_mv_n,
         # ip.stat = as.numeric(kemb[feature,]) %*% stat_mv,
         # np.stat = snorm * stat_mv_n,
         # lnw.stat = 1/np.stat,
         # cs.stat = ip.stat / np.stat,
         # nref.stat = stat_mv_n,
         # ip.gend = as.numeric(kemb[feature,]) %*% gend_mv,
         # np.gend = snorm * gend_mv_n,
         # lnw.gend = 1/np.gend,
         # cs.gend = ip.gend / np.gend,
         # nref.gend = gend_mv_n) ->
  tfnfr

# Problem: the normalization weights are perfectly correlated
# This means the model doesn't identify the projection adjustment
# The reference norm cancels on the RHS so each inner product distribution
#  is scaled by a no-variance constant 
# Additionally the sampling of the word vectors for the reference is interesting
#  Try resampling by tweaking the slice calls
# A nonlinear fit also does better 
tfnfr %>%
  ungroup() %>%
  slice_tail(prop = 1) %>%
  slice_sample(prop = 1) ->
  tfnfrc
m1 <- lm(cs.affl ~ cs.cult + cs.educ, data=tfnfrc)  # Original model
m2c <- lm(cs.affl ~ cs.cult * lnw.affl + cs.educ * lnw.affl, data=tfnfrc)  # Corrected independent ratio model
m2 <- lm(ip.affl ~ ip.cult * snorm + snorm * ip.educ, data=tfnfrc)  # Corrected independent component model
m3 <- lm(ip.affl ~ ip.cult * snorm * ip.educ, data=tfnfrc)  # Corrected interactive model

kh <- c(bptest(m1)$statistic,
        bptest(m2c)$statistic,
        bptest(m2)$statistic,
        bptest(m3)$statistic)
khp <- c(stargz(bptest(m1)$p.value),
         stargz(bptest(m2c)$p.value),
         stargz(bptest(m2)$p.value),
         stargz(bptest(m3)$p.value))
khsk <- c("LM Statistic", paste0(paste0(round(kh, 3), sep=""), khp, sep=""))
```

```{r kte_regsamp, eval=F, echo=F, include=F}
# What happens if we randomly partition X so that the error correlation isn't as tight?
tsx <- sample(rep(1:3, (n.s-2)/3))  # Ensure groups evenly sized (XXX)
tfn %>%
  filter(!is.na(frequency)) %>%
  arrange(desc(frequency)) %>%
  slice_head(n=n.s-2) ->
  tfnf_x

tfnf_x %>%
  rowwise() %>%
  mutate(ip.affl = as.numeric(kemb[feature,]) %*% affl_mv,
         np.affl = snorm * affl_mv_n,
         lnw.affl = 1/np.affl,
         cs.affl = ip.affl / np.affl,
         nref.affl = affl_mv_n, 
         ip.educ = as.numeric(kemb[feature,]) %*% educ_mv,
         np.educ = snorm * educ_mv_n,
         lnw.educ = 1/np.educ,
         cs.educ = ip.educ / np.educ,
         nref.educ = educ_mv_n, 
         ip.cult = as.numeric(kemb[feature,]) %*% cult_mv,
         np.cult = snorm * cult_mv_n,
         lnw.cult = 1/np.cult,
         cs.cult = ip.cult / np.cult,
         nref.cult = cult_mv_n) ->
    tfnfr_x

tfnfr_xa <- cbind.data.frame(tfnfr_x, tsx)
tfnfr_xa <- tfnfr_xa[sample(1:nrow(tfnfr_xa)),]
tfnfr_xa %>%
  group_by(tsx) %>%
  mutate(rn = row_number()) %>%
  ungroup() %>%
  group_by(rn) %>%
  arrange(tsx) %>%
  mutate(ip.affl = ip.affl[1],
         ip.cult = ip.cult[2],
         ip.educ = ip.educ[3],
         np.affl = np.affl[1],
         np.cult = np.cult[2],
         np.educ = np.educ[3],
         cs.affl = cs.affl[1],
         cs.cult = cs.cult[2],
         cs.educ = cs.educ[3]) %>%
  filter(tsx == 1) ->
  tfnfrc_x  # n.s/3 approx

# All of these are more or less null
m1r <- lm(cs.affl ~ cs.cult + cs.educ + rn, data=tfnfrc_x)
m2r <- lm(ip.affl ~ ip.cult * np.affl * np.cult + ip.educ * np.educ * np.affl, data=tfnfrc_x)
m3r <- lm(ip.affl ~ ip.cult * np.affl * np.cult * ip.educ * np.educ, data=tfnfrc_x)
```

```{r kte_reg6, echo=F, message=F, results="asis"}
stargazer::stargazer(m1, m2c, m2, m3,
                     title="Distance-scale regression: Affluence as a function of education, cultivation.",
                     dep.var.labels=c("Cosine similarity", "Inner product"),
                     dep.var.caption="",
                     covariate.labels=c("$\\cos(\\Omega_C, w)$",
                                        "Local normalization weight",
                                        "$\\cos(\\Omega_E, w)$",
                                        "$\\cos(\\Omega_C, w)$ \\times \\ LNW",
                                        "$\\cos(\\Omega_E, w)$ \\times \\ LNW",
                                        "$\\left<\\Omega_C, w\\right>$",
                                        "||w||",
                                        "$\\left<\\Omega_E, w\\right>$",
                                        "$\\left<\\Omega_C, w\\right>$ \\times \\ ||w||",
                                        "Inner product ratio",
                                        "$\\left<\\Omega_E, w\\right>$ \\times \\ ||w||",
                                        "Inner product ratio \\times \\ ||w||",
                                        "Constant"),
                     se=list(sqrt(diag(vcovHC(m1, type="HC3"))),
                             sqrt(diag(vcovHC(m2c, type="HC3"))),
                             sqrt(diag(vcovHC(m2, type="HC3"))),
                             sqrt(diag(vcovHC(m3, type="HC3")))),
                     add.lines=list(khsk),
                     star.cutoffs=c(0.05, 0.01, 0.001),
                     omit.stat=c("adj.rsq"),
                     single.row = T,
                     column.sep.width = "1pt",
                     font.size = "footnotesize",
                     float.env = "sidewaystable",
                     header=F)
```

\pagebreak

# Discussion

Meaningful heterogeneity over the distribution of word frequencies is a fact of life in statistical text analysis (Piantadosi 2014).^[Salton and Buckley (1988) suggest that a presence-only quantity yields worse semantics than any word frequency weighted similarity metric. A low-rank approximation to presence-only vectors must considerably coarsen the corresponding distance function because there is little comparative information available in such a representation.] Log-bilinear embedding models yield semantic vector spaces that avoid some of the pitfalls of doing data analysis in this setting. Unfortunately, a key drawback of the embedding approach is that it is easy to lose sight of the underlying frequency dependence of any semantic vector space. Frequency bias emerges from a mismatch between the mathematical operations at our disposal for interpreting semantic vector spaces and the intuitive null model we use when we expect meaning to be unrelated to frequency. Similarity metrics encode different notions of relatedness that can be used to magnify different parts of the frequency distribution relative to focal positions of interest, but when characterizing variation in associations is the research goal, it is risky to compute similarities without an appropriate adjustment for uncertainty due to the reprojection(s) relative to the design of the comparison.

Along these lines, researchers may benefit from viewing similarity as a separate weight estimation procedure that occurs *after* the estimation of vector space models, rather than as an intrinsic part of the semantic model per se (cf. Lowe 2001). Although modern word embedding models have been developed for optimal prediction under a particular set of semantically enriched reprojections (Levy \& Goldberg 2014), nothing about these models requires a researcher interested in describing the resulting vector space to also use these reprojections, and depending on the descriptive goal this procedure may not yield the desired answer. Cosines of linear combinations of vectors make for particularly difficult targets of inference due to the strong assumptions inherent in making this sequence of reprojections exactly. Thinking of similarity as a separate and optional transformation of a semantic vector space may help to clarify some confusions in the literature surrounding the use and interpretation of similarity coefficients. By taking this ratio, we are estimating another quantity that helps us see the space we have estimated. Whether this new estimate brings us closer to our research goal depends strongly on the fit of this rearrangement of the data to our question, and not just to the underlying model or data.

From an estimation perspective, cosine similarity can be thought of as an overly confident projection of the inner product. By employing this ratio directly, we assume we know the exact scale correction to make at every (sampled) point in the estimated inner product space and that there is no uncertainty attributable to making this adjustment. This is not a bad default assumption, but it is far from clear that this is the "best" adjustment we could make given a focal set of word vectors. In particular, typical vector set selections in the social sciences often do not span the full vector space, implying that some of the information contained in the inner product is extraneous. This implies that we should prefer a model of that allows some of this variation to be uninformative to our similarity estimate. The inner product regression explicitly models uncertainty in the pairwise scaling rather than assuming that this adjustment can be performed without error.

As the pervasiveness of this frequency dependence issue suggests, sampling keywords for an analysis of meaning is a difficult prospect. The use of fixed word lists to generate word pairs tends to heighten this dependence on researchers' modeling choices (Antoniak \& Mimno 2018). For a vast range of interesting concepts, it is possible to construct many plausibly pertinent word lists with widely varying frequency distributions  (Ethayarajh, Duvenaud & Hirst 2018). The use of fixed word lists to generate word pairs tends to heighten this dependence on researchers' modeling choices. This practice with word embeddings is commonly justified in terms of the performance of these models on analogy tasks. However, direct evaluation on analogy benchmarks is rare in the applied literature. Conversely, assessments against human ratings of similarity are commonly pursued in greater depth in the social sciences (e.g. Kozlowski et al. 2019), but this external validation evidence provides little insight into whether the internal structure of a proposed comparison is appropriate given the question.

The findings in this paper have implications for replicability standards in social research with word embeddings. Neural word embedding architectures rely on training procedures that make it challenging to reconstruct the exact effective sample for a given model run. In addition to making replication itself difficult, these issues also create obstacles for researchers attempting to describe their own embedding models in sufficient detail to facilitate replication. Applications employing pre-trained models suffer in interpretability when the training code and/or corpus are unavailable. For example, the training corpora for the four publicly available GloVe models are unknown beyond the general source, and they cannot be reconstructed given what is known about the training process from the paper. These factors suggest that it is important for researchers releasing pre-trained word vectors to also provide the exact code that generates the word vectors; the corpus of texts included in the analysis; and some record of the word frequency distribution *after pre-processing* (i.e. as seen by the model). The necessity of keeping detailed records of corpus modeling choices rather than simply discarding the model after prediction is likely to continue to pose challenges as social scientists become interested in employing even more complex statistical models of sociocultural phenomena.

Cosine similarity is the similarity metric of choice when the research goal is to predict which vectors are close to a vector we know we are interested in (e.g. for classification or query response). This partially accounts for why the measure became widely used in the first place. Cosine similarity became well-known in the statistical classification literature in the 1960s (Gower 1967; Cormack 1971), but it was not especially widely used until researchers in information retrieval noticed that it was useful for nearest-neighbor classification and ranked indexing of document collections modeled as vector spaces (Salton \& Buckley 1988). It has been taken up subsequently in applied statistical text analysis as a basis for similarity measurement across a wide range of fields without much focus on whether it behaves appropriately under different data analysis procedures.

There are reasons to doubt that its applicability is so universal (Singhal, Salton & Buckley 1995; van Eck & Waltman 2009). In particular, the key motivation originally was that documents vary in length, so document-clustered word count data tended to reflect document length more than was desirable without normalization (Salton \& Buckley 1988). But, in the word-word co-frequency setting, vector length no longer reflects document length because this has been fixed by the context window hyperparameter. The scale of the vectors is dominated instead by the relative marginal frequency of the corresponding word in the corpus, up to modeling choices that cause the effective dimensionality or context window to vary (Arora et al. 2016; Ethayarajh, Duvenaud \& Hirst 2018).^[The scale variation of word vectors is the primary modern justification for preferring cosine similarity as a measure of pairwise word association. For example, Grimmer, Roberts and Stewart (2022) motivate a discussion of cosine similarity by remarking that "[a] potential problem with the inner product is that its result depends on the magnitude of the two vectors." Jurafsky and Martin (2021) similarly introduce cosine similarity by writing that "[m]ore frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency." Turney and Pantel (2010) summarize the key advantage of cosine: "...the cosine of the angle between two vectors is the inner product of the vectors, after they have been normalized to unit length. ...Cosine captures the idea that the length of the vectors is irrelevant; the important thing is the angle between the vectors." Another commonly cited justification is that cosine similarity is efficient to compute, but this only applies to the sparse LSA setting and not the dense low-rank approximations used in modern embedding analysis.] Perhaps more importantly, the typical application of the measure assumes that the computed similarity coefficients will be ignored after obtaining small number of top N matches because the coefficients themselves are not of substantive interest (i.e. we just want the "best" match and not the full spectrum of matching to not matching; see Salton \& Buckley 1988; Levy, Goldberg \& Dagan 2014). The measurement of concepts by aggregating cosine similarities makes the exact opposite assumption by placing analytic emphasis on describing the structure of location variation in this space rather than its downstream performance as an index-generating device.

This divergence in research goals is informative for thinking through the general interface between prediction and explanation in the social sciences. It has been argued that these two general research goals are, if not exactly coextensive (Watts 2014), then at least complementary (Salganik 2019; Molina \& Garip 2019; Hofman et al. 2021). But in any given practical setting, it is easy for these goals to fail to align. Adapting the design of a predictive system for explanation can lead to submerged errors in reasoning that are difficult to disentangle from the data analysis pipeline.

\pagebreak

# Works referenced

\begingroup

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{4pt}
\onehalfspacing
\noindent

Antoniak, M and D. Mimno. 2018. "Evaluating the stability of embedding-based word similarities." *Transactions of the Association for Computational Linguistics* 6, p. 107-119.

Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. 2016. "A latent variable model approach to PMI-based word embeddings." *Transactions of the Association for Computational Linguistics* 4, p. 385-399.

Arseniev-Koehler, A. and J. Foster. 2020. "Machine learning as a model for cultural learning: Teaching an algorithm what it means to be fat." arXiv preprint:https://arxiv.org/abs/2003.12133.

Bartlett, R. P. and F. Partnoy. 2020. “The Ratio Problem.” Working paper (SSRN 3605606).

Breusch, T. S., & Pagan, A. R. 1979. "A simple test for heteroscedasticity and random coefficient variation." *Econometrica* 47(5): 1287-1294.

Caillez, F., and P. Kuntz. 1996. "A contribution to the study of the metric and Euclidean structures of dissimilarities." *Psychometrika* 61(2), 241-253.

Caliskan, A, J.J. Bryson and A. Narayanan. 2017. “Semantics derived automatically from language corpora contain human-like biases.” *Science* 356(6334): 183-186.

Casson, M. C. 1973. "Linear regression with error in the deflating variable." *Econometrica* 41(4): 751-759.

Cook, R. D., and S. Weisberg. 1983. "Diagnostics for heteroscedasticity in regression." *Biometrika* 70(1), 1-10.

Cormack, R. M. 1971. "A review of classification." *Journal of the Royal Statistical Society: Series A (General)* 134(3): 321-353.

Dominich, S. 2001. *Mathematical Foundations of Information Retrieval.* Springer.

Duncan, O.D. 1984. *Notes on Social Measurement: Historical and Critical.* Russell Sage Foundation.

Ethayarajh, K., Duvenaud, D., and Hirst, G. 2018. "Understanding Undesirable Word Embedding Associations." *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics* p. 1696-1705.

Firebaugh, G., and J. P. Gibbs, J. P. 1985. "User's guide to ratio variables." *American Sociological Review* 50(5): 713-722.

Firebaugh, G., and J. P. Gibbs, J. P. 1986. "Using ratio variables to control for population size." *Sociological Methods & Research* 15(1-2): 101-117.

Fleiss, J. L., and J. Zubin, 1969. "On the methods and theory of clustering." *Multivariate Behavioral Research* 4(2): 235-250.

Goldberg, Y., and Levy, O. 2014. "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method." arXiv preprint arXiv:1402.3722.

Gower, J. C. 1967. "Multivariate analysis and multidimensional geometry." *Journal of the Royal Statistical Society: Series D (The Statistician)* 17(1): 13-28.

Grimmer, J., Roberts, M. E., and Stewart, B. M. 2022. *Text as Data: A new framework for machine learning and the social sciences.* Princeton University Press.

Hofman, J. M., *et al.* 2021. "Integrating explanation and prediction in computational social science." *Nature* 595(7866), 181-188.

Jurafsky, D. and J.H. Martin. 2021. *Speech and Language Processing* (3rd edition). https://web.stanford.edu/~jurafsky/slp3/

Koenker, R. 1981. "A note on studentizing a test for heteroscedasticity." *Journal of Econometrics* 17(1), 107-112.

Kozlowski, A. C., Taddy, M., and Evans, J. A. 2019. "The geometry of culture: Analyzing the meanings of class through word embeddings." *American Sociological Review* 84(5), 905-949.

Kronmal, R. A. 1993. "Spurious correlation and the fallacy of the ratio standard revisited." *Journal of the Royal Statistical Society: Series A (Statistics in Society)* 156(3): 379-392.

Levy, O., and Y. Goldberg. 2014. "Neural word embedding as implicit matrix factorization." *Advances in neural information processing systems* 27.

Levy, O., Y. Goldberg, and I. Dagan. 2015. "Improving distributional similarity with lessons learned from word embeddings." *Transactions of the Association for Computational Linguistics* 3, 211-225.

Long, J. S. and L. H. Ervin. 2000. "Using heteroscedasticity consistent standard errors in the linear regression model." *The American Statistician* 54(3): 217-224.

Lowe, W. 2001. "Towards a theory of semantic space." *Proceedings of the Annual Meeting of the Cognitive Science Society 23*.

Lundberg, I., Johnson, R., and Stewart, B. M. 2021. "What is your estimand? Defining the target quantity connects statistical evidence to theory." *American Sociological Review* 86(3), 532-565.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. 2013. "Distributed representations of words and phrases and their compositionality." *Advances in neural information processing systems* 26.

Mimno, D., and Thompson, L. 2017. "The strange geometry of skip-gram with negative sampling." *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, p. 2873-2878.

Mnih, A., and Hinton, G. E. 2008. "A scalable hierarchical distributed language model." *Advances in neural information processing systems* 21.

Mohr, John W., Christopher A. Bail, Margaret Frye, Jennifer C. Lena, Omar Lizardo, Terence E. McDonnell, Ann Mische, Iddo Tavory, and Frederick F. Wherry. 2020. *Measuring Culture.* Columbia University Press.

Molina, M., & Garip, F. 2019. "Machine learning for sociology." *Annual Review of Sociology* 45: 27-45.

Monk Jr, E. P. 2022. "Inequality without Groups: Contemporary Theories of Categories, Intersectional Typicality, and the Disaggregation of Difference." *Sociological Theory* 40(1): 3-27.

Mu, J. and Viswanath, P. 2018. "All-but-the-top: Simple and effective postprocessing for word representations." *6th International Conference on Learning Representations, ICLR 2018*.

Nelson, L. K. 2020. "Computational grounded theory: A methodological framework." *Sociological Methods & Research* 49(1), 3-42.

Nelson, L. K. 2021. "Leveraging the alignment between machine learning and intersectionality: Using word embeddings to measure intersectional experiences of the nineteenth century US South." *Poetics* 88.

Pechenick, E. A., C. M. Danforth, and P. S. Dodds. 2015. "Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution." *PloS one* 10(10): e0137041.

Pennington, J., Socher, R., and Manning, C. D. 2014. "Glove: Global vectors for word representation." *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*, p. 1532-1543.

Piantadosi, S. T. 2014. "Zipf’s word frequency law in natural language: A critical review and future directions." *Psychonomic Bulletin & Review* 21(5), 1112-1130.

Rodriguez, P. L., & Spirling, A. 2022. "Word embeddings: What works, what doesn’t, and how to tell the difference for applied research." *The Journal of Politics* 84(1).

Rodriguez, P. L., A. Spirling, and B. M. Stewart. 2022. "Embedding Regression: Models for Context-Specific Description and Inference." Working paper.

Salton, G., and Buckley, C. 1988. "Term-weighting approaches in automatic text retrieval." *Information Processing & Management* 24(5), 513-523.

Singhal, A., Salton, G., and Buckley, C. 1995. "Length normalization in degraded text collections." Technical report, Cornell University.

Turney, P. D., and Pantel, P. 2010. "From frequency to meaning: Vector space models of semantics." *Journal of Artificial Intelligence Research* 37, 141-188.

van Eck, N. J., and Waltman, L. 2009. "How to normalize cooccurrence data? An analysis of some well‐known similarity measures." *Journal of the American Society for Information Science and Technology* 60(8), 1635-1651.

van Loon, A., Giorgi, S., Willer, R., and Eichstaedt, J. 2022. "Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency." arXiv preprint arXiv:2201.08451.

Watts, D. J. 2014. "Common sense and sociological explanations." *American Journal of Sociology* 120(2): 313-351.

Western, B. and D. Bloome. 2009. "Variance function regressions for studying inequality." *Sociological Methodology* 39(1), 293-326.

White, H. C. 2008. *Identity and Control: How social formations emerge.* Princeton University Press.

Xie, Y. 2013. "Population heterogeneity and causal inference." *Proceedings of the National Academy of Sciences* 110(16), 6262-6268.

Zhou, K., Ethayarajh, K., and Jurafsky, D. 2021. "Frequency-based distortions in contextualized word embeddings." arXiv preprint arXiv:2104.08465.

\endgroup

\pagebreak

# Supplementary material

## Appendix A: Additional tables and figures, Nelson (2021)

The regression tables corresponding to the model comparisons displayed in Figure \ref{fig:nelson_diffinmeans} are included below. All standard errors are computed using the HC3 estimator (Long \& Ervin 2000), which adjusts each observation by its residual weighted by a function of its leverage score ($\hat e^2_i / (1 - h_{ii})^2$). The LM statistic reported for each model is the studentized Breusch-Pagan statistic, which tests the null hypothesis of homoskedasticity (Breusch \& Pagan 1979; Koenker 1981); there is usually strong evidence to suggest the inner product regression model is heteroskedastic over the frequency distribution in this setting. There appears to be a bias-variance correlation in the construction of the social institution subspaces; lower estimated frequency-related error variance in the corrected model is associated with larger average total frequency bias in the uncorrected model. (For example, note the relatively low LM statistics in the \texttt{domestic} models.) The social identity comparison subspaces are relatively similar in their frequency error correlation profiles, in part because they tend to share some proportion of vectors. Figure \ref{fig:lfbias_coca_estimates} displays the estimates for the model substituting COCA frequencies for the corpus frequencies.

To visualize the degree of frequency bias, we can plot the word frequency estimate against the distribution of cosine similarities to the composed vectors that structure the analysis. Figure \ref{fig:nelson_focal_composite} shows the resulting scatterplot for the social category mean vectors; Figures \ref{fig:nelson_frequencycomp_institutions} and \ref{fig:nelson_authority_mv} display the relationship for the social institution unigram/sum vectors. Each plot shows that the mean cosine similarity is non-constant over the word frequency distribution, and tends to be systematically higher for shorter vectors (less frequent words). Another intriguing feature of the measure is the systematic distortion in the shape of the cosine projection with respect to frequency when the focal vector is composed of more word vectors. The manifold shape in Figure \ref{fig:nelson_focal_composite} is markedly shifted upwards on the extreme ends of the word frequency distribution relative to the lower-dimensional focal vectors in Figure \ref{fig:nelson_frequencycomp_institutions}. The figures jointly suggest that the cosine similarity to sum and mean vectors is systematically frequency biased in a way that increases in proportion to the number of terms included in the vector. The large subspaces implied by the vectors in Figure 2 correspond to cosine distributions that are more skewed in the extreme regions of the frequency distribution. 

In semantic vector space analysis, word vectors have Euclidean norms that scale with their frequencies. Each word vector has been selected such that its scale optimally represents this information. The easiest way to see this is to plot the vector norms and word frequency estimates against each other (Arora et al. 2016). This pattern is reproduced in Euclidean-arithmetic comparisons of word vectors, and it can be visualized by plotting the pairwise product of norms (the local normalization weight, $\phi_{AB}$) against the pairwise product of frequencies. Figure \ref{fig:nelson_aroraplot} shows these relationships side-by-side for a random sample of term pairs.

This relationship implies that the global distribution of cosine similarity with respect to frequency is not uniform. Figure \ref{fig:nelson_cosfreq} plots cosine similarity against the product of root-log frequencies, optionally split by local normalization weight quantiles. We observe the predicted relationship: mean cosine similarity is systematically larger in the rare region of the word frequency distribution. The degree of association between mean cosine similarity and frequency is not constant; the association is low among pairs representing proportionally more common word and high among pairs representing proportionally more rare words. Also, observe that the relationship also has non-constant variance: the range of cosine similarities observed at a given scale varies smoothly by scale. Figure \ref{fig:global_mean_composite} visualizes the difference in frequency biases between the inner product and cosine similarity.

Mu and Viswanath (2018) suggest that the vector spaces learned by classic word embeddings exhibit improved cosine semantics when the global mean vector is subtracted from the space. Figure \ref{fig:global_composite_demeaning} illustrates the effect of global demeaning on frequency bias in Nelson (2021) relative to the frequency distributions of cosine similarity and the inner product. Removing the mean vector improves the frequency uniformity of the cosine neighborhoods, but note that there is still some skew in the conditional distribution. Some skew is unavoidable due to the dependence of the shape of the sampling distribution on the dimensionality of the vector space.

\pagebreak

```{r lfbias_coca_estimates, echo=F, message=F, fig.height=8.8, out.width="\\textwidth", fig.align="center", fig.cap="Change in estimated difference in mean cosine similarity between paired social category mean vectors and social institution vectors (Nelson 2021). Original model estimate in blue and COCA-estimated frequency-adjusted linear model estimate in purple with parametric 95\\% confidence intervals superimposed. Labels indicate $\\text{R}^2$ in the original (left) and adjusted (right) models. The change in the difference in means reflects the relative difference between the implicit word cofrequency distributions $P(Category(A), Institution)$ and $P(Category(B), Institution)$."}
# Show linear model frequency bias 
linear_frequency_bias <- function(dimdf, dimtitle, use.scale=F, use.nprod=T, robust=F, ftest=F, stargazer=T, focal.p=F, only.dim=F, fullsumm=F, delta.rsq=F) {
  if(use.scale) {
    dimdf$fbmeasure <- dimdf$snorm^2
  } else if(use.nprod) {
    dimdf$fbmeasure <- 1/dimdf$nprod  
  } else {
    dimdf$fbmeasure <- log(dimdf$frequency)
  }
  ols <- lm(cs ~ identity, data=dimdf)
  ols.freqbias <- lm(cs ~ identity * fbmeasure, data=dimdf)
  ols.fb2 <- lm(cs ~ identity * fprod, data=dimdf %>% mutate(fprod=1/fprod))
  # ols.fb2 <- lm(ip ~ identity * snorm, data=dimdf)
  # ols.fb2 <- lm(cs ~ identity * fbmeasure + snorm, data=dimdf %>% mutate(snorm=1/snorm))
  ols.freqbias2 <- lm(ip ~ identity * nprod, data=dimdf)
  hsk <- c(bptest(ols)$statistic,
           bptest(ols.freqbias)$statistic,
           bptest(ols.fb2)$statistic,
           bptest(ols.freqbias2)$statistic)
  hskp <- c(stargz(bptest(ols)$p.value),
            stargz(bptest(ols.freqbias)$p.value),
            stargz(bptest(ols.fb2)$p.value),
            stargz(bptest(ols.freqbias2)$p.value))
  chsk <- c("LM Statistic", paste0(paste0(round(hsk, 3), sep=""), hskp, sep=""))
  if(ftest) {
    anova(ols, ols.freqbias)
  } else if(stargazer) {
    stargazer::stargazer(ols, ols.freqbias, ols.fb2, ols.freqbias2,
                         title=dimtitle,
                         # dep.var.labels=c("Cosine similarity"),
                         dep.var.labels=c("Cosine similarity", "Inner product"),
                         dep.var.caption="",
                         covariate.labels=c("Social category vector",
                                            "Local normalization weight",
                                            "Identity \\times \\ LNW",
                                            # "||A||",
                                            # "Identity \\times \\ ||A||",
                                            "Scaled log COCA frequency",
                                            "Identity \\times \\ log(freq)",
                                            "Norm product",
                                            "Identity \\times \\ NP",
                                            "Constant"),
                         se=list(sqrt(diag(vcovHC(ols, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.freqbias, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.fb2, type="HC3"))),
                                 sqrt(diag(vcovHC(ols.freqbias2, type="HC3")))),
                         add.lines=list(chsk),
                         star.cutoffs=c(0.05, 0.01, 0.001),
                         omit.stat=c("adj.rsq"),
                         single.row = T,
                         column.sep.width = "1pt",
                         font.size = "footnotesize",
                         header=F)
  } else if(focal.p) {
    summary(polity_mb_fb.freqbias)$coefficients[2,4]
  } else if(only.dim) {
    # which.mod <- ols.freqbias  # Default
    which.mod <- ols.fb2  # Shows the figure with the COCA frequency estimate instead
    return(c(comparison = paste0(unique(dimdf$identity)),
             # r2=summary(which.mod)$r.squared,
             r2=sprintf("R²: %s, %s", round(summary(ols)$r.squared, 3), round(summary(which.mod)$r.squared, 3)),
             coef(summary(which.mod))[2,1:2]))
  } else if(fullsumm) {
    if(robust) {
      print(coeftest(ols, vcov = vcovHC(ols, type="HC1")))
      print(coeftest(ols.freqbias, vcov = vcovHC(ols.freqbias, type="HC1")))
    } else {
      print(summary(ols))
      print(summary(ols.freqbias))
      summarize_ovb(dimdf)
    }
  } else if(delta.rsq) {
    data.frame(rsq=c(summary(ols)$r.squared, summary(ols.freqbias)$r.squared),
               model=c("original model", "frequency interaction"))
  } else {
    rbind.data.frame(data.frame(summary(ols)$coefficients, model="original model"),
                     data.frame(summary(ols.freqbias)$coefficients, model="frequency interaction")) %>%
      mutate(sig=tsig(`Pr...t..`))
  }
}

# Show linear model frequency bias

# Frequency bias tends to create omitted variable bias because frequency is correlated
#  with the cosine similarity and the relative frequency distributions differ by group
# Adding a measure of frequency into the model tends to increase the standard error on the
#  grouping variable in the difference in mean, and the model fit goes up a lot
# Note that this doesn't always make the effect "go away" per se; the problem is that
#  frequency has a somewhat unpredictable relationship to this problem because it depends
#  on the exact difference in mean log frequency by group and the linear correlation of 
#  the log frequency with the cosine similarity
# In general the reciprocal local normalization weight is the best frequency measure to use
#  because it's a component of cosine similarity, but any functional form will work; the 
#  key thing here is that the LPNW tends to vary by group (frequency bias is relative)
rbind(

# Black male <-> Black female
data.frame(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), comparison="Black, Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), comparison="Black, Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), comparison="Black, Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), comparison="Black, Male x Black, Female (domestic)"),

# White male <-> White female
data.frame(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), comparison="White Male x White Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), comparison="White Male x White Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), comparison="White Male x White Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), comparison="White Male x White Female (domestic)"),

# White female <-> Black female
data.frame(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), comparison="White Female x Black, Female (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), comparison="White Female x Black, Female (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), comparison="White Female x Black, Female (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), comparison="White Female x Black, Female (domestic)"),

# Black male <-> White male
data.frame(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), comparison="Black, Male x White Male (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), comparison="Black, Male x White Male (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), comparison="Black, Male x White Male (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), comparison="Black, Male x White Male (domestic)"),

# White male <-> Black female
data.frame(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), comparison="White Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), comparison="White Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), comparison="White Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), comparison="White Male x Black, Female (domestic)"),

# White female <-> Black male
data.frame(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), comparison="White Female x Black, Male (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), comparison="White Female x Black, Male (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), comparison="White Female x Black, Male (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), comparison="White Female x Black, Male (domestic)"),

# Authority (Fig. 5)
data.frame(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), comparison="Black, Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), comparison="White Male x White Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), comparison="White Female x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), comparison="Black, Male x White Male (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), comparison="White Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), comparison="White Female x Black, Male (authority)")) ->
  all_comparisons

# Get all of the regression results in one place
# This is unspeakably ugly but oh well
rbind(

# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)", only.dim=T, stargazer=F),

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)", only.dim=T, stargazer=F),

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)", only.dim=T, stargazer=F),

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)", only.dim=T, stargazer=F)) ->
  fblm_results_f

institutions <- c("polity", "economy", "culture", "domestic", "authority")
data.frame(fblm_results_f) %>%
  rowwise() %>%
  mutate(institution = institutions[which(str_detect(comparison1, institutions))],
         comparison = str_replace_all(paste0(str_remove(comparison2, institution), "- ",
                                             str_remove(comparison1, institution)), "_", " "),
         comparison2 = str_replace_all(paste0(str_remove(comparison1, institution), "- ",
                                              str_remove(comparison2, institution)), "_", " ")) %>%
  rename(lm.est.dim = Estimate, lm.est.se = Std..Error) ->
  fblm_results

# Plot all comparisons
all_comparisons %>%
  mutate(institution=str_remove_all(str_extract(comparison, "\\([a-z]+\\)"), "[()]")) %>%
  group_by(institution, identity) %>%
  summarize(mean_cs = mean(cs), sd_cs = sd(cs)) %>%
  ungroup() ->
  ac_summ

ac_summ %>%
  left_join(ac_summ, by="institution") %>%
  filter(!duplicated(paste0(pmin(identity.x, identity.y),
                            pmax(identity.x, identity.y))) &
           identity.x != identity.y) %>%
  mutate(diffm_cs = mean_cs.y - mean_cs.x,
         diff.se_cs = sqrt(sd_cs.x^2/k + sd_cs.y^2/k),
         comparison = str_replace_all(paste0(str_remove(identity.x, institution), "- ",
                                             str_remove(identity.y, institution)), "_", " ")) %>%
  left_join(fblm_results, by=c("comparison", "institution")) %>%
  left_join(fblm_results, by=c("comparison"="comparison2", "institution")) %>%
  mutate(lm.est.dim = as.numeric(coalesce(lm.est.dim.x, lm.est.dim.y)),
         lm.est.se = as.numeric(coalesce(lm.est.se.x, lm.est.se.y)),
         # r2 = as.numeric(coalesce(r2.x, r2.y))) ->
         r2 = coalesce(r2.x, r2.y)) ->
  nemb_dim_estimates

nemb_dim_estimates %>%
  rowwise() %>%
  mutate(label.pin.x = max(lm.est.dim+(1.96*lm.est.se), diffm_cs+(1.96*diff.se_cs)) + 0.15, # + 0.035,
         # label.text = sprintf("R² = %s", round(r2, 3))) %>%
         label.text = r2) %>%
  ggplot() +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_point(aes(y=comparison, x=diffm_cs), color="steelblue2",
             position=position_nudge(y=0.1)) +
  geom_errorbarh(aes(y=comparison,
                     xmin=diffm_cs - 1.96*diff.se_cs,
                     xmax=diffm_cs + 1.96*diff.se_cs),
                 height=0.2, color="steelblue2", position=position_nudge(y=0.1)) +
  geom_point(aes(y=comparison, x=lm.est.dim), color="darkorchid3",
             position=position_nudge(y=-0.1)) +
  geom_errorbarh(aes(y=comparison,
                   xmin=lm.est.dim - 1.96*lm.est.se,
                   xmax=lm.est.dim + 1.96*lm.est.se),
               height=0.2, color="darkorchid3", position=position_nudge(y=-0.1)) +
  geom_label(aes(label=label.text, y=comparison, x=label.pin.x), size=3, color="darkorchid3") +
  scale_x_continuous(limits=c(NA,0.6), n.breaks=10) +
  facet_wrap(~institution, ncol=1) +
  labs(x="Difference in mean cosine similarity", y="")
```

\pagebreak

```{r nelson_lfb_tables, echo=F, message=F, results="asis"}
# Print all tables for Nelson (2021) cosine differences
tsig <- function(p) {
  ifelse(p < 0.001, "***",
         ifelse(p < 0.01, "**",
                ifelse(p < 0.05, "*", "x")))
}

summarize_ovb <- function(dimdf) {
  print(sprintf("Linear correlation of cosine similarity and log word frequency, %s: %f", unique(dimdf$identity)[1],
                cor(dimdf %>% slice_head(n=k) %$% cs, log(dimdf %>% slice_head(n=k) %$% log(frequency)))))
  print(sprintf("Linear correlation of cosine similarity and log word frequency, %s: %f",unique(dimdf$identity)[2],
                cor(dimdf %>% slice_tail(n=k) %$% cs, log(dimdf %>% slice_tail(n=k) %$% log(frequency)))))
  print(sprintf("Covariance of local normalization weight and group (i.e. relative frequency): %f",
                cov(dimdf$identity == unique(dimdf$identity)[1], dimdf$nprod)))
}


# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)")

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)")

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)")

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)")
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)")
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)")
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)")

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)")

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)")
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)")
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)")
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)")

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)")
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)")
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)")
```

\pagebreak

```{r nelson_focal_composite, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, social category mean vectors. Color indicates norm of context word; note increasing vector lengths by word frequency."}
# All of the mean vectors exhibit frequency bias as well
ggarrange(plot_composite_view(view_from_composite(nemb, mv_male_black), "male + black", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_female_black), "female + black", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_male_white), "male + white", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_female_white), "female + white", mv=T),
          ncol=2, nrow=2, common.legend=T, legend = "bottom")
```

```{r nelson_frequencycomp_institutions, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, social institution inducing vectors. Color indicates norm of context word; note increasing vector lengths by word frequency. Linear fit above red dotted line shows trend in top 50 vectors selected for analysis."}
# It's also useful to look at what we're averaging over
# The biases on each of the component 2sum vectors vary
# Not shown in the paper
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["man",] + nemb["black",])), "man + black")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["his",] + nemb["black",])), "his + black")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["herself",] + nemb["colored",])), "herself + colored")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["she",] + nemb["white",])), "she + white")

# Also look at the "social institution" vectors
# Top 50 terms thresholds, respectively: 0.638, 0.622, 0.665, 0.665
ggarrange(plot_composite_view(view_from_composite(nemb, v_polity), "nation + state", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_economy), "money", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_culture), "culture", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_domestic), "housework + children", topsel = 50),
          ncol=2, nrow=2, common.legend=T, legend = "bottom")
```

```{r nelson_authority_mv, echo=F, message=F, fig.height=5, out.width="0.75\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, \\texttt{authority} vector. Color indicates norm of context word; note increasing vector lengths by word frequency. Linear fit above red dotted line shows trend in top 50 vectors selected for analysis."}
plot_composite_view(view_from_composite(nemb, v_authority), "authority", topsel = 50)
```

```{r nelson_aroraplot, echo=F, message=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Squared Euclidean norm of embedding vectors against root-log frequency for all words in the underlying corpus, with median log frequency indicated by red dashed line (top panel). local normalization weight against product of root-log frequencies for all word pairs in the corpus; uniformly random sample of 5000 word vectors without replacement (bottom panel). Note inflection point and changes in variance over the trend in both panels."}
# Function to plot squared norm as a function of log frequency
# This is Fig. 2 in Arora et al. (2016)
# Also see Eqn. 2.4: log p(w) ~ ||w||/2d - log Z (plus error),
#  where Z = sum exp <w, c>, which is nearly to constant for any choice of context/word
# This is the integral of the softmax function wrt the inner product space; PMI is a function
#  of a Euclidean distance that has been discounted by the relative maximum distance of this
#  point to everything else (log sum exp over the inner product space at this context) as well
#  as the dimensionality of the inner product space
aroraplot <- function(embm, termfreqs, vocab) {
  termfreqs %>%
    filter(feature %in% vocab) %>%
    rowwise() %>%
    mutate(snorm = norm(embm[feature,], "2"))%>%
    ggplot(aes(x=log(frequency), y=snorm^2)) +
    geom_point() +
    geom_vline(xintercept=median(log(termfreqs$frequency)), linetype="dashed", color="tomato") +
    geom_smooth(method="gam") +
    labs(x="Word frequency (log)", y="Squared Euclidean norm", color="Cosine similarity")
}

# Alternatively, use the random angle set and color by cosine similarity
lnpw_plot <- function(r_angles) {
  r_angles %>%
    arrange(ab_cs) %>%
    ggplot(aes(x=sqrt(log(a$frequency))*sqrt(log(b$frequency)), y=nprod, color=ab_cs)) +
    geom_point() +
    geom_smooth() +
    scale_color_viridis_c() +
    theme(legend.position = "bottom") +
    labs(x="Frequency product (log)",
         y="Local normalization weight")
}

# Squared vector norm is proportional to sqrt log term frequency
# The relationship in this model is a little weird for the high frequency terms
# It should be approximately curvilinear but it's quadratic (???)
aroraplot(nemb, nt_freq, nemb.tok) -> fbp1

# Similarly, local normalization weight is proportional to product of sqrt log term frequency
lnpw_plot(ra_nemb) -> fbp2

# Plot these side-by-side
ggarrange(fbp1, fbp2, ncol=1)
```

```{r nelson_cosfreq, echo=F, message=F, fig.height=6, out.width="\\textwidth", fig.align="center", fig.cap="Distribution of cosine similarities by product of log frequencies of component words. Linear trend in blue and generalized additive fit in red; points colored by local normalization weight of this word pair. Right panel splits left panel by local normalization weight quantiles; note partial overlap in frequency product. Cosine similarity is inflated on average when frequency is low and deflated on average when frequency is high."}
# Plot cosine simlarity against the log frequency ratio
# I think this one kinda looks like a peacock.
# If you split by LPNW quantiles, you can see the relationship get (close to) flat
ra_nemb %>%
  ungroup() %>%
  mutate(lfr = sqrt(log(a$frequency)) * sqrt(log(b$frequency))) %>%
  arrange(nprod) %>%
  ggplot(aes(x=lfr, y=ab_cs, color=nprod)) +
  geom_point(size=3) +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  geom_smooth(method="lm") +
  geom_smooth(color="tomato") +
  scale_color_viridis_c() +
  labs(x="Freq. ratio (log)", y="Cosine similarity", color="LNW") -> rasqf1
ra_nemb %>%
  ungroup() %>%
  mutate(lfr = log(a$frequency) * log(b$frequency),
         ile = ntile(nprod, 9)) %>%
  arrange(nprod) %>%
  ggplot(aes(x=lfr, y=ab_cs, color=nprod)) +
  geom_point(size=3) +
  geom_smooth(method=MASS::rlm, method.args=list(method="MM"), color="tomato") +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  scale_color_viridis_c() +
  facet_wrap(~ile) +
  theme(legend.position="bottom") +
  labs(x="Freq. ratio (log)", y="Cosine similarity", color="LNW")  -> rasqf2
ggarrange(rasqf1, rasqf2, ncol=2, common.legend = T, legend="bottom")
```

```{r global_mean_composite, echo=F, warning=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Frequency bias in distribution of cosine similarity (top panel) and inner product (bottom panel) between context words and global mean vector; point color indicates squared norm of context word. Cosine similarity bends the region of the inner product space containing the short/infrequent vectors toward the mean vector."}
global_mean_vector <- colMeans(nemb)
view_from_composite(nemb, global_mean_vector) %>%
  mutate(freq = sqrt(log(frequency))) ->
  gmv

# Append inner product
# Takes a while to compute all of them
gmv <- add_composite_ip(gmv, nemb, global_mean_vector)

# Plot
gmv %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=similarity, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Frequency-cosine plot, global mean vector",
       x="Word frequency (sqrt log)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb1
gmv %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=inner_product, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv$inner_product, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  scale_y_reverse() +
  theme(legend.position="bottom") +
  labs(title="Frequency-inner product plot, global mean vector",
       x="Word frequency (sqrt log)",
       y="Inner product",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb2
ggarrange(gfb1, gfb2, ncol=1, common.legend = T, legend="bottom")
```


```{r global_mean_csbend, echo=F, warning=F, fig.height=6, out.width="0.8\\textwidth", fig.align="center", fig.cap="Mean cosine similarity at observed frequencies (sqrt log); piecewise linear fit split at $\\sqrt{\\log p(w_i)} = 2.3$ overlaid in red, and points are color scaled by their squared Euclidean norm. Vectors with low frequencies (fewer than approximately 200 occurrences) exhibit a strong negative linear correlation with high mean cosine similarity to the global mean vector ($\\rho=-0.9863$). High frequencies are minimally correlated with cosine similarity. The variance in mean cosine similarity is much higher in the common word region, and there is visible residual bias in the scale-cosine relationship among words with similarly high frequencies.", include=F}
# Compute means
gmv %>%
  group_by(frequency) %>%
  summarize(n=n(), mean_sim=mean(similarity), mean_norm=mean(snorm), freq=first(freq)) %>%
  mutate(regime=freq <= 2.8) ->
  gmv_means

# Slope of just the negative linear component gives the rarity bias coefficient
# 0.1 units of change in sqrt-log frequency implies a -0.0351 change in mean cosine to the mean vector
#  for terms satisfying p(w) <= 200; note R2=0.973, but also note visible bias
# summary(lm(mean_sim ~ freq, data=gmv_means %>% filter(regime)))
# cor(gmv_means %>% filter(regime) %$% freq, gmv_means %>% filter(regime) %$% mean_sim, use="complete.obs")

# Plot
gmv_means %>%
  arrange(mean_norm) %>%
  ggplot(aes(x=freq, y=mean_sim, color=mean_norm)) +
  geom_point() +
  # geom_smooth(aes(group=regime), method="m", color="tomato", linetype="dashed", alpha=0.5) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Mean cosine-frequency, global mean vector",
       x="Word frequency (sqrt log)",
       y="Mean cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$"))
```

```{r global_composite_demeaning, echo=F, warning=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Centering the vector space (see Mu \\& Viswanath 2018) partially alleviates frequency bias, particularly in the top-N similarity region, but note non-constant variance and persistence of curvature (compare middle to top panel). The frequency bias profile of the demeaned space is closer to the behavior of the inner product, but the semantic behavior in the rare word region is notably less skewed from a top-N perspective than the inner product (compare middle to bottom panel)."}
# Compute demeaned mean vectors for comparison
nemb.demean <- float::sweep(nemb, 2, global_mean_vector)
# gmv2 <- view_from_composite(nemb.demean, global_mean_vector) %>% mutate(freq=sqrt(log(frequency)))
gmv2 <- view_from_composite(nemb.demean, colMeans(nemb.demean)) %>% mutate(freq=sqrt(log(frequency)))

gmv2 %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=similarity, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv2$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv2$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Frequency-cosine plot, demeaned-global mean vector (origin)",
       x="Word frequency (sqrt log)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb3
ggarrange(gfb1, gfb3, gfb2, ncol=1, common.legend = T, legend="bottom")
```

\pagebreak

## Appendix B: Additional figures, Kozlowski et al. (2019)

Figures \ref{fig:kte_freqspectra} and \ref{fig:kte_freqspectra2} compare the scale-frequency distributions implied by each vector subspace in the analysis. Figure \ref{fig:kte_cof} displays the distribution of frequency ratios in each subspace, and Figure \ref{fig:kte_scalefreq} displays the relationship between this ratio and the corresponding normalization weight. Figure \ref{fig:kte_omegan} shows the intrinsic bias distribution of the mean vector cosine relative to the mean cosine for randomly drawn subspaces with mean vectors $\Omega(2)$ and $\Omega(5)$. Note that the subspaces in the paper (and subspaces usually specified in the literature) are larger than this, often on the order of $\Omega(50)$.

\pagebreak

```{r kte_freqspectra, echo=F, warning=F, fig.height=10, out.width="0.8\\textwidth", fig.align="center", fig.cap="Distribution of word frequency product for subspace term pairs."}
# Plot each comparison's frequency spectrum
# Fixed X axis facilitates comparison easier
# Note that the scale-frequency relationship is not itself constant
plot_comp_freq <- function(vs, tlab) {
  vs %>%
    ggplot(aes(x=sqrt(log(freq.dominant))*sqrt(log(freq.subordinate)),
               y=norm.dominant * norm.subordinate)) +
    geom_point() +
    geom_smooth(method="lm") +
    ggtitle(tlab) +
    labs(x="Freq. product (log)", y="LNW") +
    theme(legend.position = "none") +
    xlim(7.5, 22)
}

plot_comp_scale <- function(vs, tlab) {
  vs %>%
    ggplot(aes(y=sqrt(log(freq.dominant))*sqrt(log(freq.subordinate)),
               x=norm.dominant * norm.subordinate)) +
    geom_point() +
    geom_smooth(method="lm") +
    ggtitle(tlab) +
    labs(y="Freq. product (log)", x="LNW") +
    theme(legend.position = "none") +
    xlim(8, 50)
    # ylim(7.5, 22)
}

ggarrange(plot_comp_freq(affl_f, "Affluence"),
          plot_comp_freq(cult_f, "Cultivation"),
          plot_comp_freq(educ_f, "Education"),
          plot_comp_freq(empl_f, "Employment"),
          plot_comp_freq(good_f, "Morality"),
          plot_comp_freq(stat_f, "Status"),
          plot_comp_freq(gend_f, "Gender"),
          ncol=1)

```

```{r kte_freqspectra2, echo=F, warning=F, fig.height=10, out.width="0.8\\textwidth", fig.align="center", fig.cap="Distribution of word frequency product for subspace term pairs."}
ggarrange(plot_comp_scale(affl_f, "Affluence"),
          plot_comp_scale(cult_f, "Cultivation"),
          plot_comp_scale(educ_f, "Education"),
          plot_comp_scale(empl_f, "Employment"),
          plot_comp_scale(good_f, "Morality"),
          plot_comp_scale(stat_f, "Status"),
          plot_comp_scale(gend_f, "Gender"),
          ncol=1)
```

```{r kte_cof, echo=F, warning=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Distribution of word frequency ratio between component vectors of $\\Omega(2)$ components of each class term subspace."}
# Distribution of paired co-frequencies
cof_dist <- function(cultd, tlab) {
  cultd %>%
    ggplot(aes(x=log(freq.dominant),y=log(freq.subordinate))) +
    geom_point() +
    geom_abline(intercept=0, slope=1, linetype="dashed") +
    geom_smooth(method="lm") +
    labs(x="Log freq. (dominant)",
         y="Log freq. (subordinate)",
         title=tlab)
}

ggarrange(cof_dist(affl_f, "Affluence"),
          cof_dist(cult_f, "Cultivation"),
          cof_dist(educ_f, "Education"),
          cof_dist(empl_f, "Employment"),
          cof_dist(good_f, "Morality"),
          cof_dist(stat_f, "Status"),
          cof_dist(gend_f, "Gender"))
```

```{r kte_scalefreq, echo=F, warning=F, fig.height=9, out.width="\\textwidth", fig.align="center", fig.cap="Variation in scale-frequency relationship by subspace."}
dim_colors <- RColorBrewer::brewer.pal(7, "Dark2")
names(dim_colors) <- c("Affluence", "Cultivation", "Education", "Employment", "Morality", "Status", "Gender")
biplot_comp <- function(vs1, vs2, vl1, vl2) {
  vs1 %<>% rename(dominant=1, subordinate=2) %>% mutate(cdim=vl1)
  vs2 %<>% rename(dominant=1, subordinate=2) %>% mutate(cdim=vl2)
  vs <- rbind.data.frame(vs1, vs2)
  vs %>%
    ggplot(aes(x=sqrt(log(freq.dominant))*sqrt(log(freq.subordinate)),
               y=norm.dominant * norm.subordinate)) +
    geom_point(aes(color=cdim)) +
    geom_smooth(aes(color=cdim), method="lm") +
    geom_smooth(method="lm", color="grey50", linetype="dashed") +
    scale_color_manual(values=dim_colors) +
    labs(title=paste0(c(vl1, vl2), collapse=", "),
         x="", y="") +
    theme(legend.position = "none",
          plot.title=element_text(size=5),
          plot.margin = unit(c(2,2,2,2), "pt"))
}

m <- matrix(NA, 6, 6)
m[lower.tri(m, diag = T)] <- 1:21
plots <- list(biplot_comp(affl_f, cult_f, "Affluence", "Cultivation"),
          biplot_comp(affl_f, educ_f, "Affluence", "Education"),
          biplot_comp(affl_f, empl_f, "Affluence", "Employment"),
          biplot_comp(affl_f, good_f, "Affluence", "Morality"),
          biplot_comp(affl_f, stat_f, "Affluence", "Status"),
          biplot_comp(affl_f, gend_f, "Affluence", "Gender"),
          biplot_comp(cult_f, educ_f, "Cultivation", "Education"),
          biplot_comp(cult_f, empl_f, "Cultivation", "Employment"),
          biplot_comp(cult_f, good_f, "Cultivation", "Morality"),
          biplot_comp(cult_f, stat_f, "Cultivation", "Status"),
          biplot_comp(cult_f, gend_f, "Cultivation", "Gender"),
          biplot_comp(educ_f, empl_f, "Education", "Employment"),
          biplot_comp(educ_f, good_f, "Education", "Morality"),
          biplot_comp(educ_f, stat_f, "Education", "Status"),
          biplot_comp(educ_f, gend_f, "Education", "Gender"),
          biplot_comp(empl_f, good_f, "Employment", "Morality"),
          biplot_comp(empl_f, stat_f, "Employment", "Status"),
          biplot_comp(empl_f, gend_f, "Employment", "Gender"),
          biplot_comp(good_f, stat_f, "Morality", "Status"),
          biplot_comp(good_f, gend_f, "Morality", "Gender"),
          biplot_comp(stat_f, gend_f, "Status", "Gender"))
gridExtra::grid.arrange(grobs = plots, layout_matrix = m, bottom="Freq. product (log)", left="LNW")
```

```{r kte_omegan, echo=F, warning=F, fig.height=4, out.width="0.9\\textwidth", fig.align="center", fig.cap="Cosine similarity estimate bias distribution for mean vectors of size 2 (left) and 5 (right)."}
# Show 2-7 composed vectors
j <- 2000
ak <- kemb[sample(rownames(kemb), j),]
bk <- kemb[sample(rownames(kemb), j),]
ck <- kemb[sample(rownames(kemb), j),]
dk1 <- kemb[sample(rownames(kemb), j),]
dk2 <- kemb[sample(rownames(kemb), j),]
dk3 <- kemb[sample(rownames(kemb), j),]
dk4 <- kemb[sample(rownames(kemb), j),]
dk5 <- kemb[sample(rownames(kemb), j),]

data.frame(i=1:2000) %>%
  rowwise() %>%
  mutate(an = norm(ak[i,], "2"),
         bn = norm(bk[i,],"2"),
         abn = norm(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]))), "2"),
         cn = norm(ck[i,], "2"),
         ip = as.numeric(ak[i,]) %*% as.numeric(bk[i,]),
         scs = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]))), as.numeric(ck[i,])),
         acs = lsa::cosine(as.numeric(ak[i,]), as.numeric(ck[i,])),
         bcs = lsa::cosine(as.numeric(bk[i,]), as.numeric(ck[i,])),
         mcs = mean(c(acs, bcs)),
         d1cs = lsa::cosine(as.numeric(dk1[i,]), as.numeric(ck[i,])),
         d2cs = lsa::cosine(as.numeric(dk2[i,]), as.numeric(ck[i,])),
         d3cs = lsa::cosine(as.numeric(dk3[i,]), as.numeric(ck[i,])),
         d4cs = lsa::cosine(as.numeric(dk4[i,]), as.numeric(ck[i,])),
         d5cs = lsa::cosine(as.numeric(dk5[i,]), as.numeric(ck[i,])),
         scs2 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]))), as.numeric(ck[i,])),
         scs3 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]))), as.numeric(ck[i,])),
         scs4 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]))), as.numeric(ck[i,])),
         scs5 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]))), as.numeric(ck[i,])),
         scs6 = lsa::cosine(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]), as.numeric(dk5[i,]))), as.numeric(ck[i,])),
         abn6 = norm(colMeans(rbind(as.numeric(ak[i,]), as.numeric(bk[i,]), as.numeric(dk1[i,]), as.numeric(dk2[i,]), as.numeric(dk3[i,]), as.numeric(dk4[i,]), as.numeric(dk5[i,]))), "2"),
         mcs2 = mean(c(acs, bcs, d1cs)),
         mcs3 = mean(c(acs, bcs, d1cs, d2cs)),
         mcs4 = mean(c(acs, bcs, d1cs, d2cs, d3cs)),
         mcs5 = mean(c(acs, bcs, d1cs, d2cs, d3cs, d4cs)),
         mcs6 = mean(c(acs, bcs, d1cs, d2cs, d3cs, d4cs, d5cs))) ->
  omegah

# Plot bias in cos(mean(X), Y) as an estimator of mean(cos(X, Y))
omegah %>%
  ggplot(aes(x=scs, y=mcs)) +
  geom_point() +
  scale_color_viridis_c() +
  geom_smooth(method="lm", alpha=0.5) +
  geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
  labs(x=latex2exp::TeX("$\\cos(\\Omega(2), X_i)$"),
       y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$")) -> pomg1
omegah %>%
  ggplot(aes(x=scs6, y=mcs6)) +
  geom_point() +
  scale_color_viridis_c() +
  geom_smooth(method="lm", alpha=0.5) +
  geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
  labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
       y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$")) -> pomg2

ggarrange(pomg1, pomg2, ncol=2, nrow=1)

# Color points by mean composed/component norm ratio
# The bias goes the other way when this value is high (?)
# omegah %>%
#     rowwise() %>% mutate(ascn=abn/an, bscn=abn/bn, cl = mean(c(ascn, bscn))) %>%
#     ggplot(aes(x=scs, y=mcs, color=cl)) +
#     geom_point() +
#     scale_color_viridis_c() +
#     geom_smooth(method="lm", alpha=0.5) +
#     geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
#     labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
#          y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))

# Color points by external composed normalization weight ||X_i||,||Omega(n)||
# omegah %>%
#     rowwise() %>% mutate(ascn=abn/an, bscn=abn/bn, cl = mean(c(ascn, bscn)), elnw=abn*cn) %>%
#     ggplot(aes(x=scs, y=mcs, color=cl, size=elnw)) +
#     geom_point() +
#     scale_color_viridis_c() +
#     geom_smooth(method="lm", alpha=0.5) +
#     geom_abline(intercept=0, slope=1, linetype="dashed", color="tomato", size=2) +
#     labs(x=latex2exp::TeX("$\\cos(\\Omega(5), X_i)$"),
#          y=latex2exp::TeX("mean $\\cos(\\Omega_j, X_i)$"))

# Joint distribution of ratio abn/an, abn/bn
# omegah %>%
#   rowwise() %>%
#   mutate(cl = mean(c(ascn, bscn))) %>%
#   ggplot(aes(x=log(ascn), y=log(bscn), color=cl)) +
#   geom_point() +
#   scale_color_viridis_c()
```