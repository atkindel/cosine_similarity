---
title: "cosine_paper_v3_extra"
output: html_document
---


### Sampled scale-cosine plot

```{r nelson_anglemanifold, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Distribution of cosine similarity against local normalization weight from uniformly random sample of 5000 words from embedding vocabulary. Color indicates product of log frequencies for this word pair. The mean and variance of the cosine similarity distribution changes as the relative scale increases."}
plot_angle_manifold(ra_nemb)
```

### Scale-cosine plots, fixed word list terms

```{r nelson_focalword, echo=F, message=F}
# Next, look at the distribution of cosine similarities for some arbitrary term
view_from_focal_word <- function(embmat, ft = NA) {
  if(is.data.frame(embmat)) {
    embmat <- as.matrix(embmat)
  }
  if(is.na(ft)) {
    ft <- sample(1:nrow(embmat), 1)
  }

  similmat <- word2vec_similarity(embmat[ft,], embmat, top_n=nrow(embmat), type="cosine")
  
  similmat %>%
    mutate(angle = acos(similarity) * (180/pi)) %>%
    left_join(tfn %>% as.data.frame() %>% dplyr::select(feature, frequency, snorm), by=c("term2"="feature")) %>%
    filter(!is.na(frequency)) %>%  # Drop no-frequency data points
    rowwise() %>%
    mutate(inner_product = embmat[ft,] %*% embmat[term2,]) ->
    test_1term
  return(test_1term)
}


# Plot cosine similarity distribution against log frequency
# Smoothed fits show linear (blue) and generalized additive (red) trends
# Median log frequency indicated by dotted vertical line
# Mean cosine similarity indicated by dashed horizontal line
# In general frequency bias is concentrated in the low-frequency subspace
# The GAM fit in particular is usually almost zero-slope in the high-frequency subspace
# Sometimes it goes back up though (maybe due to high-frequency high-relevance terms, e.g. colors)
# Point color shows vector norm; note somewhat noisy relationship to frequency
plot_focal_view <- function(csims) {
  focal_term <- csims$term2[1]
  csims[-1,] %>%
    ggplot(aes(x=log(frequency), y=similarity, color=snorm)) +
    geom_point() +
    geom_vline(xintercept=median(log(csims$frequency), na.rm=T), linetype="dotted", alpha=0.8) +
    geom_hline(yintercept=mean(csims$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
    geom_smooth(method="gam", color="tomato") +
    geom_smooth(method="lm") +
    scale_color_viridis_c() +
    ggtitle(latex2exp::TeX(sprintf("Frequency-cosine distribution: $\\textit{%s}$", focal_term))) +
    labs(x="Word frequency (log scale)",
         y="Cosine similarity",
         color=latex2exp::TeX("$||w_j||$"))
}
```

```{r nelson_fwl_focalview, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# From fixed word list
plot_focal_view(view_from_focal_word(nemb, "men"))
plot_focal_view(view_from_focal_word(nemb, "women"))
plot_focal_view(view_from_focal_word(nemb, "white"))
plot_focal_view(view_from_focal_word(nemb, "black"))
```

### Cosine-frequency relationship by component frequency

```{r nelson_cosfreq_supp, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot split by component frequencies, n=5000 random words in embedding vocabulary."}
# Plot uniformly random cosine similarity sample against log frequency of one component term
# This isn't as informative as the other plots but it's good to know it's not super asymmetric
# it's way more interesting to look at when the A and B sets are highly frequency imbalanced
ra_nemb %>%
  filter(!is.na(a$frequency) & !is.na(b$frequency)) %>%  # Missing term frequency data
  arrange(b$frequency) %>%
  ggplot(aes(x=log(a$frequency), y=ab_cs, color=log(b$frequency))) +
  geom_point() +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  geom_smooth(method="lm") +
  geom_smooth(color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") ->
  fp1
ra_nemb %>%
  filter(!is.na(a$frequency) & !is.na(b$frequency)) %>%  # Missing term frequency data
  arrange(a$frequency) %>%
  ggplot(aes(x=log(b$frequency), y=ab_cs, color=log(a$frequency))) +
  geom_point() +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  geom_smooth(method="lm") +
  geom_smooth(color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") ->
  fp2
ggarrange(fp1, fp2, ncol=1, common.legend = T, legend="bottom")
```

### Hyperbolic decomposition plots, fixed word list terms

```{r nelson_focal_lld, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Local linear decomposition from one term's perspective
local_linear_decomp(view_from_focal_word(nemb, "men") %>%
                      transmute(ab_cs=similarity, ab_ip=inner_product, nprod=snorm*.$snorm[1]) %>%
                      filter(ab_cs < 1))  # We don't care about the self-comparison point
```

### 3D visualization of cosine similarity surface

```{r nelson_3d, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", eval=F}
# 3D visualization makes the hyperbolic-parabolic shape more visible
library(rgl)

vid <- view_from_focal_word(nemb, "men")
vid$nprod <- vid$snorm * vid$snorm[1]
vid %<>% filter(similarity < 1)

# Original scale
vid.x <- vid$nprod
vid.y <- vid$similarity
vid.z <- -vid$inner_product

# Plot the surface
rgl.open()
rgl.bg(color = "white")
par3d(windowRect = 50 + c(0,0,1000,1000))
rgl.points(vid.x, vid.y, vid.z, color="tomato", size=4, alpha=0.7)
rgl.bbox(color=c("#333377","black"), emission="#333377",
         specular="#3333FF", shininess=5, alpha=0.4)
rgl.lines(c(0, max(vid.x)), c(0, 0), c(0, 0), color = "red", lwd=2)  # local normalization weight is red/X
rgl.lines(c(0, 0), c(0,max(vid.y)), c(0, 0), color = "blue", lwd=2)  # CS is blue/Y
rgl.lines(c(0, 0), c(0, 0), c(0,max(vid.z)), color = "green", lwd=2)  # IP is green/Z
title3d(xlab = "LNW", ylab = "Cosine", color="black")
mtext3d("IP", "z-+", line = 2, color="black")

# Rescaled (misleading?)
vid.x <- (vid$nprod - min(vid$nprod)) / (max(vid$nprod) - min(vid$nprod))
vid.y <- (vid$similarity - min(vid$similarity)) / (max(vid$similarity) - min(vid$similarity))
vid.z <- (vid$inner_product - min(vid$inner_product)) / (max(vid$inner_product) - min(vid$inner_product))

# Plot the rescaled (idealized) surface
rgl.open()
rgl.bg(color = "white")
par3d(windowRect = 50 + c(0,0,1000,1000))
rgl.points(vid.x, vid.y, vid.z, color="tomato", size=4, alpha=0.7)
rgl.bbox(color=c("#333377","black"), emission="#333377",
         specular="#3333FF", shininess=5, alpha=0.4)
rgl.lines(c(0, max(vid.x)), c(0, 0), c(0, 0), color = "red", lwd=2)  # local normalization weight is red/X
rgl.lines(c(0, 0), c(0,max(vid.y)), c(0, 0), color = "blue", lwd=2)  # CS is blue/Y
rgl.lines(c(0, 0), c(0, 0), c(0,max(vid.z)), color = "green", lwd=2)  # IP is green/Z
title3d(xlab = "LNW", ylab = "Cosine", color="black")
mtext3d("IP", "z-+", line = 2, color="black")

# Fit polynomial to this surface with least squares
# We can fit it perfectly since the relationship is fixed
library(rsm)
curvature <- lm(inner_product ~ poly(similarity, nprod, degree=2), data=vid)
persp(curvature, nprod ~ similarity, zlab = "inner_product")
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=30)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=60)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=90)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=120)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=150)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=180)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=210)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=240)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=270)
persp(curvature, nprod ~ similarity, zlab = "inner_product", theta=300)
persp(curvature, nprod ~ similarity, zlab = "inner_product")
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = 10)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = 0)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -10)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -20)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -30)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -40)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -50)
persp(curvature, nprod ~ similarity, zlab = "inner_product", phi = -60)
```


```{r distortion_animation, echo=F, message=F, include=F}
# Animation of distortion from growing mean vector perspective
# Generate random mean vectors from a starting point
sslim <- 10000
nemb %>% sample_n(sslim) -> nemb.ss
rbind(data.frame(view_from_composite(nemb, colMeans(nemb.ss[1,])), size=1),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:2,])), size=2),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:3,])), size=3),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:4,])), size=3),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:5,])), size=5),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:6,])), size=6),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:7,])), size=7),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:8,])), size=8),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:9,])), size=9),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:10,])), size=10),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:20,])), size=20),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:30,])), size=30),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:40,])), size=40),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:50,])), size=50),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:100,])), size=100),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss[1:1000,])), size=1000),
      data.frame(view_from_composite(nemb, colMeans(nemb.ss)), size=10000)) ->
  composites_by_size

# Animate distortion
# Note the behavior of the low-norm points in the low-frequency region
composites_by_size %>%
  arrange(snorm^2) %>%
  filter(similarity < 1) %>%  # Drop self-comparison if in data
  ggplot(aes(x=sqrt(log(frequency)), y=similarity, color=snorm^2)) +
  geom_point() +
  scale_color_viridis_c() +
  gganimate::transition_manual(frames=size) +
  labs(x="Word frequency (log scale)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$"),
       title = "n = {current_frame} (mean vector)")

# From the other side
composites_by_size %>%
  arrange(desc(snorm^2)) %>%
  filter(similarity < 1) %>%  # Drop self-comparison if in data
  ggplot(aes(x=sqrt(log(frequency)), y=similarity, color=snorm^2)) +
  geom_point() +
  scale_color_viridis_c() +
  gganimate::transition_manual(frames=size) +
  labs(x="Word frequency (log scale)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$"),
       title = "n = {current_frame} (mean vector)")

# Show each frame (not run)
# ggarrange(plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1,])), "n=1", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:5,])), "n=5", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:10,])), "n=10", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:25,])), "n=25", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:50,])), "n=50", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:100,])), "n=100", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss[1:1000,])), "n=1k", mv=T),
#           plot_composite_view(view_from_composite(nemb, colMeans(nemb.ss)), "n=10k", mv=T),
#           ncol=2, nrow=4, common.legend=T, legend = "bottom")
```

# Diagnostic plots for frequency bias

I propose three families of diagnostic plot for comparative similarity analysis in word embedding models.

- \textbf{Frequency error plots}: Plots showing the relationship between word frequency and key quantities describing the pointwise structure of the embedding space (vector norms, cosine similarity, Gaussian curvature).
- \textbf{Inner product decomposition plots}: Plots facilitating inspection of the inner product subspace implied by a particular choice of focal word, sum vector, or mean vector.
- \textbf{Normalization error plots}: Plots showing changes in the distribution of quantities of interest between the original embedding vector column space and the normalized unit-ball basis implied by the cosine projection.

I use the semantic vector space in Nelson (2021) to show example plots throughout this section.

## Frequency bias plots

### Scale-frequency plot

The simplest way to observe frequency bias is to plot the squared Euclidean norm against the log word frequency distribution for all words in the corpus (Arora et al. 2016).

### Cosine-frequency and cosine-scale plots

Word frequencies and vector norms can also be plotted against the distribution of cosine similarities directly. Calculating all possible pairwise cosine similarities can be prohibitive computationally. A large random sample of word pairs provides a useful approximation; it is often more informative to construct this type of plot in accordance with a proposed arithmetic aggregation procedure, such as with respect to a focal term vector, sum vector, or mean vector.

## Inner product decomposition plots

As previously noted, the local normalization weight $||A||||B||$ plays an important role in characterizing the geometry of cosine similarity with respect to the original inner product space.

### Cosine-LPNW plot

The simplest diagnostic visualization is to plot the local normalization weight against the corresponding cosine similarity; the product of these two values is $\left< A, B \right>$.

The cosine-LPNW plot is useful for visualizing the difference in the role of frequency bias between rank indexing and comparative measurement with cosine similarity. To see this, set a threshold indicating the top $m$ similar words (say $m = 50$), and fit two smooth functions, one involving all points and one involving only points selected by the threshold. In general, the degree of frequency bias increases with $m$.

### Inner product bilinearization plots

It is occasionally helpful to view the cosine-LPNW plot from two additional perspectives implied by the fact that the surface lies in $\mathbb{R}^3$. These plots make the non-Euclidean shape of the decomposition easier to see. They also help to visualize the doubly linear structure of the decomposition. At fixed values of the cosine similarity, there is a linear relationship between the LPNW and the inner product; conversely, at fixed values of the LPNW, there is a linear relationship between the inner product and the cosine similarity. The corresponding fixed values control the slopes on the other side of the bilinear relation.



### Curvature-frequency plot

As discussed above, cosine similarity can be interpreted as a non-Euclidean decomposition of the inner product space. The decomposition lies on a hyperbolic paraboloid in $\mathbb{R}^3$. 


## Normalization error plots

```{r nelson_columns_setup, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center"}
# Get minimum principal angle
# This quantity is in degrees by default (more easy to think about)
principal_angle <- function(embm, t1, t2, n1, n2, deg=F) {
  sv1 <- svd(embm[c(t1,t2),])
  sv2 <- svd(rbind(embm[t1,]/n1, embm[t2,]/n2))
  dists <- c(geometry::dot(sv1$v[,1], sv2$v[,1]),
             geometry::dot(sv1$v[,2], sv2$v[,1]),
             geometry::dot(sv1$v[,2], sv2$v[,2]),
             geometry::dot(sv1$v[,1], sv2$v[,2]))
  if(deg) {
    return(min(acos(dists)) * (180/pi))
  } else {
    return(min(acos(dists)))
  }
}

# Alternative measure of coincidence
# Krzanowski common space embedding trace statistic
# Sum of squared principal angles between original and norm spaces
krz_trace <- function(embm, t1, t2, n1, n2) {
  d1 <- prcomp(embm[c(t1, t2),])$rotation
  d2 <- prcomp(rbind(embm[t1,]/n1, embm[t2,]/n2))$rotation
  
  krztest <- t(d1) %*% d2 %*% t(d2) %*% d1
  return(sum(svd(krztest)$d))
}

# Or, the minimum angle between vectors in the subspace
krz_angle <- function(embm, t1, t2, n1, n2) {
  d1 <- prcomp(embm[c(t1, t2),])$rotation
  d2 <- prcomp(rbind(embm[t1,]/n1, embm[t2,]/n2))$rotation
  
  krztest <- t(d1) %*% d2 %*% t(d2) %*% d1
  return(acos(max(svd(krztest)$d)))
}

# One helpful way to think about this is in terms of the column space of the embeddings
# Look at the planar basis for the random draws we've done
append_dimensionality <- function(embm, rangs) {
  rangs %>%
    rowwise() %>%
    mutate(vsnorm = norm(embm[aterm,] + embm[bterm,], "2"),
           vdnorm = norm(embm[aterm,] - embm[bterm,], "2"),
           vsnorm.sc = norm(embm[aterm,]/anorm + embm[bterm,]/bnorm, "2"),
           vdnorm.sc = norm(embm[aterm,]/anorm - embm[bterm,]/bnorm, "2"),
           sv1 = svd(embm[c(aterm, bterm),])$d[1],
           sv2 = svd(embm[c(aterm, bterm),])$d[2],
           sv1.sc = svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$d[1],
           sv2.sc = svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$d[2],
           basis.ang = lsa::cosine(svd(embm[c(aterm, bterm),])$v[,1],
                                   svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1]),
           pr.ang = principal_angle(embm, aterm, bterm, anorm, bnorm, deg=T),
           krz = krz_trace(embm, aterm, bterm, anorm, bnorm),
           krza = krz_angle(embm, aterm, bterm, anorm, bnorm),
           d.samedir = sum(sign(embm[aterm,]) * sign(embm[bterm,]) > 0),
           # basis.dir = sum(sign(svd(embm[c(aterm, bterm),])$v[,1]) * sign(svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1]) > 0),
           basis.dir.opp = paste0(which(sign(svd(embm[c(aterm, bterm),])$v[,1]) * sign(svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1]) < 0), collapse="_"),
           basis.dir = ifelse(nchar(basis.dir.opp) == 0, 100, 99 - str_count(basis.dir.opp, "_")),  # Orthant alignment index: 100 - number of opposed dimensions (saves some SVDs)
           basis.dir.2 = ifelse(basis.dir < 50, 100-basis.dir, basis.dir),
           v1 = sv1^2 / (sv1^2 + sv2^2), 
           v2 = sv2^2 / (sv1^2 + sv2^2), 
           v1.sc = sv1.sc^2 / (sv1.sc^2 + sv2.sc^2), 
           v2.sc = sv2.sc^2 / (sv1.sc^2 + sv2.sc^2),
           scaleratio = max(anorm, bnorm)/min(anorm, bnorm)) ->
    rangs_plus
  return(rangs_plus)
}

# Run dimensional analysis for a random subsample (can take a while)
ra_nemb_dim <- append_dimensionality(nemb, ra_nemb %>% ungroup() %>% sample_n(2000))
```

```{r nelson_columns_pra, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Some informative quantities and plots relating to columnar orientation and normalization
# First principal angle: minimum angle between principal vectors of AB and AB'
# Norm deflection score: abs(cos(theta)) between first principal axes of original and normed vector pairs
#  This is the angle between the plane implied by the original vectors and the plane implied by the normalized vectors
#  This value lies approximately on the interval [cos(0), cos(pi/4)] (i.e. parallel to "half orthogonal")
#  So they can be almost parallel or somewhat intersecting
#  We take the absolute value because sometimes we get the normal vector going the other way, but we don't care
#  This is a transformation (hyperbolic centering?) of the first principal angle
#  Note that these are **not** always parallel subspaces! This is super counterintuitive
#  It is very surprising that A and A' are parallel and B and B' are parallel but AB and AB' are not necessarily parallel
#  How is this possible? You have to remember these are in 300 dimensions; 
# Scale ratio: max/min norm{A, B} (how "uneven" or "pointy" is the comparison)
#  There is a clear relationship between the scale ratio and the norm deflection score wrt cos(A, B)
#  In general the normed vectors aren't in the same column space as the original vectors
#  Extremal cosine similarities must have a larger scale ratio when NDS < 1
# Orthant overlap index index: discrete measure on [0,1] of how close they are to being in the same orthant
# 

# First principal angle distribution by scale ratio
# To have a high cosine similarity, the subspaces must be pointy and coincident
ra_nemb_dim %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=pr.ang, color=ab_cs)) +
  geom_point(size=2) +
  scale_color_viridis_c() +
  xlim(0, 90)
```

```{r nelson_columns_nds, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Superimposing the positive and negative NDS regions
# You can see the relationship better this way
ra_nemb_dim %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=abs(basis.ang), color=ab_cs)) +
  geom_point(size=2) +
  geom_vline(xintercept=cos(pi/4), linetype="dashed") +
  scale_color_viridis_c()

# This relationship is self-similar (graph always looks the same no matter where you set the NDS threshold)
ra_nemb_dim %>%
  filter(abs(basis.ang) > 0.95) %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=abs(basis.ang), color=ab_cs)) +
  geom_point(size=2) +
  scale_color_viridis_c() -> bap1
ra_nemb_dim %>%
  filter(abs(basis.ang) > 0.99) %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=abs(basis.ang), color=ab_cs)) +
  geom_point(size=2) +
  scale_color_viridis_c() -> bap2
ggarrange(bap1, bap2, ncol=2, common.legend = T, legend="bottom")
```

```{r nelson_lld_nds, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Show NDS on the inner product manifold
# NDS describes Var[cos(A,B)|LPNW(A,B)]
# Smaller NDS values imply cos(A, B) closer to zero
# TODO: This is important! It causes the ZCME violation. How does NDS relate to the LPNW?
ra_nemb_dim %>%
  arrange(desc(abs(basis.ang))) %>%
  ggplot(aes(x=nprod, y=ab_cs, color=abs(basis.ang), size=-abs(basis.ang))) +
  geom_point() +
  scale_color_viridis_c(direction=-1) +
  scale_size_continuous(range=c(2, 5))

# Using first principal angle
ra_nemb_dim %>%
  arrange(pr.ang) %>%
  ggplot(aes(x=nprod, y=ab_cs, color=pr.ang, size=pr.ang)) +
  geom_point() +
  scale_color_viridis_c() +
  scale_size_continuous(range=c(2, 5))
```

```{r nelson_normscale, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Linear relationship between (scaled) norm of summed normalized vectors and spectral norm
# Scaling is linearizing: cosine similarity is perfectly linear in both quantities when rescaled
# In contrast, cosine similarity has "pointiness variance" when not normalized
ra_nemb_dim %>%
  ggplot(aes(x=vsnorm.sc, y=sv1.sc, color=ab_cs)) +
  geom_point() + scale_color_viridis_c() +
  ggtitle("Normalized") + theme(legend.position="bottom") -> scp1
ra_nemb_dim %>%
  ggplot(aes(x=vsnorm, y=sv1, color=ab_cs)) +
  geom_point() + scale_color_viridis_c() +
  ggtitle("Original scale") + theme(legend.position="bottom") -> scp2
ggarrange(scp1, scp2, ncol=2, common.legend = T, legend="bottom")


# By component vector % variance explained
# When the space is pointier/longer (?) the cosine similarities become less differentiated, smaller
ra_nemb_dim %>%
  ggplot(aes(x=v1, y=v1.sc, color=ab_cs)) +
  geom_point() +
  scale_color_viridis_c()
```

```{r nelson_pra_nds, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Relationship between first principal angle and NDS
# The principal angle ranges from 0 to 90d; the NDS ranges from 0 to 45d
# When the principal angle is greater than 45d, 
# These are jittered so you can see the concentration of cosines on this manifold
# Low cosines are half-orthogonal; high cosines are close to parallel
# Moderate cosines can be anywhere, sorta
ra_nemb_dim %>%
  arrange(ab_cs) %>%
  mutate(abs.basis.ang.deg = acos(abs(basis.ang)) * (180/pi)) %>%
  ggplot(aes(x=pr.ang, y=abs.basis.ang.deg, color=ab_cs)) +
  geom_jitter(size=2, width=3) +
  geom_vline(xintercept=45, linetype="dashed") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> pbr1
ra_nemb_dim %>%
  arrange(desc(ab_cs)) %>%
  mutate(abs.basis.ang.deg = acos(abs(basis.ang)) * (180/pi)) %>%
  ggplot(aes(x=pr.ang, y=abs.basis.ang.deg, color=ab_cs)) +
  geom_jitter(size=2, width=3) +
  geom_vline(xintercept=45, linetype="dashed") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> pbr2
ggarrange(pbr1, pbr2, ncol=2, common.legend = T, legend="bottom")
```

```{r nelson_orthants, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Orthant proximity index plots.", include=F}
# One potentially helpful way to think about it is in terms of the vector space orthants
# Orthants are the n-dimensional equivalent of quadrants; rough descriptor of direction
# Plot the number of directions they have in common against cosine similarity
# Color points by local normalization weight to show the bias
# This has the strong linear relationship we expect
ra_nemb_dim %>%
  arrange(nprod) %>%
  ggplot(aes(y=ab_cs, x=d.samedir, color=nprod)) +
  geom_point(size=3) +
  scale_color_viridis_c()

# Plot orthant overlap index against NDS
# Cosine similarity is lower on average and more variable when the norm-scaled
#  vector subspace and the original-scale vector subspace don't overlap
# Only ~4% pair subspaces of a sample of 2000 even lie in the same orthant of the
#  vector space! Wow
ra_nemb_dim %>%
  arrange(ab_cs) %>%
  ggplot(aes(x=basis.dir.2, y=abs(basis.ang), color=ab_cs)) +
  geom_point(size=3) +
  scale_color_viridis_c()

# Remember that within this orthant there is also some variation in the inner product space
ra_nemb_dim %>%
  filter(basis.dir.2==100) %>%
  ggplot(aes(x=nprod, y=ab_cs)) +
  geom_point()
```

```{r nelson_krz, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Histogram of Krzanowski coincidence statistic.", include=F}
# Distribution of Krzanowski common subspace embedding trace statistic
# Note concentration at 1
# Note that it appears to be *never* full rank! They have to draw on different parts of the vector space!
ra_nemb_dim %>%
  ggplot(aes(x=krz)) +
  geom_histogram(binwidth=0.01)

# Plot minimum angle between subspaces (Krzanowski method) against scale ratio
# Large cosine similarities tend to be in more square, less coincident subspaces
ra_nemb_dim %>%
  arrange(krza) %>%
  ggplot(aes(y=krza*(180/pi), x=scaleratio, color=ab_cs)) +
  geom_point() +
  geom_smooth(color="gray") +
  scale_color_viridis_c()
```

```{r nelson_normalization, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Normalization basis transformation between pairs by product of norm deflection scores within pairs. The more parallel the pairwise comparison space is to its normalized counterpart (NDS product closer to 1), the more the between-comparisons are invariant to normalization. Conversely, when normalization moves the vector pairs into a more different basis, the between-comparison normalized bases tend to be closer to parallel than in their original subspaces, and there is more variance in how close to parallel the corresponding normalized subspaces are. In the more extreme NDSP region, they are less correlated. In particular, when the NDS product is low, less coincident original subspaces rescale more variably.", include=F}
# How does this affect arithmetic comparisons of cosine similarities?

# Create many random paired-paired comparisons
# We want to know how the normalization and the frequency bias interact when
#  we compare two paired comparisons
make_angles_geomquad <- function(embm, k=10) {
  sm <- sample(tfn$feature, k*4)
  sA <- sm[1:k]
  sB <- sm[(k+1):(k*2)]
  sC <- sm[(k*2+1):(k*3)]
  sD <- sm[(k*3+1):(k*4)]
  
  data.frame(aterm=sA, bterm=sB, cterm=sC, dterm=sD) %>%
    rowwise() %>%
    mutate(anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           cnorm = norm(embm[cterm,], "2"),
           dnorm = norm(embm[dterm,], "2"),
           a = tfn[which(tfn$feature == aterm),"frequency"],
           b = tfn[which(tfn$feature == bterm),"frequency"],
           c = tfn[which(tfn$feature == cterm),"frequency"],
           d = tfn[which(tfn$feature == dterm),"frequency"],
           ab_cs = lsa::cosine(as.numeric(embm[aterm,]), as.numeric(embm[bterm,]))[1],
           ab_ip = as.numeric(embm[aterm,]) %*% as.numeric(embm[bterm,]),
           nprod = anorm * bnorm,
           cd_cs = lsa::cosine(as.numeric(embm[cterm,]), as.numeric(embm[dterm,]))[1],
           cd_ip = as.numeric(embm[cterm,]) %*% as.numeric(embm[dterm,]),
           mprod = cnorm * dnorm,
           ab.scaleratio = max(anorm, bnorm)/min(anorm, bnorm),
           cd.scaleratio = max(cnorm, dnorm)/min(cnorm, dnorm),
           ab.pra = principal_angle(embm, aterm, bterm, anorm, bnorm, deg=T),
           cd.pra = principal_angle(embm, cterm, dterm, cnorm, dnorm, deg=T),
           ab.basis.ang = lsa::cosine(svd(embm[c(aterm, bterm),])$v[,1],
                                      svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1]),
           cd.basis.ang = lsa::cosine(svd(embm[c(cterm, dterm),])$v[,1],
                                      svd(rbind(embm[cterm,]/cnorm, embm[dterm,]/dnorm))$v[,1]),
           abcd.ang.sc = lsa::cosine(svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1],
                                     svd(rbind(embm[cterm,]/cnorm, embm[dterm,]/dnorm))$v[,1]),
           abcd.ang = lsa::cosine(svd(rbind(embm[aterm,], embm[bterm,]))$v[,1],
                                      svd(rbind(embm[cterm,], embm[dterm,]))$v[,1])) ->
    random_angles
  
  return(random_angles)
}

gq.nemb <- make_angles_geomquad(nemb, k=4000)

# Plot angle of AB and CD planar bases to each other in original scale (X) and in "cosine scale" (Y)
# Color points by the product of the AB-AB' and CD-CD' angles ("norm deflection score product")
# Split by quantile to show changing relationship; 9iles works well and looks nice, 16iles also useful
# The more parallel the within-comparisons are to their normalized counterparts (NDS product closer to 1),
#  the more the between-comparisons are invariant to normalization. Conversely, when normalization moves
#  the vector pairs into a more different basis, the between-comparison normalized bases tend to be closer
#  to parallel than in their original subspaces, and there is more variance in how close to parallel the
#  corresponding normalized subspaces are. In the more extreme NDSP region, they are less correlated. In 
#  particular, when the NDS product is low, less coincident original subspaces rescale more variably.
gq.nemb %>%
  ungroup() %>%
  mutate(ile = ntile(abs(ab.basis.ang) * abs(cd.basis.ang), 16)) %>%
  arrange(desc(abs(ab.basis.ang) * abs(cd.basis.ang))) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=abs(ab.basis.ang) * abs(cd.basis.ang))) +
  geom_point(size=3) +
  geom_smooth(method="lm") +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c(direction=-1) + 
  theme(legend.position="bottom") +
  facet_wrap(~ile)
```

```{r nelson_norm_nds, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Split above plot by AB and BC norm deflection score
gq.nemb %>%
  ungroup() %>%
  mutate(ile = ntile(abs(ab.basis.ang), 16)) %>%
  arrange(desc(abs(ab.basis.ang))) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=abs(ab.basis.ang))) +
  geom_point(size=3) +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c(direction=-1) + 
  theme(legend.position="bottom") +
  facet_wrap(~ile) -> bsp1
gq.nemb %>%
  ungroup() %>%
  mutate(ile = ntile(abs(cd.basis.ang), 16)) %>%
  arrange(desc(abs(cd.basis.ang))) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=abs(cd.basis.ang))) +
  geom_point(size=3) +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c(direction=-1) + 
  theme(legend.position="bottom") +
  facet_wrap(~ile) -> bsp2
ggarrange(bsp1, bsp2, ncol=2, common.legend = T, legend="bottom")
```

```{r nelson_pra_nds_split, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Normalization basis transformation by principal angle product quantiles. Smaller principal angle ratios imply that the local normalization is less curved.", include=F}
# Use untransformed principal angle product instead
# Same pattern (but the quantiles go in the other direction)
gq.nemb %>%
  ungroup() %>%
  mutate(ile = ntile(ab.pra * cd.pra, 16)) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=ab.pra * cd.pra)) +
  geom_point(size=3) +
  geom_smooth(method="lm") +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c() + 
  theme(legend.position="bottom") +
  facet_wrap(~ile)
```

```{r nelson_pra_basis_extra, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
gq.nemb %>%
  ungroup() %>%
  arrange(ab.pra) %>%
  mutate(ile = ntile(ab.pra, 16)) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=ab.pra)) +
  geom_point(size=3) +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c(direction=-1) + 
  theme(legend.position="bottom") +
  facet_wrap(~ile) -> bpp1
gq.nemb %>%
  ungroup() %>%
  arrange(cd.pra) %>%
  mutate(ile = ntile(cd.pra, 16)) %>%
  ggplot(aes(x=abs(abcd.ang), y=abs(abcd.ang.sc), color=cd.pra)) +
  geom_point(size=3) +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  scale_color_viridis_c(direction=-1) + 
  theme(legend.position="bottom") +
  facet_wrap(~ile) -> bpp2
ggarrange(bpp1, bpp2, ncol=2, common.legend = T, legend="bottom")
```


### Scale ratio

When working with the column space of the embeddings, it is often more informative to work with the scale ratio $\frac{\max(||A||,\ ||B||)}{\min(||A||,\ ||B||)}$ instead of the local normalization weight. The scale ratio can be interpreted straightforwardly as a measure of how much longer $A$ or $B$ is compared to the shorter vector in the pair. This quantity clarifies the functional relationship 

### Krzanowski coincidence measure

Krzanowski (1979) describes a summary statistic describing the degree to which two subspaces of a vector space coincide. The statistic is based on the eigenvalues of a common-space embedding of two subspaces of equal rank:

TODO: Krz trace statistic

$\text{Coincidence}(AB, CD)$ is zero when the subspaces are orthogonal. Its maximum value is the rank $k$ of the subspaces (in this case, $k=2$); this value is attained when the subspaces coincide completely.

The coincidence measure reveals an important and often overlooked feature of cosine similarity: for any two $p$-dimensional vectors $A, B$ and their normalized versions $A'=A/||A||, B'=B/||B||$, $\text{Coincidence}(AB, A'B') < p$. That is, the local normalization weight on any cosine similarity implies a normalized subspace that only partially coincides with the original semantic vector space. 

Krzanowski additionally showed that the minimum angle between two vectors in each subspace is equal to the arccosine of the largest singular value of the common space embedding: $\theta_{min}(AB, A'B') = \cos^{-1}(s_1)$. This quantity is closely related to the scale ratio and the cosine similarity; roughly, the quantities have a curvilinear relationship conditional on cosine similarity (up to an error term). Note that this quantity is zero only when the scale ratio is 1; this implies that the original and normalized subspaces for any two vectors share a principal direction only when the two corresponding words are close in frequency. An easy way to see this visually is to plot the A and B log frequencies and color the points by the minimum angle; the minimum angle becomes larger as the frequency ratio becomes more imbalanced.

### Orthant proximity index

Word embedding vectors lie in a $p$-dimensional Euclidean orthant described by the sign vector $\text{sgn}(v_A)$. A purely directional quantity, the \textit{orthant proximity index}, can be obtained by multiplying the sign vectors of $A$ and $B$ and summing the non-negative entries:

$$\text{OPI}(A, B) = \sum_{v(p)>0} \text{sgn}(v_A) \cdot \text{sgn}(v_B)$$

This quantity is similar to Kendall's nonparametric rank correlation coefficient $\tau$; it describes the alignment of $v_A$ and $v_B$ in terms of how many orthants away from each other they are. There is an implied range of possible cosine similarities between two vectors at any given index value. Note that two vectors can be parallel only when $\text{OPI}(A, B) = 0$, but this does not imply that they are in fact parallel. This value does not use any of the scalar information in the underlying vectors, but it is still sensitive to the stability of estimated dimensions that are located close to zero (i.e. when a slight perturbation in the model would flip the sign in the $i$-th dimension for a large number of vectors).

### Norm deflection score

The norm deflection score of two vectors $A, B$ is the absolute value of the cosine between the left singular vectors corresponding to the largest singular values of the original and normalized subspaces spanned by $A$ and $B$:

$$\text{NDS}(A, B) = |\cos(V_{AB}(s_1), V'_{A'B'}(s'_1))|$$

This quantity is a transformation of the first principal angle between the two subspaces. Usefully, it is always positive and ignores trivial differences in the direction of the normal vector to each plane; typically we do not care if these normal vectors point "down" or "up", just what their inclination to each other is.

### First principal angle

The first principal angle between the two subspaces can also be used directly.



### Norm alignment-scale plot

### Basis deflection plot
