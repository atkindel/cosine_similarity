---
title: "Relative scale bias in cosine similarity arithmetic"
author: "Alexander T. Kindel^[PhD Candidate, Department of Sociology, Princeton University. Contact: akindel@princeton.edu.]"
abstract: |
 |  Researchers often use the cosine ratio as a measure of similarity between pairs of observations. A common methodology for comparing these cosine similarities is to add or subtract them (e.g., taking the mean). I show that various kinds of comparative arithmetic over cosine similarities share a key problem: they result in measures that are biased in proportion to the relative scale of the component vectors. Relative scale bias is a mechanical consequence of the fact that the cosine similarity is a function of the component vectors and their respective norms. An implication is that relative scale bias is not an error in an underlying model or model class, nor is it a feature of any particular application domain. Cosine similarities are useful for local comparisons between two vectors, but sums of cosine similarities intended to describe higher-order comparisons are scale dependent.
 | 
 | **Keywords:** cosine similarity, relative scale bias, measurement, arithmetic
date: |
  | 4 March 2022
  | 
  | 
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: TRUE
    number_sections: true
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \setlength\parindent{24pt}
  - \usepackage[bottom]{footmisc}
---

```{r setup, include = FALSE}
library(tidyverse)
library(magrittr)
library(gridExtra)
library(latex2exp)
library(here)

# Document settings
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, cache.lazy=FALSE)
set.seed(2718281)

# Load THE MATRIX (embeddings) and get token list
# The choice of embeddings obviously always matters
# I think the Wikipedia + newswire one has the best semantics
# You see a lot more terms in the bigger ones, but not super "meaningful" ones
# "qantas.com is my favorite example"
glove_d <- read_table2(here("data", "glove.6B.300d.txt"), col_names = FALSE)  # Wikipedia & Gigaword 5
# glove_d <- read_table2(here("data", "glove.twitter.27B.200d.txt"), col_names = FALSE)  # Twitter
# glove_d <- read_table2(here("data", "glove.42B.300d.txt"), col_names = FALSE)  # Common Crawl (42B)
# glove_d <- read_table2(here("data", "glove.840B.300d.txt"), col_names = FALSE)  # Common Crawl (840B)
colnames(glove_d) <- c("token", sapply(1:(ncol(glove_d)-1), function(x) paste0("d", x)))
glove_mt <- glove_d %>% pivot_longer(contains("d"), names_to="dim")
glove_tokens <- unique(glove_mt$token)
glove_d %<>% select(-token) %>% as.matrix()
rownames(glove_d) <- glove_tokens
n_glove <- length(glove_tokens)
embm <- glove_d
etokens <- glove_tokens
nvoc <- n_glove
rm(glove_mt, glove_d, glove_tokens)

# Alternatively, use word2vec
# I prefer GloVe but people also use this for some reason
# A useful preprocessing step is to prune the vocabulary to GloVe (higher quality)
# w2v <- read.word2vec(here("data", "GoogleNews-vectors-negative300.bin"))
# w2v_tokens <- summary(w2v, type='vocabulary')
# xemb_vocab <- w2v_tokens[which(w2v_tokens %in% glove_tokens)]
# w2v_d <- predict(w2v, xemb_vocab, type="embedding")
# n_w2v <- length(xemb_vocab)
# embm <- w2v_d
# etokens <- xemb_vocab
# nvoc <- n_w2v
# rm(w2v, w2v_tokens, w2v_d)

# Set rarity index window
# GloVe is sorted by term frequency, approximately
# In practice, methodologies are hugely biased toward frequent words (proxied by low term index)
# In social science we are almost never looking at ultra-rare positive or negative terms
#  e.g. "statuesque", "rapturous", "felicitous" or "lachrymose", "lugubrious", "dolorous"
# And, a lot of the excess terminology in the larger embeddings is hard to interpret anyways
#  e.g. "qantas.com", "ribbonwork", "rhamnolipid", "synchronal", "thesis-writing"
# The rarity windows implied by the WEAT lexicons are potentially interesting to benchmark against as well
g_rmin <- 1
g_rmax <- nvoc
g10_rmax <- nvoc * 0.1

# Function to norm vectors
scale2 <- function(vc) {
  return(vc / norm(vc, "2"))
}
```

\pagebreak

# Introduction

Cosine similarity is widely used across the social sciences as a measure of correlation between observations. The alignment of two $p$-dimensional vectors $A$ and $B$ is defined as the ratio of their inner product to the product of their norms:

$$cos(\theta_{AB}) = \frac {A \cdot B}{||A||\;||B||}$$  
The ratio describes the amount that the two vectors point in the same, different, or opposite direction; it is bounded from -1 (pointing in opposite directions) to 0 (pointing in different/orthogonal directions) to 1 (pointing in the same direction).

Many researchers in the social sciences apply cosine similarity to measure patterns of cultural association, including schemas, analogies, ideologies, discourses, and stereotypes. This methodological strategy is particularly common in studies of large-scale patterns in language use, where researchers often rely on vector space embeddings of word co-occurrence data (e.g. GloVe, word2vec) to model analogical semantics in large corpora (see Salton & McGill 1986; Antoniak & Mimno 2018; Arseniev-Koehler 2020; Rodriguez & Spirling 2022; Grimmer, Roberts & Stewart 2022). Researchers in a wide variety of fields (cultural sociology, computer science, political science, management) have used sets of cosine similarities to measure:
\begin{itemize}\singlespace
  \item cultural schemas (Kozlowski et al. 2019; Arseniev-Koehler \& Foster 2022)
  \item stereotypical gender and race associations (Caliskan, Bryson \& Narayanan 2017; Jones et al. 2019; van Loon et al. 2022)
  \item intersectional identity (Nelson 2021)
  \item density of clusters of scientific terms (Evans 2010)
  \item sonic similarity of hit songs (Askin \& Mauskapf 2017)
  \item occupation name/description similarity (Martin-Caughey 2021)
  \item discursive diversity (Lix et al. 2022)
  \item contrarian climate discourse (Farrell 2016)
  \item ideology in political discourse (Fuhse et al. 2020)
  \item emotion in political speech (Cochrane et al. 2022)
\end{itemize}

When measuring meaning using sets of cosine similarities, the usual data analysis task is to find a summary quantity over structured pairs of underlying term vectors. The typical methodology for this comparison involves adding the corresponding cosine similarities together.^[Researchers employ a number of ways of constructing such sums in the literature; see appendix for a more detailed discussion.] The purpose of this paper is to show that any such arithmetic comparison of cosine similarities exhibits relative scale bias. The next section characterizes relative scale bias as a misfit between standard arithmetic comparison methodologies and the geometry of angle and magnitude in word embeddings. In the following section, I discuss how relative scale bias appears in a more complex measurement setting: the measurement of stereotypical associations (Caliskan, Bryson & Narayanan 2017; Jones et al. 2019; van Loon et al. 2022). Both sections employ the public GloVe embeddings of Wikipedia 2014 and Gigaword 5 (Pennington, Socher & Manning 2014).^[The pretrained Twitter and Common Crawl embeddings are perhaps more commonly used; however, the expanded term set in these larger embeddings does not necessarily add much information of substantive interest to researchers over the scope of Wikipedia and newswire text. In practice, researchers rely on few fixed lists of approximately 20-50 relatively common terms in practice (Antoniak & Mimno 2018), so few studies employ this extended lexicon except incidentally. It is very easy to understate the depths of unusualness in the tail lexicon of large-scale word embeddings; Piantadosi (2014) gives the triplet "accordion", "catamaran", "ravioli" as examples of low-frequency words, but all three are in the top 15% of terms by frequency in GloVe. I think it is genuinely unclear whether and when unigrams like "rutherfordium" (atomic number 104), "apochromat" (a type of lens that corrects for distortions of color), "qantas.com" (originally an acronym for Queensland and Northern Territory Aerial Services), or "6dogs9cats" (a Tumblr user) is relevant to measures of term-associative meaning. Also note that the phenomenon of relative scale bias is a property of cosine arithmetic in vector spaces and not the inner product distribution of word embeddings per se (Mimno & Thompson 2017). The exact bias for fixed word lists varies between models and datasets.] In the appendix, I discuss the relative scale bias in three commonly employed means of cosine similarities; show the behavior of the bias for changing term set sizes, term rarity windows, and local norm ratios; and exactly characterize the relative scale bias in a few common fixed word lists in the literature.

# Relative scale bias in standard basis cosine sums

It is sometimes observed that cosine similarity is not a distance metric because it does not satisfy the triangle inequality. Formally, this feature of the cosine similarity means that it is not a distance function $d(A, B)$ that satisfies the rule $\forall C: d(A, B) \leq d(A, C) + d(B, C)$. In more plain language, the triangle inequality is a rule about adding things that are measurements of distance, like length. Consider the aphorism "detours are never shortcuts"; in the same way that making an extra stop on a road trip does not make the trip shorter, taking the distance between $A$ and $B$ by summing their respective distances with $C$ cannot result in a shorter overall distance in a metric space. So what does it mean for the cosine similarity to not obey this rule? One way of thinking about it is that the cosine ratio describes the amount that one can heuristically "shortcut" the triangle inequality by factoring out *local* relative scale and treating every vector as if it were the same length, but oriented in its original direction. This unit ball projection can then be used as a navigational guide to the underlying non-normalized vector space. However, because any given cosine similarity has been scaled by this local quantity, it only has units relative to the geometric relationship between its particular pair of underlying vectors; the amount any given cosine similarity "shortcuts" is particular to the local context in which it is taken. The sense in which cosine similarity does not satisfy this inequality is also the sense in which we have factored out the relative magnitude of the two vectors in order to arrange them by their angular alignment.

The methodological implications of this fact have not been fully appreciated. In particular, although cosine provides a useful way of thinking about local term associations in terms of their alignment, its intentional disregard for scale renders it only locally meaningful. Sums and means of a set of cosine similarities over a globally distributed set of vectors are dependent on the scale of the inner product space. The amount that cosine similarity "shortcuts" is different in different subspaces; consequently, the set of transformations implied between pairs of cosine similarities is *anisotropic*. That is, each transformation varies in its principal directions; a step in the largest direction in subspace $\mathbf{A}$ is not generally the same as a step in the largest direction in subspace $\mathbf{B}$. This implies a non-standard basis for arithmetic. The degree to which adding cosine similarities between anisotropic subspaces of a vector space model approximates the kind of addition we are used to is dependent on the degree to which the corresponding sets of vectors under comparison lie in bases that are relatively similar in shape. More unbalanced comparisons receive more weight in measures that add cosine similarities in this way. This can lead to unanticipated scale-dependent surprises in any statistic constructed as a sum of cosine similarities.

Cosine similarity can be described more generally as a hyperbolic-parabolic factorization of the inner product space, where each inner product has a unique pair of coordinates: the cosine similarity, and the product of the norms of the component vectors, which I term the local norm product weight. This surface is linear in each of these components, a fact which makes the factorization extremely useful for high-dimensional data analysis despite the fact that they do not provide an orthogonal basis for the inner product. A particularly convenient way of working with the factorization is to write the cosine similarity as the ratio of the inner product to the local norm product weight, but this way of thinking about it can obscure the equal importance of the local norm product weight, which describes the dual linear structure in the decomposition. 

```{r parallelotope, echo=F, out.width="\\textwidth", fig.cap="Pairwise vector comparisons $A_{\\alpha}OB_{\\alpha}$; normalized comparison in orange and green. $||A_{i}|| = ||A_{j}|| = ||A_{k}||$; $||B_{i}|| < ||B_{j}|| < ||B_{k}||$. At any given angle, the global major diagonal (blue dashed line) undershoots the local major diagonal (gray dashed line) by an amount described by the proportion of the norms of $A_{\\alpha}$ and $B_{\\alpha}$. Consequently, when we add the corresponding cosine similarities, the resulting statistic is systematically biased by the relative scale of the comparison, i.e. the ratio of the norm products of the component vectors."}
knitr::include_graphics("./parallelograms.png")
```

Figure \ref{fig:parallelotope} illustrates the basic geometry of relative scale bias. Consider two random draws from a vector space, $A_i$ and $B_i$. There are two commonly employed semantic operators on $A_i$ and $B_i$: vector addition and cosine similarity. Employing vector addition and subtraction as a semantic operator implies each pair of vectors forms a unique parallelogram $AOB$. The diagonals of this parallelogram correspond to the sum and difference vectors obtained by adding and subtracting the component vectors. Denote the sum vector $D(+)$ and the difference vector $D(-)$.

Employing cosine similarity as a semantic operator implies a local transformation of the parallelogram $AOB$ to a normalized parallelogram $A'OB'$ with unit-length sides in the direction of $A$ and $B$ (Figure \ref{fig:parallelotope}, top left). The transformation to $A'OB'$ is local in the sense that it involves a specific ratio between the norms of the component vectors relative to the original angle, i.e., a proportional change in the area of $AOB$. Accordingly, cosine similarity can be thought of as a local weighting of the inner product space by the ratio of the norms $||A||$ and $||B||$. The set of parallelograms we can form in this way has identical angular geometry to the original parallelogram set; however, the alignment of the diagonals is predictably biased by the local scale of the original parallelogram.

When we add across parts of the inner product space with varying local norm adjustments (Figure \ref{fig:parallelotope}, top row) the resulting statistic is systematically biased to a degree described directly by the norm ratio of the component vectors. A consequence of this fact is that cosine similarities are variably comparable as the ratio of relative scales moves further from equality. If we compare these two comparisons, we are comparing two quantities that have been adjusted by an amount that is predictable by their relative scale. In Figure \ref{fig:iprod_dnorm}, the small and bright cosine similarities represent more positively skewed comparisons (the sum vector is proportionally much longer than the difference vector) and the large and dull cosine similarities represent more negatively skewed comparisons. Cosine similarity smoothly differentiates these comparisons, but the way that it accomplishes this is not uniformly distributed over the local norm product weight. As this weight increases, the cosine similarity collapses onto a  Importantly, this means that linear sums approximate local comparisons of cosine similarities well, but this approximation breaks down as the comparison becomes more global.

The predicted geometric relationship can be shown mathematically in any vector space word embedding model (i.e. "classic" word embeddings like GloVe and word2vec). I use the publicly available pre-trained GloVe embeddings of Wikipedia 2014 and Gigaword 5 (Pennington, Socher & Manning 2014) to describe the geometry of the three sums described in Appendix A taken over random sets of GloVe vectors. To describe the differing shapes of these vector subspaces, I take the singular value decomposition of the matrices $\textbf{A}$ and $\textbf{B}$, resulting in a set of singular vectors of rank less than or equal to the number of terms in the set. The resulting singular values describe the overall shape of the linear subspace spanned by each set of vectors (see Bai & Silverstein 2010). The proportion of variance contained in each dimension across pairs of vector sets describes the difference in shape.

```{r iprod_dnorm, echo=F, message=F, fig.height=5, fig.width=5, fig.align="center", fig.cap="Pairwise cosine similarities between $A$ and $B$ of size $k=100$. Pointwise comparisons are locally linear, but globally skewed by the local norm product. The sum norm (color) and difference norm (size) describe the ratio of the diagonals of the parallelogram formed by each pair of vectors; similar parallelograms are clustered, but the distribution of parallelograms is not uniform. Marginal densities are plotted on their respective axes."}
# Sample two sets of k random vectors from GloVe (with optional term index rarity window)
# Compute the norms, cosine similarities, and major/minor diagonals for each term pair
# Takes a long time for k >= 100
# Realistically people are using k = 20 to 50
# Appendix shows k = {75, 50, 25}
# Optionally, make set B be rarer
make_angles <- function(k=100, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
  sA <- sample(etokens[a_rmin:a_rmax], k)
  sB <- sample(etokens[b_rmin:b_rmax], k)
  expand_grid(i=1:k, j=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Different sampling procedure
# Split window by quantile, sample higher frequencies proportionately more
# Tries to get the suggested number of vectors, but with rounding.
# Only works with GloVe (w2v is not in order)
make_angles_2 <- function(k=48, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
  qnt <- 5
  a_rng_w <- round((a_rmax - a_rmin)/qnt)
  b_rng_w <- round((b_rmax - b_rmin)/qnt)
  sA <- c(sample(etokens[a_rmin:(a_rng_w*1/2)], k),
          sample(etokens[(a_rmin+a_rng_w*1/2):(a_rng_w*1)], round(k/2)),
          sample(etokens[(a_rmin+a_rng_w*1):(a_rng_w*2)], round(k/4)),
          sample(etokens[(a_rmin+a_rng_w*2):(a_rng_w*3)], round(k/8)),
          sample(etokens[(a_rmin+a_rng_w*3):(a_rng_w*5)], round(k/16)))
  sB <- c(sample(etokens[b_rmin:(b_rng_w*1/2)], k),
          sample(etokens[(b_rmin+b_rng_w*1/2):(b_rng_w*1)], round(k/2)),
          sample(etokens[(b_rmin+b_rng_w*1):(b_rng_w*2)], round(k/4)),
          sample(etokens[(b_rmin+b_rng_w*2):(b_rng_w*3)], round(k/8)),
          sample(etokens[(b_rmin+b_rng_w*3):(b_rng_w*5)], round(k/16)))
  expand_grid(i=1:k, j=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Angles for WEAT
# ioff parameter offsets the index list to get terms with same frequency distribution
make_angles_weat <- function(ioff = 0) {
  weat_white <- c("adam", "harry", "josh", "roger", "alan", "frank", "justin", "ryan", "andrew", "jack",
                "matthew", "stephen", "brad", "greg", "paul", "jonathan", "peter", "amanda", "courtney",
                "heather", "melanie", "katie", "betsy", "kristin", "nancy", "stephanie", "ellen", "lauren",
                "colleen", "emily", "megan", "rachel")
  weat_black <- c("alonzo", "jamel", "theo", "alphonse", "jerome", "leroy", "torrance", "darnell", "lamar",
                  "lionel", "tyree", "deion", "lamont", "malik", "terrence", "tyrone", "lavon", "marcellus",
                  "wardell", "nichelle", "shereen", "ebony", "latisha", "shaniqua", "jasmine", "tanisha",
                  "tia", "lakisha", "latoya", "yolanda", "malika", "yvette")
  weat_good <- c("caress", "freedom", "health", "love", "peace", "cheer", "friend", "heaven", "loyal", "pleasure",
                 "diamond", "gentle", "honest", "lucky", "rainbow", "diploma", "gift", "honor", "miracle", "sunrise",
                 "family", "happy", "laughter", "paradise", "vacation")
  weat_bad <- c("abuse", "crash", "filth", "murder", "sickness", "accident", "death", "grief", "poison", "stink",
                "assault", "disaster", "hatred", "pollute", "tragedy", "bomb", "divorce", "jail", "poverty", "ugly",
                "cancer", "evil", "kill", "rotten", "vomit")
  
  sA <- c(weat_white, weat_black)
  sB <- c(weat_good, weat_bad)
  expand_grid(i=1:length(sA), j=1:length(sB)) %>%
  rowwise() %>%
  mutate(aterm = sA[i],
         bterm = sB[j],
         anorm = norm(embm[aterm,], "2"),
         bnorm = norm(embm[bterm,], "2"),
         azip = which(etokens == aterm),
         bzip = which(etokens == bterm),
         ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
         ab_ip = embm[aterm,] %*% embm[bterm,],
         nprod = anorm * bnorm,
         plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
         minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
  weat_angles
  
  return(weat_angles)
}

# Plot the inner product decomposed by the sum/difference vector norms and the angle/magnitude factors
# You can see how the angular decomposition classifies the relative scale (parallelogram shape)
# But cosine similarities that are close have more similar relative scale
# And the distribution of relative scale is not uniform
plot_angle_manifold <- function(r_angles) {
  r_angles %>%
    ggplot(aes(x=nprod, y=ab_cs)) +
    geom_point(aes(color=plus_dnorm, size=minus_dnorm)) +
    # geom_rug(aes(color=plus_dnorm), outside=T) +
    geom_smooth(method="loess", color="grey75") +
    scale_color_viridis_c() +
    theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) +
    labs(x=latex2exp::TeX("$||A||*||B||$"), y=latex2exp::TeX("$cos(A, B)$"),
         color=latex2exp::TeX("||D(+)||"), size=latex2exp::TeX("||D(-)||"))
}

# Function to plot the inner product space instead
# 
# plot_angle_manifold_ip <- function(r_angles) {
#   r_angles %>%
#     ggplot(aes(x=anorm*bnorm, y=ab_cs)) +
#     geom_point(aes(color=ab_ip, size=ab_ip)) +
#     # geom_rug(aes(color=plus_dnorm), outside=T) +
#     geom_smooth(method="loess", color="grey75") +
#     scale_color_viridis_c() +
#     theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) +
#     labs(x=latex2exp::TeX("$||A||*||B||$"), y=latex2exp::TeX("$cos(A, B)$"),
#          color=latex2exp::TeX("||D(+)||"), size=latex2exp::TeX("||D(-)||"))
# }
# plot_angle_manifold_ip(random_angles_100)


# Get a relatively big graph to work with
# Alternatively, also use a rarity window and/or up-sample the more frequent term space
# In practice I think the latter especially results in term samples that are more "realistic"
#  relative to what people are actually doing in the social sciences. Generally you want the
#  rare terms to be more common rare words and not "UCI_ProTour_cycling" or whatever
random_angles_200 <- make_angles_2(200)
p <- plot_angle_manifold(random_angles_200)

# WEAT name range sample
# Look at 100 random tokens in the WEAT name frequency ranges
# wbt <- which(etokens %in% weat_black)
# wwt <- which(etokens %in% weat_white)
# random_angles_100_uneven <- make_angles_2(100, a_rmin=min(wwt), a_rmax=max(wwt), b_rmin=min(wbt), b_rmax=max(wbt))
# p <- plot_angle_manifold(random_angles_100_uneven)

ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 6,
  colour = "dodgerblue",
  fill = "dodgerblue",
  alpha = 0.3
)
```

```{r inpro, echo=F, message=F, fig.height=4, fig.width=6, fig.align="center"}
# Cosine similarity as a locally linear perspective on the inner product space
# The factorization implies you can kind of look at the manifold from three "sides"
local_linear_decomp <- function(rangs) {
  # Plot some inner product distribution by components. Larger objects in the
  # space have more differentiated IPs, but there are also way fewer of them.
  rangs %>%
    ggplot(aes(y=ab_cs, color=ab_ip, x=anorm*bnorm, size=ab_ip)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v1
  
  # Cosine similarity differentiates three regions on the inner product space
  # that we can more easily see if we look from the cosine similarity into the
  # manifold. Looking from this perspective helps you see the "twist" structure
  # in A by rotating to the position in which the "flat" structure has a minimal
  # profile and the twist has a maximal profile. The extremal region of the twist
  # (by LNPW) is generally more "flat" than it was from the other perspective.
  # The linear relationship between LNPW and the IP is controlled by cosine similarity.
  rangs %>%
    ggplot(aes(color=ab_cs, y=ab_ip, x=anorm*bnorm, size=ab_cs)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v2
  
  # Cosine similarity has a linear relationship to the inner product, but only
  # conditional on the local norm product weight, which defines a class of vector pairs
  # with the same CS-IP linear relationship. The distribution of cosine similarities in
  # any given class is generally skewed and tends to have non-zero mean. The variance of
  # the cosine similarity distribution is non-constant and varies systematically with the
  # local norm product weight. One way of thinking about this is that the cosine similarity
  # is a maximally linear perspective on the manifold.
  rangs %>%
    ggplot(aes(x=ab_cs, y=ab_ip, color=anorm*bnorm, size=anorm*bnorm)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v3
  
  grid.arrange(v1, v2, v3, ncol=3)
}

local_linear_decomp(random_angles_200)
```

```{r, include=F, eval=F}
# Another interesting thing is to look at how the scale dimension differentiates the big IPs
# IOW there's a differentiated notion of unrelatedness and relatedness in the model (?)
# Interestingly this is sorta symmetric? 
# The middle range seems to capture a lot of different semantic unrelations
# e.g "giuliani-filmography", "liberalization-favourite", "arizona-byzantines", "president-catwalk"
# This is also true for the forms of relatedness
# e.g. "abundant-liquidity", "libertarians-baptists", "kiosks-hangars", "justice-protest"
# The extremes seem to be "different language" and "related but ungrammatical together" ????
# e.g. "yen-liquidity" and "yen-imf" are in different domains but are strongly related
# On the other end, "tempt-convinces", "evaluate-augment", "empty-gone", and "emphasize-observe" are like
#  related acts/states with different modes?
# Or put it this way: you could come up with a model that differentiates meaning in this way instead
# But the model optimizes for a maximally expressive cosine similarity spread
random_angles_200 %>%
  filter(ab_ip <= -6) ->
  unrelatedness

random_angles_200 %>%
  filter(ab_ip >= 8) ->
  relatedness

random_angles_200 %>%
  filter(ab_ip <= 0.1, ab_ip >= -0.1) ->
  nonrelatedness
```

```{r, include=F, eval=F}
# 3D visualization easter egg for those reading the code :)
# Makes the hyperbolic paraboloid really obvious
library(rgl)

# Rescale dimensions of the decomposition
x1 <- (random_angles_200$nprod - min(random_angles_200$nprod)) / (max(random_angles_200$nprod) - min(random_angles_200$nprod))
y1 <- (random_angles_200$ab_cs - min(random_angles_200$ab_cs))/(max(random_angles_200$ab_cs) - min(random_angles_200$ab_cs))
z1 <- (random_angles_200$ab_ip - min(random_angles_200$ab_ip))/(max(random_angles_200$ab_ip) - min(random_angles_200$ab_ip))

# Plot the whole 
rgl.open()
rgl.bg(color = "white")
par3d(windowRect = 50 + c(0,0,650,650))
rgl.points(x1, y1, z1, color="tomato", size=4, alpha=0.7)
rgl.bbox(color=c("#333377","black"), emission="#333377",
         specular="#3333FF", shininess=5, alpha=0.4)
rgl.lines(c(0, 1), c(0, 0), c(0, 0), color = "red", lwd=2)  # LNPW
rgl.lines(c(0, 0), c(0,1), c(0, 0), color = "blue", lwd=2)  # CS
rgl.lines(c(0, 0), c(0, 0), c(0,1), color = "green", lwd=2)  # IP
```

Figure \ref{fig:sumvar1} shows that mean of cosine similarities between the vector subspace $A$ and any term in $B$ is predictable in terms of the length of the sum vector $||D_A(+)||$. The behavior of the mean is non-linear and non-monotonic in the number of vectors in $A$; the variance is considerable when the number of vectors is low.

```{r sumvar_setup, echo=F, message=F}
plot_sumv_projection <- function(test_term="alex") {
  wgs_rand_r400000 <- t(apply(embm[sample(1:nrow(embm), 1000),], 1, scale2))  # Scaled 
  wgs_rand_r200000 <- t(apply(embm[sample(1:(0.5 * nrow(embm)), 1000),], 1, scale2))  # Scaled 
  wgs_rand_r100000 <- t(apply(embm[sample(1:(0.25 * nrow(embm)), 1000),], 1, scale2))  # Scaled 
  wgs_rand_r40000 <- t(apply(embm[sample(1:(0.1 * nrow(embm)), 1000),], 1, scale2))  # Scaled 
  
  cossim_ <- function(i, rmat) {
    lsa::cosine(embm[test_term,], rmat[i,])
  }
  
  sum_cossim <- function(i, rmat) {
    sum(sapply(1:i, cossim_, rmat=rmat))
  }
  
  # Plot a single term against an expanding sum vector of random terms from embedding (rarity stratified)
  # Note varying X and Y axes.
  k <- 100
  expand.grid(i=2:k) %>%
    rowwise() %>%
    mutate(dnorm_r40000 = norm(colSums(wgs_rand_r40000[1:i,]), "2"),
           dnorm_r100000 = norm(colSums(wgs_rand_r100000[1:i,]), "2"),
           dnorm_r200000 = norm(colSums(wgs_rand_r200000[1:i,]), "2"),
           dnorm_r400000 = norm(colSums(wgs_rand_r400000[1:i,]), "2"),
           sim_r40000 = lsa::cosine(embm[test_term,], colSums(wgs_rand_r40000[1:i,])),
           sim_r100000 = lsa::cosine(embm[test_term,], colSums(wgs_rand_r100000[1:i,])),
           sim_r200000 = lsa::cosine(embm[test_term,], colSums(wgs_rand_r200000[1:i,])),
           sim_r400000 = lsa::cosine(embm[test_term,], colSums(wgs_rand_r400000[1:i,])),
           simsum_r40000 = sum_cossim(i, wgs_rand_r40000),
           simsum_r100000 = sum_cossim(i, wgs_rand_r100000),
           simsum_r200000 = sum_cossim(i, wgs_rand_r200000),
           simsum_r400000 = sum_cossim(i, wgs_rand_r400000),
           simmean_r40000 = simsum_r40000/i,
           simmean_r100000 = simsum_r100000/i,
           simmean_r200000 = simsum_r200000/i,
           simmean_r400000 = simsum_r400000/i) %>%
    mutate(cmean_sk = mean(.$sim_r40000[1:i-1]),
           cmean_sr = mean(.$sim_r100000[1:i-1]),
           cmean_sp = mean(.$sim_r200000[1:i-1]),
           cmean_sv = mean(.$sim_r400000[1:i-1])) ->
    term_simd
  
  grid.arrange(term_simd %>%
                 ggplot(aes(color=dnorm_r40000, y=sim_r40000, x=i)) +
                 geom_point() +
                 labs(title=latex2exp::TeX(paste0("proj($v_{", test_term, "}$, $D_{rand})$")),
                      color="||D|| (top 10% terms)", y=latex2exp::TeX("$\\frac{1}{n}\\sum_{D} cos(v, d)$")) +
                 theme(legend.position="bottom"),
               term_simd %>%
                 ggplot(aes(color=dnorm_r100000, y=sim_r100000, x=i)) +
                 geom_point() +
                 labs(title=latex2exp::TeX(paste0("proj($v_{", test_term, "}$, $D_{rand})$")),
                      color="||D|| (top 25% terms)", y=latex2exp::TeX("$\\frac{1}{n}\\sum_{D} cos(v, d)$")) +
                 theme(legend.position="bottom"),
               term_simd %>%
                 ggplot(aes(color=dnorm_r200000, y=sim_r200000, x=i)) +
                 geom_point() +
                 labs(title=latex2exp::TeX(paste0("proj($v_{", test_term, "}$, $D_{rand})$")),
                      color="||D|| (top 50% terms)", y=latex2exp::TeX("$\\frac{1}{n}\\sum_{D} cos(v, d)$")) +
                 theme(legend.position="bottom"),
               term_simd %>%
                 ggplot(aes(color=dnorm_r400000, y=sim_r400000, x=i)) +
                 geom_point() +
                 labs(title=latex2exp::TeX(paste0("proj($v_{", test_term, "}$, $D_{rand})$")),
                      color="||D|| (all terms)", y=latex2exp::TeX("$\\frac{1}{n}\\sum_{D} cos(v, d)$")) +
                 theme(legend.position="bottom"),
               ncol=2)
}

# TODO: Show more of the 25%-50% range?
```

```{r sumvar1, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Non-monotonicity and non-convergence of arithmetic mean cosine similarity between arbitrary term vector (v) and a set of $i$ random vectors ($d \\in D$). Cells split by term rarity window; note free X and Y scales. Color shows increasing magnitude of sum vector along $i$. Note non-monotonicity and non-convergence."}
plot_sumv_projection("sociology")
```

```{r try}
make_angles_tri <- function(k=50, rmin=g_rmin, rmax=g_rmax) {
  sA <- sample(etokens[rmin:rmax], k)
  sB <- sample(etokens[rmin:rmax], k)
  sC <- sample(etokens[rmin:rmax], k)
  data.frame(i=1:k, j=1:k, o=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           cterm = sC[o],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           cnorm = norm(embm[cterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           czip = which(etokens == cterm),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           bc_cs = lsa::cosine(embm[bterm,], embm[cterm,])[1],
           ac_cs = lsa::cosine(embm[aterm,], embm[cterm,])[1],
           tri = (bc_cs * ac_cs) + sqrt((1 - bc_cs^2)*(1 - ac_cs^2)),
           ab_ip = embm[aterm,] %*% embm[bterm,],
           ab_nprod = anorm * bnorm,
           bc_nprod = bnorm * cnorm,
           ac_nprod = anorm * cnorm) ->
    random_angles_3
  
  return(random_angles_3)
}

ra3_200 <- make_angles_tri(200, rmax=g10_rmax)

# TODO: Try cos(xy) <= cos(xz)*cos(yz) + sin(xz)*sin(yz)
# Also cos(xy) <= cos(xz-yz)
# In what sense does this help me "add" them?
```

\pagebreak

# Relative scale bias in a few common arithmetic comparisons

A number of arithmetic strategies for cosine similarity comparison can be found in the literature. Consider statistics of the form $\psi(\mathcal{S, \sim})$ where $\mathcal{S}$ is one or more sets of cosine similarities and $\sim$ is an arithmetic comparison operator (usually `+`). The most common arithmetic strategies for averaging cosine similarities can be described in terms of the fully connected undirected graph $G(A,B)$ between term sets $A$ and $B$, where $A_i$ and $B_j$ have an edge weighted by their cosine similarity. I show that the scalar projection $||D^+_{AB}||\cos(W, D^+_{AB})$ recurs in each of three basic arithmetic approaches to summarizing $G(A,B)$. These approaches are summarized in Figure \ref{fig:arithmetic_strats}: summing all of the entries (1), summing the diagonal entries when $A \neq B$ (2), and summing the off-diagonal lower- or upper-triangular entries when $A = B$ (3).

```{r arithmetic_strats, echo=F, out.width="\\textwidth", fig.cap="Arithmetic comparisons on $G(A, B)$; matrix entries represent $\\cos(A_i, B_j)$."}
knitr::include_graphics("./arithmetic_comparisons.png")
```

An awkward feature of this methodology is the possibility of even or odd choices of set size; in general I focus on the even case for simplicity of notation. However, the simplest case of the off-diagonal sum involves three vectors, which provides an opportunity to discuss the geometry implied by higher-order comparisons.

## Axial comparison as a simple case

Consider the comparison of one focal vector $W$ against a growing set of $n$ alternative vectors $\Omega(n)$. Denote the smallest possible alternative set $\Omega(2): \{A, B\}$. The usual way to compare $W$ to $A$ and $B$ is to add their cosine similarities: $cos(\theta_{WA}) + cos(\theta_{WB})$. The relative scale dependence of this sum can be expressed in terms of the normalized sum vector $D = \frac{A}{||A||} + \frac{B}{||B||}$. The sum is a scalar projection of the sum vector onto the the comparison vector:

$$
\begin{aligned}
cos(\theta_{WA}) + cos(\theta_{WB}) &= \frac {W \cdot A}{||W||\;||A||} + \frac {W \cdot B}{||W||\;||B||}  \\
&= \frac{W}{||W||} \cdot \left( \frac{A}{||A||} + \frac{B}{||B||} \right) \\
&= \frac{W \cdot D}{||W||} \\
&= \frac{||W||\;||D||\; cos(\theta_{WD})}{||W||} \\
&= ||D||\;cos(\theta_{WD}).
\end{aligned}
$$

The quantity has two components with distinct behavior: a cosine similarity component between the focal vector and the sum vector $cos(\theta_{WD})$ and a vector norm component $||D||$. The norm of the sum vector $||D||$ can be written as a function of $\theta_{AB}$; let $\phi(\theta_{\alpha}) = ||D||$ be this function. Geometrically, the norm of $D$ is the length of one of the two diagonals of the parallelogram implied by $A$ and $B$, depending on which arithmetic comparison operator we are considering. Addition corresponds to the major diagonal and subtraction corresponds to the minor diagonal. Alternatively, when the comparison operator is subtraction, $D$ can also be interpreted as the chord on the unit ball corresponding to the subtended angle $\theta_{AB}$.

$$
\begin{aligned}
||D(+)|| &= ||\frac{A}{||A||} + \frac{B}{||B||}|| = \sqrt{2 + 2\cos(\theta_{AB}))} \\
||D(-)|| &= ||\frac{A}{||A||} - \frac{B}{||B||}|| = \sqrt{2 - 2\cos(\theta_{AB}))} \\
\end{aligned}
$$

It is useful to think about $\phi(\theta_{\alpha})$ as a weight on the cosine similarity component. Notably, the weight (which ranges from $[0, 2]$) is nonlinear and becomes smaller more quickly as the cosine similarity decreases (Figure \ref{fig:halfsine}, red function). When the standard basis subtraction operation is employed instead, the weight function is mirrored over its domain, so the weight function increases more slowly as the underlying cosine similarity approaches 1 (Figure \ref{fig:halfsine}, blue function).

```{r halfsine, echo=F, message=F, fig.height=4, fig.width=4, fig.align="center", fig.cap="Standard basis arithmetic weight functions. Norm of D; a function of $cos(\\theta_{AB})$. Red: Addition weight function. Purple: Subtraction weight function. Blue: Mean weight function."}
data.frame(x=seq(-1, 1, by=0.001)) %>%
  mutate(y=sqrt(2 + (2 * x)),
         y2 = y/2,
         yminus = sqrt(2 - (2 * x))) %>%
  ggplot(aes(x, y)) +
  geom_line(color="tomato") +
  geom_line(aes(y=yminus), color="dodgerblue") +  # `-` instead of `+`
  geom_line(aes(y=y2), color="purple") +  # Mean function instead of the sum
  geom_hline(yintercept=1, linetype="dashed") +
  coord_fixed() +
  labs(x=latex2exp::TeX("$cos(\\theta_{AB})$"),
       y=latex2exp::TeX("$\\phi(\\theta_{AB})$"))
```

The quantity $||D||\;cos(\theta_{WD})$ does not have the same interpretation as a cosine similarity. In particular, the sum of two cosine similarities is not independent of the underlying component vector norms $||A||$ and $||B||$; it is a function of them through $D$. Consequently, any sum of cosine similarities is a function of the underlying relative scale of the observations. I show in the appendix that a few common strategies for cosine similarity arithmetic result in analogous scalar projections that exhibit relative scale bias.

The behavior of the mean of a set of cosine similarities is of special interest. Means of cosine similarities (Figure \ref{fig:halfsine}, violet function) also exhibit relative scale bias due to the use of addition as the comparison operator. As a result of the definition of $D$ as the sum of two normalized (unit-length) vectors, $||D|| \leq 2$. The bound is achieved only when the two vectors are completely aligned ($\cos(\theta_{AB}) = 1$); this is very uncommon in empirical vector space models. On the other hand, also notice that larger values of $n$ lead to a more approximately uniform (i.e. more "flat") weight function in this more densely populated region of the cosine similarity function. Means over larger sets of vectors will consequently be more well-behaved in distribution than unadjusted sums, especially in models where the average cosine similarity tends to be positive (Mimno & Thompson 2017). However, because the degree of approximation to uniformity in the weights always breaks down as the cosine similarity approaches 1 and $\theta_{AB}$ becomes small, this approximation to a norm-adjusted cosine similarity merely obscures the relative scale bias in the resulting statistic.

The cosine similarity between the comparison word $W$ and the sum vector $D^+_{AB}$ *is also scale-dependent* with respect to $A$ and $B$, because $D$ is a vector-valued function of $A$ and $B$ as well as their respective norms. That is, it allows an observer to ignore the relative difference in scale between $W$ and $D^+_{AB}$, but it requires information about $||A||$ and $||B||$ to perform this task. The easiest way to see this is that the norm $||D||$ reappears in the denominator of the cosine similarity:

$$\cos(W, D^+_{AB}) = \frac{W \cdot D^+_{AB}}{||W||||D^+_{AB}||}$$  
Consequently, factoring out the norm component of the scalar projection does not fully address the problem. Readers may notice that the norm recurs in the numerator and the denominator; however, allowing the norms to cancel simply leads to an inner product that has only been normalized by the scale of $W$ (i.e. a scalar projection). The bias is inescapably linked to the arithmetic design of the vector comparison.

## Arithmetic compositions of scalar projections

\textbf{Two-way sum.} Similarity of each vector in $A$ with each vector in $B$. Results in a sum of scalar projections of paired sum vectors over $A$ onto comparison vectors in $B$.

$$
\begin{aligned}
\psi(\{A, B\}, +) &= \sum_{a_i \in A}\sum_{b_i \in B} \cos(\theta_{a_ib_i}) \\
&= \sum_{b_i \in B} \cos(\theta_{a_1b_i}) + \sum_{b_i \in B} \cos(\theta_{a_2b_i}) + ... + \sum_{b_i \in B} \cos(\theta_{a_nb_i}) \\
&= \sum_{b_i \in B} ||D^{+}_{a(1, 2)}|| \cos(b_i, D^{+}_{1, 2}) + ... + \sum_{b_i \in B} ||D^{+}_{a(n-1,n)}|| \cos(b_i, D^{+}_{a(n-1,n)}) \\
&= \sum_{a_i, a_i+1 \in A} \sum_{b_i \in B} ||D^{+}_{a(i, i+1)}|| \cos(b_i, D^{+}_{i, i+1}). \\
\end{aligned}
$$

\textbf{Diagonal (external pairwise) sum.} Similarity of matched (potentially ordered?) pairs of vectors in $A$ and $B$.

$$
\begin{aligned}
\psi(\{(A, B)\}, +) &= \sum_{i = 0}^{|A|}\cos(\theta_{a_ib_i})
\end{aligned}
$$

\textbf{Off-diagonal (internal pairwise) sum.} Similarity of $A_i$ with vectors in $A(-i)$; that is, comparing $A_i$ to $A$ with the $i$-th observation withheld to avoid the trivial self-comparison. The $k=2$ case is trivial, so the simplest case to work with is the pairwise $k=3$ sum: $\cos(a, b) + \cos(a, c) + \cos(b, c)$. This quantity can be written as a function of the norm of their sum vector $||D^+_{abc}||$. This identity can be derived by rearranging the three-dimensional law of cosines for the unit parallelepiped defined by ${a, b, c}$ (line 2); this is analogous to finding the length of the major diagonal of the parallelogram implied by ${a, b}$ in section 2 of the paper.

$$
\begin{aligned}
\psi(A, +)  &= \sum_{a_i \in A}\sum_{a_j \in A(-i)} \cos(\theta_{a_ia_j}). \\
\cos(a, b) + \cos(a, c) + \cos(b, c) &= ||D^+_{ab}||\cos(c, D^+_{ab}) + \cos(a, b) \\
&= ||D^+_{bc}||\cos(a, D^+_{bc}) + \cos(b, c) \\
&= ||D^+_{ac}||\cos(b, D^+_{ac}) + \cos(a, c) \\
\cos(a, b) + \cos(a, c) + \cos(b, c) &= 3||D^+_{abc}|| - \left[||D^+_{ab}||\cos(c, D^+_{ab}) + ||D^+_{bc}||\cos(a, D^+_{bc}) + ||D^+_{ac}||\cos(b, D^+_{ac})\right] \\
\end{aligned}
$$

```{r paired_sm, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Three-way paired sum vector norm function."}
# Paired one-way sum, N=3
# Just look at the positive space for a second
expand.grid(x=seq(-1, 1, by=0.01),
            y=seq(-1, 1, by=0.01),
            z=seq(-1, 1, by=0.01)) ->
  pair_oneway

# Plot all three bivariate views of the function
# When all of the cosine similarities are more negative, you can factor out the -1
grid.arrange(pair_oneway %>%
               filter(z >= 0.5) %>%
               rowwise() %>%
               mutate(s=sqrt(3 + 2*x + 2*y + 2*z)) %>%
               ggplot(aes(x, y, fill=s)) +
               geom_raster() +
               coord_fixed() +
               scale_fill_viridis_c() +
               theme(legend.position="bottom"), 
             pair_oneway %>%
               filter(y >= 0.5) %>%
               rowwise() %>%
               mutate(s=sqrt(3 + 2*x + 2*y + 2*z)) %>%
               ggplot(aes(x, z, fill=s)) +
               geom_raster() +
               coord_fixed() +
               scale_fill_viridis_c() +
               theme(legend.position="bottom"),
             pair_oneway %>%
               filter(x >= 0.5) %>%
               rowwise() %>%
               mutate(s=sqrt(3 + 2*x + 2*y + 2*z)) %>%
               ggplot(aes(y, z, fill=s)) +
               geom_raster() +
               coord_fixed() +
               scale_fill_viridis_c() +
               theme(legend.position="bottom"),
             ncol=3)
  # scale_fill_viridis_c() +
  # labs(x=latex2exp::TeX("$cos(\\theta_{BC})$"),
  #      y=latex2exp::TeX("$cos(\\theta_{AD})$"))
```

\pagebreak

# Alignment, similarity, meaning

Greater caution is warranted in applying trigonometric measures of similarity to social research. The thing that makes vector space models expressive as a representation of data is the decomposition of a high-dimensional inner product into independent angular and scalar components. This factorization of the data is remarkably useful, but for better or worse, the way that the cosine function accomplishes this task is dependent on the norms of the component vectors. Consequently the statistic's scale independent interpretation is sensitive to the method of comparison we apply when we summarize sets of cosine similarities. In particular, the dominant strategy for comparison (standard arithmetic) results in statistics weighted in an unusual way by the underlying geometry of the vector subspaces implied by adding vectors together.

Research in statistical text analysis frequently invokes the idea that "the meaning of words lies in their use" (Wittgenstein 1953 cited in Firth 1957). But Wittgenstein's view was more complicated than this. In particular, he writes, when we examine the many correspondences between features and qualities of members of a category (e.g. games), "...we see a complicated network of similarities overlapping and criss-crossing: sometimes overall similarities, sometimes similarities of detail. ...I can think of no better expression to characterize these similarities than 'family resemblances'" (66-67).

\pagebreak

# Works referenced

\begingroup

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{4pt}
\onehalfspacing
\noindent

Antoniak, M. and D. Mimno. 2021. "Bad Seeds: Evaluating Lexical Methods for Bias Measurement." *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing*: 1889-1904.

Arseniev-Koehler, A. 2022. "Theoretical foundations and limits of word embeddings: what types of meaning can they capture?" SocArXiv preprint: https://osf.io/preprints/socarxiv/vrwk3/.

Arseniev-Koehler, A., & Foster, J. G. 2020. "Machine learning as a model for cultural learning: Teaching an algorithm what it means to be fat." arXiv preprint: 2003.12133.

Askin, N. and M. Mauskapf. 2017. "What Makes Popular Culture Popular? Product Features and Optimal Differentiation in Music." *American Sociological Review* 82(5): 910-944.

Bertrand, M. and S. Mullainathan. 2004. "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination." *American Economic Review*  94: 991–1013.

Caliskan, A, J.J. Bryson and A. Narayanan. 2017. "Semantics derived automatically from language corpora contain human-like biases." *Science* 356(6334): 183-186.

Cochrane, C., L. Rheault, J.-F. Godbout, T. Whyte, M. W.-C. Wong, and S. Borwein. "The Automatic Analysis of Emotion in Political Speech Based on Transcripts." *Political Communication* 39(1): 98-121.

Evans, J.A. 2010. “Industry Induces Academic Science to Know Less about More.” *American Journal of Sociology* 116(2): 389-442.

Farrell, J. 2016. "Network structure and influence of the climate change counter-movement." *Nature Climate Change* 6: 370-374.

Fuhse, J., O. Stuhler, J. Riebling, and J.L. Martin. 2020. "Relating social and symbolic relations in quantitative text analysis: A study of parliamentary discourse in the Weimar Republic." *Poetics* 78: 101363.

Greenwald, A.G, D. E. McGhee, and J. L. Schwartz. 1998. "Measuring individual differences in implicit cognition: The implicit association test." *Journal of Personality and Social Psychology* 74: 1464–1480.

Grimmer, J., M.E. Roberts, and B.M. Stewart. 2022. *Text as Data: A new framework for machine learning and the social sciences.* Princeton University Press.

Jones, J.J., M.R. Amin, J. Kim, and S. Skiena. 2019. “Stereotypical Gender Associations in Language Have Decreased Over Time.” *Sociological Science* 7:1-35.

Kozlowski, A.C., M. Taddy, and J.A. Evans. 2019. "The geometry of culture: Analyzing the meanings of class through word embeddings." *American Sociological Review* 84(5): 905-949.

Krzanowski, W.J. 1979. "Between-groups comparison of principal components." *Journal of the American Statistical Association* 74(367): 703-707.

Lix, K., A. Goldberg, S. B. Srivastava, M. A. Valentine. 2022. "Aligning Differences: Discursive Diversity and Team
Performance." *Management Science*, in press. https://doi.org/10.1287/mnsc.2021.4274

Martin-Caughey, A. 2021. "What’s in an Occupation? Investigating Within-Occupation Variation and Gender Segregation Using Job Titles and Task Descriptions." *American Sociological Review* 86(5): 960-999.

Mimno, D. and L. Thompson. 2017. "The strange geometry of skip-gram negative sampling." *ICLR* TODO

Mu, J. and P. Viswanath. 2018 "All-but-the-Top: Simple and Effective Postprocessing for Word Representations." *Proceedings of the 6th International Conference on Learning Representations (2018)*: 1-25.

Nelson, L.K. 2021. "Leveraging the alignment between machine learning and intersectionality: Using word embeddings to measure intersectional experiences of the nineteenth century U.S. South." *Poetics* 88: 101539.

Pennington, J., R. Socher and C.D. Manning. 2014. "GloVe: Global Vectors for Word Representation." *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*: 1532-1543.

Rodriguez, P. and A. Spirling. 2022. "Word Embeddings: What works, what doesn’t, and how to tell the difference for applied research." *Journal of Politics* 84(1): 101-115.

Salton, G. and M.J. McGill. 1986. *Introduction to modern information retrieval.*

van Loon, A., S. Giorgi, R. Willer, and J. Eichstaedt. 2022. "Negative Associations in Word Embeddings Predict anti-Black Bias Across Regions -- but only via Name Frequency." arXiv preprint: 2201.08451.


\endgroup

\pagebreak

# Appendix A: Additional plots of inner product surface

Figures \ref{fig:iprod_dnorm2}, \ref{fig:iprod_dnorm3} and \ref{fig:iprod_dnorm4} show the behavior of the bias as the number of vectors in each set decreases from $k=100$. The next two figures display the pairwise cosine similarities corresponding to vector pairs with close to equal norms (Figure \ref{fig:iprod_close}) and vector pairs with a more uneven norm ratio (Figure \ref{fig:iprod_far}). The set of cosine similarities considered thus far underlies the two-way sum; the lower-triangular $AA$ sum and the diagonal $AB$ sum are shown in Figures \ref{fig:ip_lowertri} and \ref{fig:ip_diag}. Figures \ref{fig:iprod_top50} through \ref{fig:iprod_top01} display increasingly common term subspaces. Figures \ref{fig:sumvar2} and \ref{fig:sumvar3} show the behavior of the mean over alternative choices of comparison vector.

Figures \ref{fig:isoclines1} through \ref{fig:isoclines3} display the isoclines of the inner product surface depicted in the main paper. Values on the surface are grouped by quantile, showing the locally linear structure of the factorization: any given point on the manifold is linearly related to a set of points with the same relative scale and linearly related to a set of points with the same cosine similarity. Each of these linear relationships varies smoothly in the other one, so local arithmetic comparisons of cosine similarities are approximately correct. But the whole surface is curved!


```{r iprod_dnorm2, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=100"}
random_angles_100 <- make_angles(100)
plot_angle_manifold(random_angles_100)
```

```{r iprod_dnorm3, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50"}
random_angles_50 <- make_angles(50)
plot_angle_manifold(random_angles_50)
```

```{r iprod_dnorm4, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=25."}
random_angles_25 <- make_angles(25)
plot_angle_manifold(random_angles_25)
```

```{r iprod_close, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=200. Filtered to show only close norm ratios (the vectors are almost equal length). Note more linear relationship in local norm product."}
# Another view of the close comparisons
# Note linear relationship
close_ratio_t <- 0.02
random_angles_200 %>%
    rowwise() %>%
    filter(((anorm / bnorm) < 1.02) & ((anorm / bnorm) > 0.98)) %>%
    ggplot(aes(x=anorm*bnorm, y=ab_cs)) +
    geom_point(aes(color=plus_dnorm, size=minus_dnorm)) +
    geom_smooth(color="grey75") +
    scale_color_viridis_c()
```

```{r iprod_far, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Two-way pairwise cosine similarities, k=200. Filtered to show only distant norm ratios (one vector is at least 1.5x as long). Note more uniform relationship over local norm product."}
# Look at just the ones where one vector is a lot longer
# The relationship is pretty flat!
far_ratio_t <- 0.5
random_angles_200 %>%
    rowwise() %>%
    filter(anorm / bnorm >= (1+far_ratio_t) | bnorm / anorm >= (1+far_ratio_t)) %>%
    ggplot(aes(x=anorm*bnorm, y=ab_cs)) +
    geom_point(aes(color=plus_dnorm, size=minus_dnorm)) +
    geom_smooth(color="grey75") +
    scale_color_viridis_c()
```

```{r ip_lowertri, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Internal pairwise cosine similarities, k=50."}
k <- 50
rarity_imin <- 1
rarity_imax <- nrow(embm)
sA <- sample(etokens[rarity_imin:rarity_imax], k)
sB <- sample(etokens[rarity_imin:rarity_imax], k)
expand_grid(i=1:k, j=1:k) %>%
  filter(i > j) %>%  # Lower triangular sum/mean of aTa
  rowwise() %>%
  mutate(aterm = sA[i],
         bterm = sA[j],
         anorm = norm(embm[aterm,], "2"),
         bnorm = norm(embm[bterm,], "2"),
         nprod = anorm * bnorm,
         ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
         abi_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
         plus_dnorm = norm(as.vector(scale2(embm[aterm,]) + scale2(embm[bterm,])), "2"),
         minus_dnorm = norm(as.vector(scale2(embm[aterm,]) - scale2(embm[bterm,])), "2")) ->
  random_angles_int
plot_angle_manifold(random_angles_int)
```

```{r ip_diag, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="External pairwise cosine similarities, k=50."}
plot_angle_manifold(random_angles_200 %>% filter(i == j))
```

```{r iprod_top50, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50. Top 50\\% of terms by frequency."}
random_angles_50_top50 <- make_angles(50, a_rmax=n_glove * 0.5, b_rmax=n_glove * 0.5)
plot_angle_manifold(random_angles_50_top50)
```

```{r iprod_top25, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50. Top 25\\% of terms by frequency."}
random_angles_50_top25 <- make_angles(50, a_rmax=n_glove * 0.25, b_rmax=n_glove * 0.25)
plot_angle_manifold(random_angles_50_top25)
```

```{r iprod_top10, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50. Top 10\\% of terms by frequency."}
random_angles_50_top10 <- make_angles(50, a_rmax=n_glove * 0.1, b_rmax=n_glove * 0.1)
plot_angle_manifold(random_angles_50_top10)
```

```{r iprod_top05, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50. Top 5\\% of terms by frequency."}
random_angles_50_top05 <- make_angles(50, a_rmax=n_glove * 0.05, b_rmax=n_glove * 0.05)
plot_angle_manifold(random_angles_50_top05)
```

```{r iprod_top01, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Pairwise cosine similarities, k=50. Top 1\\% of terms by frequency."}
random_angles_50_top01 <- make_angles(50, a_rmax=n_glove * 0.01, b_rmax=n_glove * 0.01)
plot_angle_manifold(random_angles_50_top01)
```

```{r sumvar2, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Sum of cosine similarities of arbitrary term vector (v) and a set of i random vectors ($d \\in D$). Cells split by term rarity window of ; note free X and Y scales. Color shows increasing magnitude of sum vector along i."}
plot_sumv_projection("alex")
```

```{r sumvar3, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Sum of cosine similarities of arbitrary term vector (v) and a set of i random vectors ($d \\in D$). Cells split by term rarity window of ; note free X and Y scales. Color shows increasing magnitude of sum vector along i."}
plot_sumv_projection("prevaricate")
```

```{r isoclines1, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Local norm product weight isoclines; note approximate linearity."}
# local_linear_decomp(weat_angles)

# Show approximately locally linear relationship within size/similarity quantiles
# Note that the extreme quantiles tend to mix more than one "group"
# The inner product isoclines are included for reference, but note curvature
iles <- c(c(0, 0.00001, 0.0001, 0.001, 0.01), seq(0.1, 0.9, by=0.01), c(0.95, 0.99, 0.999, 0.9999, 1))
npq <- quantile(random_angles_200$nprod, iles)
csq <- quantile(random_angles_200$ab_cs, iles)
ipq <- quantile(random_angles_200$ab_ip, iles)

random_angles_200 %>%
  rowwise() %>%
  mutate(sizegroup=which.max(npq >= nprod)) %>%
  ggplot(aes(x=ab_cs, y=ab_ip, color=anorm*bnorm, size=anorm*bnorm)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") +
  facet_wrap(~sizegroup)
```

```{r isoclines2, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine similarity isoclines; note approximate linearity."}
random_angles_200 %>%
  rowwise() %>%
  mutate(simgroup = which.max(csq >= ab_cs)) %>%
  ggplot(aes(x=anorm*bnorm, y=ab_ip, color=ab_cs, size=ab_cs)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") +
  facet_wrap(~simgroup)
```

```{r isoclines3, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Inner product isoclines; note predictable change in nonlinearity by quantile."}
random_angles_200 %>%
  cbind(ipgroup=sapply(random_angles_200$ab_ip, function(x) which.max(ipq >= x))) %>%
  ggplot(aes(x=anorm*bnorm, y=ab_cs, color=ab_ip, size=ab_ip)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") +
  facet_wrap(~ipgroup)

```

\pagebreak

# Appendix B: Relative scale bias in analogical term association measurement

Recently, van Loon and colleagues (2022) observed an empirical case of relative scale bias in an application of the widely-cited Word Embedding Association Test (WEAT; Caliskan, Bryson & Narayanan 2017). WEAT adapts the "implicit association" methodology developed in social psychology (Greenwald, McGhee & Schwartz 1998) to measure differential association in large collections of text; for example, it has been used to measure the association between distinctively Black/white or masculine/feminine names and positive or negative terms in large social media post corpora. In an empirical application of WEAT to geotagged Twitter posts, van Loon and colleagues find that although the WEAT statistic at first seems to exhibit a robust linear association with many other measures of racial animus, this effect disappears when the geographically clustered relative rarity of Black names is added into the model. The authors conclude that "WEAT appears to be a highly sophisticated way for detecting linguistic bias, but it is in fact just a noisy proxy measuring how rare Black names are in the data." The authors infer that the geometry of word embedding models is to blame because such models "conflate the relative frequency of words prototypical of the groups with positivity" due to the relatively higher frequency of positive terms in the corpus compared to negative terms.

Applying the results above, I show that the observed relative scale bias results from the methodological choice of an arithmetic comparison operator. The statistic takes the difference in mean cosine similarity across pairs of group-identifying terms (e.g. Black or white American names) and sentiment-identifying terms (e.g. positive or negative terms) specified by the user. For some positive or negative sentiment word $W$, the score $s(W, A, B)$ is the difference in mean cosine similarity between that word and all of the words in each of $\mathbf{A}$ and $\mathbf{B}$. The full WEAT statistic, $S_{A, B, X, Y}$ takes the absolute difference of the sum of these scores over pairs of words $W$ in $\mathbf{X}$ and $\mathbf{Y}$:

$$s(W, A, B)=\operatorname{mean}_{a \in A} \cos (\vec{w}, \vec{a})-\operatorname{mean}_{b \in B} \cos (\vec{w}, \vec{b})$$

$$S_{A, B, X, Y}=\sum_{x \in X} s_{x, A, B}-\sum_{y \in Y} s_{y, A, B}$$  

This statistic can be written in terms of the scalar projection implied by the use of standard basis arithmetic. The key quantity in the WEAT statistic is the standard basis difference in cosine similarity between $W, A$ and $W, B$:

$$cos(\theta_{WA}) - cos(\theta_{WB})$$  

As previously shown, the difference in cosine similarities between two term vectors A and B with a third term vector C can be written as the scalar projection of the normalized difference vector $D = \frac{A}{||A||} - \frac{B}{||B||}$ onto to the attribute vector $W$, where $A$ and $B$ have been normalized through the cosine similarity function. $||D||$ is the norm weight function described previously.

$$
\begin{aligned}
cos(\theta_{WA}) - cos(\theta_{WB}) &= \frac {W \cdot A}{||W||\;||A||} - \frac {W \cdot B}{||W||\;||B||}  \\
&= \frac{W}{||W||} \cdot \left( \frac{A}{||A||} - \frac{B}{||B||} \right) \\
&= \frac{W \cdot D}{||W||} \\
&= \frac{||W||\;||D||\; cos(\theta_{WD})}{||W||} \\
&= ||D||\;cos(\theta_{WD}). \\
&\\
||D|| &= ||\frac{A}{||A||} - \frac{B}{||B||}|| \\
&= \sqrt{2 - 2\cos(\theta_{AB}))}. \\
(&= \phi(\theta_{AB}).) \\
\end{aligned}
$$

WEAT takes the difference between the means of $s(W, A, B)$ over pairs of names selected from lists $\mathbf{A_N}$ and $\mathbf{B_N}$ and across pairs of attribute terms selected from lists $\mathbf{X_M}$ and $\mathbf{Y_M}$. This results in a sum of scalar projections of the name difference vectors $D_i$ onto each attribute term $W$.

$$
\begin{aligned}
s(W, A, B)  &= \frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WA_i})-\frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WB_i})\\
            &= \frac{1}{n} \sum_{i=1}^{n} \cos(\theta_{WA_i}) - \cos(\theta_{WB_i}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} ||D_i||\;\cos(\theta_{WD_i}) \\
            &= \frac{1}{n} \sum_{i=1}^{n} \phi(\theta_{A_iB_i})\cos(\theta_{WD_i}) \\
\end{aligned}
$$

The full set of paired difference comparisons in $X$ and $Y$ can be parameterized similarly. Let $T$ be the set of difference vectors obtained through the bilinear form in the $X - Y$ space, analogous to what we have done with $D$ in the $A - B$ space).

$$
\begin{aligned}
S_{A, B, X, Y} &= \sum_{x \in X} s(x, A, B)-\sum_{y \in Y} s(y, A, B) \\
  &= \sum_{j=1}^{m} \left[ \frac{1}{n} \sum_{i=1}^{n} \phi(\theta_{A_iB_i})\cos(\theta_{xD_i}) \right]  -
     \sum_{j=1}^{m} \left[ \frac{1}{n} \sum_{i=1}^{n} \phi(\theta_{A_iB_i})\cos(\theta_{yD_i}) \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\theta_{A_iB_i})\cos(\theta_{x_jD_i}) - \phi(\theta_{A_iB_i})\cos(\theta_{y_jD_i}) \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\theta_{A_iB_i})(\cos(\theta_{x_jD_i})\;-\;\cos(\theta_{y_jD_i}))  \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\theta_{A_iB_i})(||T_j||\cos(\theta_{T_jD_i}))  \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \left[ \phi(\theta_{A_iB_i})\cdot\phi(\theta_{X_jY_j})\cdot\cos(\theta_{T_jD_i})  \right] \\
   &= \frac{1}{n} \sum_{j=1}^{m} \sum_{i=1}^{n} \phi^{(2)}_{ij}\cos(\theta_{T_jD_i}) \\
\end{aligned}
$$

For each analogy $A_i : B_i :: X_j : Y_j$, the statistic computes a quantity that can be written as a function of three angles: the angles between matched pairs of names/attributes pointing away from the origin, $\theta_{A_iB_i}$ and $\theta_{X_jY_j}$); and the angle between the two corresponding difference vectors, $\theta_{T_iD_i}$. The last component $\cos(\theta_{T_iD_i})$ has an appealing interpretation as the cosine similarity between $T$ and $D$, but the statistic weights this average by the product of the norms of the difference vectors. Let $\phi^{(2)}_{ij}$ be the *norm product weight function* for the analogy $A_i : B_i :: X_j : Y_j$. This is a function of the two absolute angles, $\theta_{A_iB_i}$ and $\theta_{X_jY_j}$. The functional form of these weights is non-linear (see \ref{fig:prodnorm}; note the increasingly "bent" contour as the weight decreases).

```{r prodnorm, echo=F, message=F, fig.height=4, fig.width=4, fig.align="center", fig.cap="Norm product weight function."}
scrat_f <- function(cs_ab, cs_xy) {
  return(sqrt(2 - 2*cs_ab) * sqrt(2 - 2*cs_xy))
}

data.frame(x=seq(-1, 1, by=0.01), y=seq(-1, 1, by=0.01)) %>%
  expand(x, y) %>%
  mutate(scale_rf = scrat_f(x, y)) %>%
  ggplot(aes(x, y, fill=scale_rf)) +
  geom_raster() +
  scale_fill_viridis_c() +
  coord_fixed() +
  labs(x=latex2exp::TeX("$\\alpha$"),
       y=latex2exp::TeX("$\\beta$"),
       fill=latex2exp::TeX("$\\phi^{(2)}_{ij}$ (weight)")) +
  theme(legend.position="bottom")
```

This parameterization shows that WEAT constructs a weighted sum of cosine similarities between (e.g.) $\texttt{Harry} - \texttt{Jamel}$ and $\texttt{peace} - \texttt{sickness}$. The relative magnitudes of these difference vectors control the amount of relative scale bias in the result. The weight function tends to place higher weight on uneven comparisons (relative to even comparisons) and comparisons where both pairs of terms are more dissimilar. It will tend to upweight comparisons where the ratio of the absolute angular components $\theta_{A_iB_i}$ and $\theta_{X_jY_j}$ is larger (i.e. comparisons when one of the pairs is more similar than the other pair and thus has a shorter difference vector). In order to execute its helpful scale-overlooking trick, the component cosine similarities have pre-normalized the component vectors $A_i, B_i, X_j, Y_j$. Contrary to our intuitions, this trick does not *remove* scale information from the statistic; rather, the statistic must *contain* the scale information needed to project the vector subspace onto the unit ball. We can interpret the cosine ratio as a measure of alignment, but we cannot then interpret the mean cosine ratio as a measure of average alignment, unless we are willing to think of relative scale as meaningful. This property of the statistic is not dependent on the model used to generate the word vectors, but originates from the choice of comparison operator and the way this methodology weights the similarity measures.

WEAT is built on top of the fixed word lists in Greenwald et al. (1998) and Bertrand and Mullainathan (2004) (see Caliskan, Bryson & Narayanan 2017, supplemental material; Antoniak & Mimno 2021; van Loon et al. 2022). This implies that the problem has fixed geometry conditional on the choice of embedding. Here, I describe the shape of the "WEAT 3" test comparing examples of terms with pleasant and unpleasant connotations to examples of Black/white American first names:

\begingroup
\small

* **Black names**: Alonzo, Jamel, Theo, Alphonse, Jerome, Leroy, Torrance, Darnell, Lamar, Lionel, Tyree, Deion, Lamont, Malik, Terrence, Tyrone, Lavon, Marcellus, Wardell, Nichelle, Shereen, Ebony, Latisha, Shaniqua, Jasmine, Tanisha, Tia, Lakisha, Latoya, Yolanda, Malika, Yvette.
* **White names**: Adam, Harry, Josh, Roger, Alan, Frank, Justin, Ryan, Andrew, Jack, Matthew, Stephen, Brad, Greg, Paul, Jonathan, Peter, Amanda, Courtney, Heather, Melanie, Katie, Betsy, Donna, Kristin, Nancy, Stephanie, Ellen, Lauren, Colleen, Emily, Megan, Rachel.
  
* **Pleasant terms**: caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family, happy, laughter, paradise, vacation.
* **Unpleasant terms**: abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink, assault, disaster, hatred, pollute, tragedy, bomb, divorce, jail, poverty, ugly, cancer, evil, kill, rotten, vomit.

\endgroup

The predicted geometric relationship can be shown mathematically in any vector space word embedding model (i.e. "classic" word embeddings like GloVe and word2vec) that supports the WEAT lexicon. I use the pretrained GloVe embeddings (Pennington, Socher & Manning 2014) available in R through the \texttt{textdata} package (Hvitfeldt & Silge 2020) to describe the geometry of the three sums described in Appendix A taken over random sets of GloVe vectors. To describe the differing shapes of these vector subspaces, I take the singular value decomposition of the matrices $\textbf{A}$ and $\textbf{B}$, resulting in a set of singular vectors of rank less than or equal to the number of terms in the set. The resulting singular values describe the overall shape of the linear subspace spanned by each set of vectors (see Bai & Silverstein 2010). The proportion of variance contained in each dimension across pairs of vector sets describes the difference in shape.

```{r, eval=F, echo=F, message=F, fig.height=4, fig.width=4.5, fig.align="center"}
# Arbitrary word lists select totally different parts of the word space
# e.g in WEAT-GloVe, only 3 of the white names are outside of the top 5% of term frequencies, 
#  and only 7 of the black names are within this threshold

# The white names are entirely inside the frequency rank distribution of the good and the bad terms
# The black names are 56.25% (18/32) inside the bad term frequency rank distribution and 68.75% (22/32)
#  inside the good term frequency rank distribution. However, if the highly outlying term "caress" is
#  withheld, this falls to 12% (4/32), and the white name overlap falls to 84.375% (27/32).

library(tidyverse)
library(textdata)
library(gridExtra)

theme_set(theme_bw())

glove_d <- embedding_glove6b(dimensions=200)
glove <- glove_d %>% pivot_longer(contains("d"), names_to="dim")
glove_tokens <- unique(glove$token)
glove_d %<>% select(-token) %>% as.matrix()
rownames(glove_d) <- glove_tokens

# WEAT black/white names and good/bad terms
weat_white <- c("adam", "harry", "josh", "roger", "alan", "frank", "justin", "ryan", "andrew", "jack",
                "matthew", "stephen", "brad", "greg", "paul", "jonathan", "peter", "amanda", "courtney",
                "heather", "melanie", "katie", "betsy", "kristin", "nancy", "stephanie", "ellen", "lauren",
                "colleen", "emily", "megan", "rachel")
weat_black <- c("alonzo", "jamel", "theo", "alphonse", "jerome", "leroy", "torrance", "darnell", "lamar",
                "lionel", "tyree", "deion", "lamont", "malik", "terrence", "tyrone", "lavon", "marcellus",
                "wardell", "nichelle", "shereen", "ebony", "latisha", "shaniqua", "jasmine", "tanisha",
                "tia", "lakisha", "latoya", "yolanda", "malika", "yvette")
weat_good <- c("caress", "freedom", "health", "love", "peace", "cheer", "friend", "heaven", "loyal", "pleasure",
               "diamond", "gentle", "honest", "lucky", "rainbow", "diploma", "gift", "honor", "miracle", "sunrise",
               "family", "happy", "laughter", "paradise", "vacation")
weat_bad <- c("abuse", "crash", "filth", "murder", "sickness", "accident", "death", "grief", "poison", "stink",
              "assault", "disaster", "hatred", "pollute", "tragedy", "bomb", "divorce", "jail", "poverty", "ugly",
              "cancer", "evil", "kill", "rotten", "vomit")

# GloVe vectors for WEAT term lists
# Scaled as original (pre-trained)
weat_white_glove <- glove_d[c(weat_white),]
weat_black_glove <- glove_d[c(weat_black),]
weat_good_glove <- glove_d[c(weat_good),]
weat_bad_glove <- glove_d[c(weat_bad),]

# Get GloVe matrices
weat_white_glove <- scale(glove_d[c(weat_white),])
weat_black_glove <- scale(glove_d[c(weat_black),])
weat_good_glove <- scale(glove_d[c(weat_good),])
weat_bad_glove <- scale(glove_d[c(weat_bad),])

# Randomize the order
weat_white_glove <- weat_white_glove[sample(1:nrow(weat_white_glove)),]
weat_black_glove <- weat_black_glove[sample(1:nrow(weat_black_glove)),]
weat_good_glove <- weat_good_glove[sample(1:nrow(weat_good_glove)),]
weat_bad_glove <- weat_white_glove[sample(1:nrow(weat_bad_glove)),]

# Compute SVD to get linear subspaces
# All 4 have different characteristic polynomials
# Scaling the vectors onto the unit ball first does not solve this problem!!
wwg_basis <- svd(weat_white_glove)
wbg_basis <- svd(weat_black_glove)
good_basis <- svd(weat_good_glove)
bad_basis <- svd(weat_bad_glove)

# Compute the matrix that rotates each of these hyperellipses onto the unit ball
wwg_vb <- varimax(wwg_basis$u)
wbg_vb <- varimax(wbg_basis$u)
good_vb <- varimax(good_basis$u)
bad_vb <- varimax(bad_basis$u)

# It can be shown that these share a linear basis, but the eigenvalues are different
# See Krzanowski (1979), "Between-groups comparison of principal components", JASA 74(367): 703-707.
# Compute S matrix (embed in common space); Tr(S) ranges from 0 (orthogonal spaces) to k (coincident spaces)
wg_wb_s <- wwg_basis$u %*% t(wbg_basis$u) %*% wbg_basis$u %*% t(wwg_basis$u)
wbs_basis <- svd(wg_wb_s)
# sum(wbs_basis$d)

# First look at the shape of the name space
# Note the white names more heavily weight the larger eigenvalues
name_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="White names"),
                 data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Black names"))
name_ev %>%
  ggplot(aes(x, y, color=group)) +
  geom_point() +
  geom_line() + 
  labs(x="Eigenvector",
       y="Eigenvalue",
       title="Shape of name subspaces")

# The ratio of eigenvalues in both groups scaled as a proportion of variance provides a sense for
#  how much addition in this domain is scaled by the contribution of each example to the structure of
#  the linear subspace spanned by each set of names (i.e. how differently the A and B subspaces weight each dimension)
# An intuitive way of understanding this is how much you have to stretch or compress your unit ball
# map in each direction to get from one group of words to the other set of words while preserving the meaning of "addition" 
# Both of these elliptical regions can be projected onto the same linear subspace, but they are different projections
d_prop_name <- (wwg_basis$d / sum(wwg_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))
data.frame(x=1:length(wwg_basis$d),
           y=(wwg_basis$d / sum(wwg_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
  ggplot(aes(x, y)) +
  geom_hline(yintercept = 1, linetype="dashed") +
  geom_point() +
  labs(x="Eigenvector",
       y="Eigenvector ratio",
       title="Ratio of variance explained by \n dimension between name spaces")

# The ratio of the maximum D_prop to the minimum D_prop describes the maximum distortion
# when this ratio is 0.5, the eigenvector corresponding to the least influential exemplary pair is half as long
# as the eigenvector for the most influential exemplary pair.
# The scale of the most elliptical 2D cross-section of the hyperellipse
# min(d_prop_name) / max(d_prop_name)

# Now look at the attribute space
# The attribute space is relatively less distorted/stretched than the higher-dimensional name space
attr_ev <- rbind(data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Good words"),
                 data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Bad words"))
attr_ev %>%
  ggplot(aes(x, y, color=group)) +
  geom_point() +
  geom_line() + 
  labs(x="Eigenvector",
       y="Eigenvalue",
       title="Shape of attribute subspaces")

# Plot ratio of variance explained by eigenvalue
d_prop_attr <- (good_basis$d / sum(good_basis$d)) / (bad_basis$d / sum(bad_basis$d))
data.frame(x=1:length(good_basis$d),
           y=(good_basis$d / sum(good_basis$d)) / (bad_basis$d / sum(bad_basis$d))) %>%
  ggplot(aes(x, y)) +
  geom_hline(yintercept = 1, linetype="dashed") +
  geom_point() +
  labs(x="Eigenvector",
       y="Eigenvalue ratio",
       title="Ratio of variance explained by \n dimension between attribute spaces")

# Maximum distortion factor
# min(d_prop_attr) / max(d_prop_attr)
```

```{r, eval=F, echo=F, message=F, fig.height=4, fig.width=4.5, fig.align="center"}
# SIDEBAR: Uneven term set sizes as a source of bias in the WEAT
# Now let's look at the Krzanowski trace statistics across these comparisons
# To do this we take the top k dimensions of the larger hyperellipse
# The statistic is less than full rank (i.e. the subspaces are not in the same linear basis!)
# minrank <- 25
# ktest1 <- wwg_basis$u[1:minrank,1:minrank] %*% t(good_basis$u) %*% good_basis$u %*% t(wwg_basis$u[1:minrank,1:minrank])
# ktest2 <- wwg_basis$u[1:minrank,1:minrank] %*% t(bad_basis$u) %*% bad_basis$u %*% t(wwg_basis$u[1:minrank,1:minrank])
# ktest3 <- wbg_basis$u[1:minrank,1:minrank] %*% t(good_basis$u) %*% good_basis$u %*% t(wbg_basis$u[1:minrank,1:minrank])
# ktest4 <- wbg_basis$u[1:minrank,1:minrank] %*% t(bad_basis$u) %*% bad_basis$u %*% t(wbg_basis$u[1:minrank,1:minrank])
# sum(svd(ktest1)$d) / minrank
# sum(svd(ktest2)$d) / minrank
# sum(svd(ktest3)$d) / minrank
# sum(svd(ktest4)$d) / minrank

# The resulting addition metrics are only about 78% similar unless we equalize
# the size of the groups (net of randomization). So one way of screwing up is
# to use unequal term group sizes, i.e. to cram this into the lower-dimensional
# space defined by the attribute words.
```

```{r, eval=F, echo=F, message=F, fig.height=6.5, fig.width=7, fig.align="center", fig.cap="The name and attribute subspaces do not have the same shape."}
# We now know how to get from one basis to another basis
# Now compare what happens when you subtract the hyperellipses with and without putting them in the same basis

# Swap out the white-black names for a subsample of the space in the same dimension as the good-bad terms
weat_white_glove_s <- weat_white_glove[sample(1:nrow(weat_white_glove), 25),]
weat_black_glove_s <- weat_black_glove[sample(1:nrow(weat_white_glove), 25),]
wwg_basis <- svd(weat_white_glove_s)
wbg_basis <- svd(weat_black_glove_s)
wwg_vb <- varimax(wwg_basis$u)
wbg_vb <- varimax(wbg_basis$u)

# Now compare the name-to-attribute spaces
white_good_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="Names (white)"),
                       data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Attributes (good)"))
white_bad_ev <- rbind(data.frame(x=1:length(wwg_basis$d), y=wwg_basis$d, group="Names (white)"),
                       data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Attributes (bad)"))
black_good_ev <- rbind(data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Names (Black)"),
                       data.frame(x=1:length(good_basis$d), y=good_basis$d, group="Attributes (good)"))
black_bad_ev <- rbind(data.frame(x=1:length(wbg_basis$d), y=wbg_basis$d, group="Names (Black)"),
                       data.frame(x=1:length(bad_basis$d), y=bad_basis$d, group="Attributes (bad)"))

grid.arrange(
  white_good_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"), 
  white_bad_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"), 
  black_good_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"),
  black_bad_ev %>%
    ggplot(aes(x, y, color=group)) +
    geom_point() +
    geom_line() +
    theme(legend.position="bottom"),
  ncol=2
)

# white_good_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# white_bad_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# black_good_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()
# 
# black_bad_ev %>%
#   ggplot(aes(x, y, color=group)) +
#   geom_point() +
#   geom_line()

# Look at how these functions "cross"
black_good_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (Black)` > `Attributes (good)`) -> bgood_x
black_bad_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (Black)` > `Attributes (bad)`) -> bbad_x
white_good_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (white)` > `Attributes (good)`) -> wgood_x
white_bad_ev %>% pivot_wider(names_from=group, values_from=y) %>% mutate(`Names (white)` > `Attributes (bad)`) -> wbad_x
```

```{r, eval=F, echo=F, message=F, fig.height=6.5, fig.width=7, fig.align="center", fig.cap="Ratio of variance explained by eigenvector across paired name-attribute subspaces."}
# Plot ratio of variance explained by eigenvalues in each pair of subspaces
# These are the implicit weights on each dimension of "addition" we perform by adding across
#  these spaces without performing the necessary change of basis
# Observe that the key differentiating factor is the first two principal components (the size dimension)
grid.arrange(data.frame(x=1:length(good_basis$d),
                       y=(good_basis$d / sum(good_basis$d)) / (wwg_basis$d / sum(wwg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Good/White"),
            data.frame(x=1:length(bad_basis$d),
                       y=(bad_basis$d / sum(bad_basis$d)) / (wwg_basis$d / sum(wwg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Bad/White"),
            data.frame(x=1:length(good_basis$d),
                       y=(good_basis$d / sum(good_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                   y="Good/Black"),
            data.frame(x=1:length(bad_basis$d),
                       y=(bad_basis$d / sum(bad_basis$d)) / (wbg_basis$d / sum(wbg_basis$d))) %>%
              ggplot(aes(x, y)) +
              geom_hline(yintercept = 1, linetype="dashed") +
              geom_point() +
              labs(x="Eigenvector (exemplar)",
                     y="Bad/Black"), ncol=2)
```

# Appendix C: Sampling distribution of cosine similarity

The sampling distribution of the cosine similarity of a set of random vectors is non-normal (it is positively skewed); see Fisher (1928) and Hotelling (1953) for a general discussion in the context of the Pearson product-moment correlation coefficient. Figure \ref{fig:kmeandist} displays the empirical distribution of component cosine similarities for each of the three arithmetic comparison strategies over random vector subsets of varying size $k$. Figure \ref{fig:imbsim} shows the behavior of the two-way sum when the two group sizes are imbalanced; the other two sums are computed over only one index. Depending on the underlying sum and the vectors we happen to draw, the distribution may look approximately normal, but it is always biased upward from zero, and this bias is non-monotonic in $k$.

```{r kmeandist, echo=F, message=F, fig.height=6, out.width="\\textwidth", fig.align="center", fig.cap="Sum distributions for varying choices of k."}
# Number of vectors in each subspace (assuming equal sized groups)
# When k is lower, the skew is worse; fixed word lists have observably bad geometry
# It's also interesting that the distribution is a function of the amount of data we have.
plot_cs_distributions <- function(k1, k2, rmin=1, rmax=nrow(embm)) {
  sA <- sample(etokens[rmin:rmax], k1)
  sB <- sample(etokens[rmin:rmax], k2)
  
  expand_grid(i=1:k1, j=1:k2) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           biterm = sA[j],
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,]),
           abi_cs = lsa::cosine(embm[aterm,], embm[biterm,])) ->
    random_angles

  # Look at distribution of full graph weight set comparing two vector subspaces
  random_angles %>%
    ggplot(aes(x=ab_cs)) +
    geom_density() +
    geom_vline(aes(xintercept=mean(ab_cs)), linetype="dashed") +
    xlim(-1, 1) +
    labs(x="cos(a, b)",
         title=paste0("1. Two-way sum (", k1, ", ", k2, ")"))  ->
    ra_p1

  if(k1 != k2) {
    ra_p1  # Only show the two-way sum for rectangular comparison graphs
  } else {
    # Distribution of cosine similarities taken externally pairwise.
    # A and B are different.
    random_angles %>%
      filter(i == j) %>%
      ggplot(aes(x=ab_cs)) +
      geom_density() +
      geom_vline(aes(xintercept=mean(ab_cs)), linetype="dashed") +
      xlim(-1, 1) +
      labs(x="cos(a, b)",
           title=paste0("2. Diagonal sum (", k1, ", ", k2, ")")) ->
      ra_p2
  
    # Distribution of cosine similarities taken internally pairwise.
    # A and B are equal.
    random_angles %>%
      filter(i > j) %>%
      ggplot(aes(x=abi_cs)) +
      geom_density() +
      geom_vline(aes(xintercept=mean(abi_cs)), linetype="dashed") +
      xlim(-1, 1) +
      labs(x="cos(a, b)",
           title=paste0("3. Off-diagonal sum (", k1, ", ", k2, ")")) ->
      ra_p3
    
    return(arrangeGrob(grobs=list(ra_p1, ra_p2, ra_p3), ncol=3, nrow=1))
  }
}

# Plot sum distributions for a range of choices of k1, k2
# Larger values take quite a bit longer.
grid.arrange(plot_cs_distributions(k1=10, k2=10),
             plot_cs_distributions(k1=25, k2=25),
             plot_cs_distributions(k1=50, k2=50),
             ncol=1)
```

```{r imbsim, echo=F, message=F, fig.height=5, out.width="\\textwidth", fig.align="center", fig.cap="Sum distributions for imbalanced choices of k."}
# Imbalanced comparisons
grid.arrange(plot_cs_distributions(k1=20, k2=10), plot_cs_distributions(k1=30, k2=10),
             plot_cs_distributions(k1=50, k2=25), plot_cs_distributions(k1=50, k2=10),
             ncol=2)
```



# Appendix D: Relative scale and the subspace dimension of $D$

Additionally, the principal geometry of the parallelograms is not uniformly distributed over the underlying vector space; in particular, their alignment with the global term frequency dimension (the largest singular vector of S) is non-uniform.

Focus on what is happening when you add two cosine similarities computed in R300. Where is the information in these comparisons coming from — how does each basis weight the dimensions in GloVe differently? What does this look like when you look at the principal geometry of the added/subtracted spaces?

The size of the comparison vector sets matters for the geometry of the comparison. There is a loss of information involved in transforming $\textbf{A}$ and $\textbf{B}$ onto $\textbf{X}$ and $\textbf{Y}$ when they are embedded in fewer dimensions. The Krzanowski (1979) statistic suggests that the linear subspaces created by the unequal term group sizes in WEAT 3 are only about 78% similar. I set this fact aside and work with word lists of the same length for the remainder of the paper; however, because nothing about the construction of the word lists guarantees their basis is full rank, it is important to note that this only minimizes the issue and does not solve it completely. All of the spaces are isomorphic when the underlying word lists are the same length; this can be confirmed by checking that the common space embedding trace metric developed by Krzanowski (1979) attains its upper bound (i.e. the number of terms). However, they will in general have different eigenvalues, suggesting each has a subtly different overall shape.

Fisher (1928) introduced an inverse hyperbolic tangent transformation to address this issue in the context of the sampling distribution of the Pearson product-moment correlation coefficient (also see Hotelling 1953).

$$\tanh^{-1}(x, y) = \frac{1}{2}\ln\left(\frac{1+\cos(x, y)}{1-\cos(x, y)}\right)$$  
The behavior of the function is approximately linear for correlations in $[-0.5, 0.5]$, but more extreme associations (positive or negative) are allowed to expand nonlinearly and increase without bound as the cosine similarity approaches -1 or 1. The relative scale bias in sum or average over fixed vector sets can be described 

TODO:

* To do this, look at the eigenanalysis of each subspace.
    + Are the max-min ratios different?
    + What is the relationship to the distribution of cosine similarities? The range of the distribution matters for the comparison of any two angles
* Look at loadings on cosine similarity
    + cos(theta) is actually cos(x, y)
    + x and y are vectors in R300; cos(x, y) is a scalar in R bounded on -1, 1
    + for any two vectors x, y, cos(x, y) represents some dimensions in R300 more than others. some are potentially zero (this is why we use cosine; see Salton & McGill 1986).
    + there is a vector of weights that describes how each dimension loads onto this particular value of the cosine similarity. these weights vary for any two arbitrary pairs of points. how similar they are is different depending on which dimension(s) you use to observe them. the sense in which cos(x,y) describes an angle is dependent on this underlying geometry.
    + adding cosine similarities arithmetically results in a reweighting over the dimensions of the original vector space. does this tend to upweight information in the top eigenvalue of GloVe in the statistic?

```{r randang_dual, eval=F, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center"}
# First, look at the local anisotropy ratio
abfact <- function(aterm, bterm) {
  dc <- svd(rbind(embm[aterm,], embm[bterm,]))
  return(data.frame(sv1=dc$d[1], sv2=dc$d[2]))
}

random_angles_100 %>%
  distinct(aterm, bterm) %>%
  rowwise() %>%
  do(abfact(.$aterm, .$bterm)) %>%
  cbind(random_angles_100, .) %>%
  rowwise() %>%
  mutate(plus_dglob = norm(embm[aterm,] + embm[bterm,], "2")) ->
  ra100_lsvd

# Plot cosine similarity against the local norm product weight, color by isotropy ratio
# Look at each test term perspective separately (termwise anisotropy distribution)
# These can all be done with random angles too; fixed terms just bound the rarity window in a weird way?
ra100_lsvd %>%
  ggplot(aes(x=nprod, y=ab_cs, color=sv2/sv1, size=lfr)) +
  geom_point() +
  scale_color_viridis_c() +
  # theme(legend.position="none") +
  facet_wrap(~aterm)

# All term perspectives superimposed
# This is just a different way of doing the polarization identity plot
ra100_lsvd %>%
  ggplot(aes(x=anorm * bnorm, y=ab_cs, color=sv2/sv1, size=lfr)) +
  geom_point() +
  scale_color_viridis_c() +
  theme(legend.position="bottom")

# # Less informative view of the same surface
# ra100_lsvd %>%
#   ggplot(aes(x=anorm * bnorm, y=sv2/sv1, color=ab_cs)) +
#   geom_point() +
#   scale_color_viridis_c() +
#   # theme(legend.position="none") +
#   facet_wrap(~aterm)
# 
# ra100_lsvd %>%
#   ggplot(aes(color=anorm * bnorm, x=sv1*sv2, y=ab_cs)) +
#   geom_point() +
#   scale_color_viridis_c() +
#   # theme(legend.position="none") +
#   facet_wrap(~aterm)

# By component
ra100_lsvd %>% ggplot(aes(x=sv1, y=plus_dglob, color=ab_cs)) + geom_point() + scale_color_viridis_c()
ra100_lsvd %>% ggplot(aes(x=sv2, y=plus_dglob, color=ab_cs)) + geom_point() + scale_color_viridis_c()

# Add pairs of cosine similarities between AB (k vectors each) and CD (k vectors each)
# Get 2 sets of random angle projections
k <- 30
random_angles_30a <- make_angles(k)
random_angles_30a %>%
  distinct(aterm, bterm) %>%
  rowwise() %>%
  do(abfact(.$aterm, .$bterm)) %>%
  cbind(random_angles_30a, .) %>%
  rowwise() %>%
  mutate(plus_dglob = norm(glove_d[aterm,] + glove_d[bterm,], "2")) ->
  ra30a_lsvd
random_angles_30b <- make_angles(k)
random_angles_30b %>%
  distinct(aterm, bterm) %>%
  rowwise() %>%
  do(abfact(.$aterm, .$bterm)) %>%
  cbind(random_angles_30b, .) %>%
  rowwise() %>%
  mutate(plus_dglob = norm(glove_d[aterm,] + glove_d[bterm,], "2")) ->
  ra30b_lsvd

# Compute the pairwise Krzanowski common space embedding trace statistic between pairs
# This tells us how similar the spaces we're adding are
# t1 and t2 sets can be of arbitrary size
krz_trace <- function(t1, t2, t3, t4) {
  d1 <- prcomp(glove_d[t1,])$rotation
  d2 <- prcomp(glove_d[t2,])$rotation
  
  krztest <- t(d1) %*% d2 %*% t(d2) %*% d1
  return(sum(svd(krztest)$d))
}

# Pairwise comparison
# Arithmetic cosine sum/mean/difference
# Isotropy ratio
# Component norms
# Sum vector norm and norm sum vector norm
expand.grid(ra=1:100, rb=1:100) %>%
  rowwise() %>%
  mutate(cs1=ra30a_lsvd[ra,]$ab_cs,
         cs2=ra30b_lsvd[rb,]$ab_cs,
         t1a=ra30a_lsvd[ra,]$aterm,
         t1b=ra30a_lsvd[ra,]$bterm,
         t2a=ra30b_lsvd[rb,]$aterm,
         t2b=ra30b_lsvd[rb,]$bterm,
         t1anorm=ra30a_lsvd[ra,]$anorm,
         t1bnorm=ra30a_lsvd[ra,]$bnorm,
         t2anorm=ra30b_lsvd[rb,]$anorm,
         t2bnorm=ra30b_lsvd[rb,]$bnorm,
         ra_sv1=ra30a_lsvd[ra,]$sv1,
         ra_sv2=ra30a_lsvd[ra,]$sv2,
         cssum = ra30a_lsvd[ra,]$ab_cs + ra30b_lsvd[rb,]$ab_cs,
         csdiff = ra30a_lsvd[ra,]$ab_cs - ra30b_lsvd[rb,]$ab_cs,
         csmean = mean(c(ra30a_lsvd[ra,]$ab_cs, ra30b_lsvd[rb,]$ab_cs)),
         ktr = krz_trace(c(t1a, t1b), c(t2a, t2b))) ->
  simil_arith

simil_arith %>%
  ggplot(aes(y=(t1anorm * t1bnorm) / (t2anorm * t2bnorm), x=ktr)) +
  geom_point() +
  geom_smooth()

# Look at PC alignment of really coincident ones
data.frame(t(rbind(x=svloadings("thiolate", "promptly")[1,],
                   y=svloadings("gtpase-activating", "decent")[1,]))) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth()


data.frame(t(rbind(x=svloadings("4,138", "kamayut")[1,],
                   y=svloadings("mazzaro", "o.t.o.")[1,]))) %>%
  mutate(pc = row_number()) %>%
  ggplot(aes(x, y, label=pc)) +
  geom_text() +
  geom_smooth()

# Look at loadings vector for each pair (big matrix!)
svloadings <- function(t1, t2) {
  return(cbind.data.frame(t(prcomp(rbind(glove_d[t1,], glove_d[t2,]))$rotation)))
}

kt <- 30
random_angles_100 %>%
  ungroup() %>%
  filter(i <= kt & j <= kt) %>%
  mutate(ith=row_number()) %>%
  rowwise() %>%
  do(svloadings(.$aterm, .$bterm)) ->
  test
test2 <- cbind(random_angles_100 %>% filter(i <= kt & j <= kt) %>% .[rep(1:kt^2, each = 2), ],
               oidx=rep(1:kt^2, each = 2), pcidx = rep(1:2, times=kt^2), test)

test2 %>%
  filter(pcidx == 1) %>%
  pivot_longer(starts_with("d")) ->
  test3
test3 %>%
  ggplot(aes(x=oidx, y=name, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  scale_y_discrete(labels=unique(test3$name)) ->
  ra100_pc1_loadings
ra100_pc1_loadings
test2 %>%
  filter(pcidx == 2) %>%
  pivot_longer(starts_with("d")) ->
  test3
test3 %>%
  ggplot(aes(x=oidx, y=name, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  scale_y_discrete(labels=unique(test3$name)) ->
  ra100_pc2_loadings
ra100_pc2_loadings
grid.arrange(ra100_pc1_loadings, ra100_pc2_loadings, ncol=2)

#
```

```{r manifold_bias, eval=F}
# Arrange the vector pairs in forward and reverse similarity order
# Then compute maximally out of scale difference/sum/mean
# Look at relationship to local norm product
weat_lsvd %>%
  arrange(desc(ab_cs)) ->
  heads
weat_lsvd %>%
  arrange(ab_cs) ->
  tails
data.frame(csdiff = heads$ab_cs - tails$ab_cs,
           cssum = heads$ab_cs + tails$ab_cs,
           head_np = heads$anorm * heads$bnorm,
           tail_np = tails$anorm * tails$bnorm,
           headst = paste0(heads$aterm, heads$bterm),
           tailst = paste0(tails$aterm, tails$bterm)) ->
  headstails

headstails %>%
  mutate(csmean = cssum/2,
         npr = head_np * tail_np) %>%
  group_by(grp = paste(pmax(headst, tailst), pmin(headst, tailst), sep = "_")) %>%
  slice_sample(n=1) %>%
  ungroup() ->
  test
test %>%
  ggplot(aes(x=head_np, y=tail_np, color=csmean, size=csmean, alpha=csmean)) +
  geom_point() +
  scale_color_viridis_c() +
  coord_fixed()
test %>%
  ggplot(aes(x=head_np, y=tail_np, color=abs(csdiff), size=abs(csdiff), alpha=abs(csdiff))) +
  geom_point() +
  scale_color_viridis_c() +
  coord_fixed()


# Basically there's a big group of these that looks pretty random, but as the mean gets bigger, it looks less so
# So this distribution isn't uniform, particularly for low values of the maximally nonlocal local norm product ratio
# TODO: What happens if instead of arranging them like this, you arrange them randomly? Does it ever matter?
# TODO: Look at this but with random vectors and the right similarity component substructure.
# TODO: How exactly are the local norm products related?
test %>% ggplot(aes(x=npr, y=csmean)) + geom_point() + geom_smooth(method="loess")

# The problem has two parts that are hard to disentangle:
#   1. The model has been intentionally constrained to have this geometry, but
#   2. Arithmetic comparison strategies do not take this into account.
# The way that the model does this is predictable, so when we compare similarities arithmetically, we bias
# the result in a predictable way: we link "similar" to "relatively small". The reason why this happens is that
# the vector addition and vector alignment operations become less related over more distant positions
# in the inner product space.

# TODO: Compare subspace overlap of D(original scale, +) to D(locally normed scale, +)
```

# Extra anisotropy schematics

```{r basis_distortion, eval=F, echo=F, out.width="\\textwidth", fig.cap="Various ways of distorting the unit ball in $\\mathbb{R}^3$."}
knitr::include_graphics("./basis_distortion.png")
```

```{r basis_change, eval=F, echo=F, out.width="\\textwidth", fig.cap="Each hyperellipsoid can be transformed into the unit ball using a different change of basis."}
knitr::include_graphics("./basis_change.png")
```

# A short note on embedding lexicons

I use the Wikipedia 2014 and Gigaword 5 GloVe embeddings as an example throughout the paper. The pretrained Twitter and Common Crawl embeddings are perhaps more commonly used when pretrained embeddings are employed. However, the expanded term set in these larger embeddings does not necessarily add much information of substantive interest to social science researchers over the scope of Wikipedia and newswire text. In practice, researchers rely on few fixed lists of approximately 20-50 relatively common unigrams (Antoniak & Mimno 2018), so few studies employ this extended lexicon except incidentally.

It is very easy to understate the depths of unusualness in the tail lexicon of large-scale word embeddings. For example, Piantadosi (2014) gives the triplet "accordion", "catamaran", "ravioli" as examples of low-frequency words, but all three are in the top 15% of terms by frequency in GloVe. I think it is genuinely unclear whether and when unigrams like "rutherfordium" (the element with atomic number 104), "apochromat" (a type of lens that corrects for distortions of color), "qantas.com" (originally an acronym for Queensland and Northern Territory Aerial Services), or "6dogs9cats" (a Tumblr user) is relevant to measures of term-associative meaning.

Findings are not especially sensitive to the choice of embeddings. The most important factor is the choice of terms, which determines the frequency distribution  Also note that the phenomenon of relative scale bias is a property of cosine arithmetic in vector spaces and not the inner product distribution of word embeddings per se (Mimno & Thompson 2017). The exact bias for fixed word lists varies between models and datasets.