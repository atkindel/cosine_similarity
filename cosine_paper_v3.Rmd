---
title: "Cosine similarity and frequency bias in word embeddings"
author: "Alexander T. Kindel^[PhD Candidate, Department of Sociology, Princeton University. Contact: akindel@princeton.edu.]"
abstract: |
 |  The cosine of the angle between two *p*-dimensional word vectors is widely used as a measure of correlation in statistical text analysis. In applications of neural word embedding models (e.g. word2vec, GloVe) to problems in the social sciences, a common methodology for cosine similarity-based comparison involves performing an arithmetic operation (e.g. difference in means) over cosines of linear combinations of word vectors. This approach results in measures that are biased in proportion to the underlying word frequency distribution. Frequency bias results from the fact that cosine similarity implies a non-Euclidean transformation of the inner product space that distorts rare word vector pairs toward the global mean vector. The distribution of cosine similarity is heterogeneous over the word frequency distribution, implying that arithmetic means of cosine similarities can lead to overestimates or underestimates of the targeted relationship in a regression analysis setting. There is not one correct way to weight the inner product; different similarity schemes reflect distinct attitudes toward the frequency-association relation.
 | 
 | **Keywords:** cosine similarity, frequency bias, word embeddings, measurement, arithmetic
date: |
  | 13 April 2022
  | 
  | 
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: TRUE
    number_sections: true
header-includes:
  - \usepackage{setspace}
  - \usepackage{enumitem}
  - \usepackage{epigraph}
  - \usepackage{bbm}
  - \setlength\epigraphwidth{0.9\textwidth}
  - \setlength\epigraphrule{0pt}
  - \renewcommand{\textflush}{flushepinormal}
  - \renewcommand{\epigraphflush}{center}
  - \setlist{listparindent=\parindent, parsep=0pt}
  - \doublespacing
  - \setlength\parindent{24pt}
  - \setlength{\parskip}{0.5mm}
  - \usepackage[bottom]{footmisc}
---

```{r setup, echo=F, message=F}
library(tidyverse)
library(magrittr)
library(readtext)
library(quanteda)
library(quanteda.textstats)
library(text2vec)
library(word2vec)
library(ggpubr)
library(here)

# Document settings
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, cache.lazy=FALSE)
set.seed(2718281)

# Load COCA word frequency file
words_219k_m2138 <- read_delim(here("data", "words_219k_m2138.txt"),
                               "\t", escape_double = FALSE, trim_ws = TRUE, skip = 8)
```

\pagebreak

# Introduction

Word embedding models are an increasingly popular tool for computational research on language use in the social sciences. Researchers seek to use word embeddings to measure entities widely theorized in the study of culture previously considered difficult to quantify, such as discourse, ideology, sentiment, cognition, emotion, bias, stereotype, association, identity, experience, subjectivity, and diversity (among many other concepts; see Kozlowski, Taddy & Evans 2019). This paper addresses a point of connection between social theory and statistical evidence (Duncan 1984; Lundberg, Johnson & Stewart 2021) through a discussion of the mathematical structure of a common methodology for word embedding model-based analysis in the social sciences.

One emerging family of methods links word embeddings to theories of cultural processes by examining estimated associations between vectors in a structured analytic vocabulary of interest. The vocabulary consists of lists of words related to a concept of interest (e.g. class, gender, race, emotion) and represented by a set of vectors obtained from a word embedding model. When researchers aim to summarize overall differences in association between these concepts in the corpus, a common target quantity is the (arithmetic) mean cosine similarity over the word vectors or linear combinations of the word vectors, potentially conditional on some covariates:  
$$
\mathbb{E}[\cos\left(\sum_Av(a), \sum_Bv(b)\right)|X]
$$  
Researchers seeking to apply word embeddings in their analyses should be aware that this approach yields quantities that are frequency biased.^[The Word Embedding Association Test, a widely-used measure of this type, has been shown to be frequency biased in this way; see Ethayarajh, Duvenaud \& Hirst 2018 and van Loon et al. 2022.] In a regression analysis setting, the use of this quantity implies that estimated coefficients will tend to be distorted by the underlying word frequency distributions associated with the choice of inducing vector sets and linear combination operations.

I discuss a number of examples of frequency bias in the literature involving word-level and sentence-level arithmetic summaries of cosines. Along the way, I develop a geometric view of frequency bias that emphasizes the role of the normalization weight function needed to generate a similarity coefficient from the inner product space estimated by the word embedding model (Salton \& Buckley 1988; Caillez \& Kuntz 1996; Dominich 2001). Frequency bias is a type of shape distortion bias; it can either attenuate or amplify estimated arithmetic aggregates of cosines when these quantities are used in a regression framework. The direction of the bias depends on the particular comparative arithmetic strategy employed (i.e. the specific choice of compositions of focal words) as well as the relative location and variance of each word set within the overall word frequency distribution.

I suggest viewing \textit{similarity} as a separate weight estimation procedure that occurs after the estimation of vector space models (cf. Lowe 2001). Although modern word embedding models have been developed with particular semantic operations in mind (Levy \& Goldberg 2014), these operations are something like hypotheses about the structure of the parameter space of the model; they refer to this model, but they originate in the question we are asking by analyzing the data in this way. Thinking of similarity as a separate and optional transformation of the inner product space may help to clarify some confusions in the literature surrounding the use and interpretation of similarity coefficients. In particular, although the exact shape of semantic vector spaces is somewhat model dependent, in general log-bilinear word embedding models are designed to estimate frequency-varying vector norms in order to optimally encode the underlying word-word co-frequency data (Mnih & Hinton 2008; Andreas & Klein 2015; Arora et al. 2016, 2018). Consequently, there is no "neutral" choice of weights that results in a distance metric that would allow us to ignore word frequency.^[A uniform distribution of meaning over frequency is not a neutral choice of weights; arguably, it also does not result in a realistic model, because we should avoid the assumption that the level of generality/specificity in discourse is constant.] Similarities are heuristic projections of semantic vector spaces that explicitly express the researcher's preference for associations with common or rare words in terms of a second normalized inner product space. They provide an *intentionally* distorted perspective on the inner product space that is useful for some purposes and less for others. Another implication of viewing similarity as a weight estimation procedure is that researchers need not apply any one similarity coefficient in order to construct informative models of variation in meaning using word embeddings. Many potential weighting schemes might be appropriate for different types of question, to the extent that they help researchers explore relevant regions of the original semantic vector space.

The paper is organized as follows.^[I do not systematically analyze the implications of different embedding approaches (model architecture, training objectives, hyperparameter tuning, etc.) for answering questions in the social sciences. In general, applied researchers use one of three “classic” word embedding algorithms, either GloVe (Pennington, Socher & Manning 2014) or one of the word2vec models (CBOW and SGNS; Mikolov et al. 2013), so I focus on these applications. Model choice may imply different notions of meaning that are important relative to social science applications. I discuss these issues only briefly and selectively; readers are encouraged to consult a more comprehensive introduction to semantic vector space models (e.g. Jurafsky & Martin 2021, ch. 6; Grimmer, Roberts & Stewart 2022, ch. 7-8; Turney & Pantel 2010) than is possible to provide in this paper. I also do not devote much attention to the correspondence between theories of meaning and the empirical properties of vector spaces (i.e. whether analogies can appropriately describe the previously mentioned theoretical concepts, and if so, which ones and to what degree). Readers are directed to work by Kozlowski and colleagues (2019) and Arseniev-Koehler \& Foster (2020) for more detailed introductions to this important topic. Like prior work, I assume that operations in semantic vector spaces are reasonably analogous to the targeted cultural-associative processes and that standard embedding models are capable (in principle) of estimating vector spaces that adequately support descriptions of these associations.] Section 2 discusses a number of key results in the computational linguistic theory of classic word embedding models that relate the word frequency distribution to the mathematical structure of semantic vector spaces (i.e. the inner product and its corresponding norm). In section 3, I show how this dependence on the word frequency distribution distorts cosine similarity-based analyses of word embedding models, based on reanalysis of three studies spanning areas of social research where word embedding models have been employed (cultural sociology, management science, and political communication). Building on these results, section 4 discusses a few ways of thinking about the properties of cosine similarity as a method of discursive comparison in the social sciences. I conclude by discussing future directions and challenges for applying word embeddings in the social sciences.

# Cosine similarity and word frequency in word-word inner product spaces

Log-bilinear word embedding models like CBOW, SGNS, and GloVe can be interpreted as a family of low-rank approximations to latent semantic analysis (LSA; Deerwester et al. 1990; Mnih & Hinton 2008; Levy & Goldberg 2014; Arora et al. 2016). LSA involves taking the singular value decomposition of a large, sparse word frequency matrix; a key advantage of modern neural word embedding models is that they enable researchers to efficiently find dense low-rank approximations to this matrix with better bounds on memory and time requirements, facilitating data analysis with much larger amounts of information.^[This is an oversimplification; in practice there can be important differences in the way that each algorithm constructs word-word associations for a given problem, and the approximation error can be important.] The output of a word embedding model is a set of vectors embedded in a $p$-dimensional inner product space. This space is so-named because it supports the inner product operation $\left<v_w, v_c\right>$, which gives a measure of distance between any two word vectors. The inner product space occupied by classic word embedding vectors is a Euclidean vector space on $\mathbb{R}^p$, where the conventional inner product and norm are the dot product and the Euclidean norm respectively. The vectors estimated by classic word embedding models vary in scale (some are longer than others), so the inner product space is associated with a distribution of Euclidean norms.

Cosine similarity is often used as a measure of pairwise correlation between observations measured in this way. The quantity measures the alignment of two $p$-dimensional vectors $A$ and $B$ by taking the ratio of their inner product to the product of their norms:

$$cos(\theta_{AB}) = \frac {\left<A, B\right>}{||A||\;||B||} = \frac{\sum_i a_ib_i}{\sqrt{\sum_i a_i^2} \sqrt{\sum_i b_i^2}}$$  
The ratio describes the coplanar angle between the two vectors, and is usually interpreted as the amount that their directions are the same, different, or opposite. Due to the mathematical relationship between the inner product and the norm defined by the Cauchy-Schwarz inequality, $\left|\left<A, B\right>\right| \leq ||A||||B||$, the ratio is nominally bounded from -1 (pointing in opposite directions) to 0 (pointing in different/orthogonal directions) to 1 (pointing in the same direction). In practice the distribution of cosine similarity is positively skewed in modern log-bilinear word embeddings and it cannot be negative when all of the vector components are positive (Mimno & Thompson 2017; Mu & Viswanath 2018). It is helpful to think of cosine similarity as something separate from the inner product space, which has been estimated by an underlying embedding model. By taking this ratio, we are estimating another quantity that helps us see into the space we have estimated, particularly if our research goal is to locate vectors close to a vector we know we are interested in (e.g. for classification or query response).

The origins of cosine similarity's popularity lie in its use as a method of constructing ranked indices over collections of documents for the purposes of information retrieval; it has been taken up subsequently as a basis for *comparative* measures of similarity without much focus on whether it supports this alternative research goal. There are reasons to doubt that it does (Singhal, Salton & Buckley 1995; van Eck & Waltman 2009). In particular, the key motivation originally was that documents vary in length, so word co-frequency data tended to reflect document length more than was desirable without normalization (Salton \& Buckley 1988). But, in the word-word co-frequency setting, vector length no longer reflects document length because this has been fixed by the context window hyperparameter. The scale of the vectors is dominated instead by the relative marginal frequency of the corresponding word in the corpus, up to modeling choices that cause the effective dimensionality or context window to vary (Arora et al. 2016; Ethayarajh, Duvenaud \& Hirst 2018).^[See section 4.] Additionally, the typical application of the measure assumes that similarity coefficients can be safely ignored below a small number of top N matches because they are not of practical interest (i.e. we want the "best" match, not the full spectrum of matching to not matching; see Salton \& Buckley 1988; Levy, Goldberg \& Dagan 2014). The measurement of concepts with aggregate cosine similarity makes the exact opposite assumption by taking arithmetic sums and means over sets of word pair similarity estimates that vary systematically across the word frequency distribution.

The scale variation of word vectors is the primary modern justification for preferring cosine similarity as a measure of pairwise word association. For example, Grimmer, Roberts and Stewart (2022) motivate a discussion of cosine similarity by remarking that "[a] potential problem with the inner product is that its result depends on the magnitude of the two vectors." Jurafsky and Martin (2021) similarly introduce cosine similarity by writing that "[m]ore frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency." Turney and Pantel (2010) summarize the key advantage of cosine: "...the cosine of the angle between two vectors is the inner product of the vectors, after they have been normalized to unit length. ...Cosine captures the idea that the length of the vectors is irrelevant; the important thing is the angle between the vectors."

This leads to an obvious question: if the intent is to represent meaning, and we sometimes intuit that vector length should be irrelevant to meaning, then why do word embedding vectors vary in length at all?^[Salton and Buckley (1988) suggest that a presence-only quantity yields worse semantics than any word frequency weighted similarity metric. It is not possible to generate presence-only vectors that are low-rank without considerably coarsening the corresponding distance function.] Arora, Li, Liang, Ma, and Risteski (2016) propose a generative log-bilinear model for semantic vector spaces that models an inner product space on relative (log) word frequencies conditional on a random walk vector in the context word space (Theorem 2.2; also see Mnih \& Hinton 2008). Optimizing the proposed model implies finding a vector space that sets distances between word vectors in a way that best represents the conditional random walk frequency distribution for every position in the vocabulary. The model provides a number of clarifying insights regarding the behavior of word embedding models with respect to their word frequency distributions. I focus on the following implication. Formally, we have a set of $p$-dimensional word vectors estimated with window size $q$, and $w$ is any word observed in the corpus. We consider a set of randomly generated and smoothly varying context positions in the semantic vector space: $c \in C$. Then the following linear relationship holds (Corollary 2.3):  
$$
\begin{aligned}
\log p(w) &= \frac{||v_w||^2_2}{2p} - 2\ \text{LogSumExp}_{C,w}\left<v_w, c \right> + \log\left(\frac{q(q-1)}{2}\right) + \epsilon \\
\end{aligned}
$$
Schematically, this equation says that the log word frequency for any given word is a linear function of three quantities: its squared Euclidean norm scaled by the dimensionality of the vector space ($\frac{||v_w||^2_2}{2p}$); the approximate maximum distance^[For some set of scalars $X$, $\text{LogSumExp}(X)$ is a smooth approximation to $\max(X)$.] between this word vector and nearby contexts ($-2\ \text{LogSumExp}_{C,w}\left<v_w, c \right>$); and a constant reflecting the global scale of the frequency distribution induced by the window size ($\log\left(\frac{q(q-1)}{2}\right)$), plus an error term.

As mentioned above, an important implication of this result is that semantic vector space norms and their corresponding word frequencies in the underlying corpus are strongly related to each other (Ethayarajh, Duvenaud \& Hirst 2018). This implies that the cosine similarity between any two word vectors is partially a function of a ratio of their frequencies:  
$$
\begin{aligned}
\cos(w_A, w_B) &:= \frac{\left<w_A, w_B\right>}{||w_A||\ ||w_B||} \\
&\propto \frac{\left<w_A, w_B\right>}{\sqrt{\log p(w_A)} \ \sqrt{\log p(w_B)}} \\
\end{aligned}
$$  
The close relationship between cosine similarity and the word frequency distribution is paradoxical when we consider that the entire motivation for using cosine similarity in the first place is that it is typically understood to be a *solution* to the problem of scale dependence in vector space comparison. The confusion is principally about the role of the product of the Euclidean vector norms corresponding to a given pair of words. Due to its proportionality to the product of the corresponding root-log word frequencies, this quantity plays an important role in frequency bias. I refer to it as the *local normalization weight* (LNW) function throughout the paper. It is a local weight in the sense that $1/\text{LNW}(A,B)$ adjusts each point in the vector space in a way that is specific to the pair $\{A, B\}$. $\text{LNW}(A,B)$ describes how the transformation from the original inner product space inhabited by the word vectors to the normalized inner product space performed when applying cosine similarity varies at different locations in the underlying word frequency distribution.

A weight-based interpretation of cosine similarity clarifies the role of the ratio in producing frequency-biased quantities from word embedding models. Taking the cosine of two word vectors enables us to estimate how far apart they are adjusted by their respective positions in the underlying word frequency distribution. However, the way that cosine similarity accomplishes this transformation is not constant everywhere in the original inner product space. The local normalization weight function encodes a notion of similarity that prefers rare words: holding the inner product constant, cosine similarity increases as the product of the vector norms becomes small. Consequently, conventional arithmetic summaries of cosine similarities tend to show strong correlations with measures of word frequency, particularly when the set of cosine similarities is determined by a pairwise correlation structure, as is common in applications of word embeddings in the social sciences. The root of frequency bias is an implicit assumption that the local *within-pair* interpretation of cosine similarity extends to the global *between-pair* mathematical characteristics of arithmetic summaries of cosine similarities. In practice, the amount of distortion implied by cosine similarity varies from application to application; I turn next to discussing a few cases of frequency bias in applied settings before returning to  

# Examples of frequency bias in the literature

This section demonstrates frequency bias in a few applications of word embeddings to problems in the social sciences. The studies were selected because (1) they construct an aggregate cosine similarity quantity from locally estimated classic word embeddings; (2) they span a wide range of disciplinary perspectives; and (3) they have publicly accessible and comprehensively documented replication materials. Frequency bias is a property of this general approach and is not unique to these papers or research areas. The body of work discussed in this paper is part of a long-term effort to adapt tools designed for making optimal predictions to the task of developing theoretical explanations (Hofman et al. 2021). If our goal is to improve our ability to observe and record social variation (Xie 2013), aggregation errors may be inevitable. The findings reported in this paper are indebted to the willingness of a wide community of researchers to take risks and experiment with unfamiliar methods.

It is important for researchers to notice that frequency bias can lead to results that *simultaneously understate and overstate* the substantive degree of association in term use (Zhou, Ethayarajh \& Jurafsky 2021). It is not the case that estimates will generally go to zero when frequency bias is accounted for. Rather, the word frequency distribution creates *heterogeneous* groupings of similarities in the data that cannot be ignored by taking the mean. Comparative methodologies of this type that do not account for this heterogeneity explicitly are merely averaging over it implicitly. Conversely, researchers can expect substantial improvements in the fit of models of discursive variation by accounting for frequency explicitly, but the interpretability of the resulting model is unclear. I will discuss this in greater detail with respect to the empirical applications below.

## “Leveraging the alignment between machine learning and intersectionality: Using word embeddings to measure intersectional experiences of the nineteenth century U.S. South.” (Nelson 2021, \textit{Poetics})

```{r nelson_setup, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center"}
# Replication of "Measuring Intersectionality" (Nelson 2021 Poetics)
# Describes frequency bias in the canonical SGNS model in the paper
# See https://github.com/lknelson/measuring_intersectionality

# Load canonical embedding
nemb.d <- read_table2(here("data", "nelson2021", "word2vec_all_clean.txt"), col_names = FALSE, skip = 1)
# nemb.d <- read_table2(here("data", "nelson2021", "w2v_adjusted", "word2vec_w5_rs01.txt"), col_names = FALSE, skip = 1)
# nemb.d <- read_table2(here("data", "nelson2021", "w2v_adjusted", "word2vec_w5_rs1.txt"), col_names = FALSE, skip = 1)
nemb.tok <- nemb.d$X1
nemb <- nemb.d[,-1]  # 31362 x 100 embedding matrix
rownames(nemb) <- nemb.tok
sN <- nrow(nemb)
sp <- ncol(nemb)

# Fixed word lists
fwl_male <- c("men", "man", "boy", "boys", "he", "him", "his", "himself")
fwl_female <- c("women", "woman", "girl", "girls", "she", "her", "hers", "herself")
fwl_white <- c("white", "caucasian", "anglosaxon")
fwl_black <- c("black", "colored", "coloured", "negro", "negress", "negros", "afroamerican")

# Load corpus; get lexicon
n1 <- readtext(here("data", "nelson2021", "first-person-narratives-american-south", "data", "texts"))
n2 <- readtext(here("data", "nelson2021", "na-slave-narratives", "data", "texts"))
nc <- corpus(rbind(n1 %>% mutate(doc_id = paste0("a_", doc_id)),
                   n2 %>% mutate(doc_id = paste0("b_", doc_id))))
nt <- tokens(nc, remove_punct=T)  # Drop punctuation

# Form DFM and term frequency matrix
nd <- dfm_trim(dfm(nt), min_termfreq=10)
nt_freq <- textstat_frequency(nd)

# Also useful to have the norms laying around
nt_freq %>%
  filter(feature %in% nemb.tok) %>%
  rowwise() %>%
  mutate(snorm = norm(nemb[feature,], "2")) ->
  tfn
```

```{r nelson_angles_setup, echo=F, message=F}
# Function to plot squared norm as a function of log frequency
# This is Fig. 2 in Arora et al. (2016)
# Also see Eqn. 2.4: log p(w) ~ ||w||/2d - log Z (plus error),
#  where Z = sum exp <w, c>, which is nearly to constant for any choice of context/word
# This is the integral of the softmax function wrt the inner product space; PMI is a function
#  of a Euclidean distance that has been discounted by the relative maximum distance of this
#  point to everything else (log sum exp over the inner product space at this context) as well
#  as the dimensionality of the inner product space
aroraplot <- function(embm, termfreqs, vocab) {
  termfreqs %>%
    filter(feature %in% vocab) %>%
    rowwise() %>%
    mutate(snorm = norm(embm[feature,], "2"))%>%
    ggplot(aes(x=log(frequency), y=snorm^2)) +
    geom_point() +
    geom_vline(xintercept=median(log(termfreqs$frequency)), linetype="dashed", color="tomato") +
    geom_smooth(method="gam") +
    labs(x="Word frequency (log)", y="Squared Euclidean norm")
}

# Now let's examine a sample of the inner product space created by this model
# Function to plot inner product manifold given a set of angles/frequencies
plot_angle_manifold <- function(r_angles) {
  r_angles %>%
    ungroup() %>%
    mutate(lfr = log(a$frequency) + log(b$frequency)) %>%
    arrange(lfr) %>%
    ggplot(aes(x=nprod, y=ab_cs)) +
    geom_point(aes(color=lfr, size=lfr), alpha=0.9) +
    geom_hline(yintercept=mean(r_angles$ab_cs), linetype="dashed") +
    scale_color_viridis_c() +
    geom_smooth(method="lm") +
    geom_smooth(color="tomato") +
    theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) +
    labs(x=latex2exp::TeX("$||A||*||B||$"), y=latex2exp::TeX("$cos(A, B)$"),
         color=latex2exp::TeX("$log(p(A)) * log(p(B))$"), size=latex2exp::TeX("$log(p(A)) * log(p(B))$"))
}

# Look at a uniformly random (wrt. term frequency) sample of term pairs
# Default is 5k term pairs
# aZ and bZ are something like LogSumExp<w,c> (maximum expected inner product to a context)
make_angles <- function(embmat, k=5000) {
  sm <- sample(tfn$feature, k*2)
  sA <- sm[1:k]
  sB <- sm[(k+1):(k*2)]
  data.frame(aterm=sA, bterm=sB) %>%
    rowwise() %>%
    mutate(anorm = norm(embmat[aterm,], "2"),
           bnorm = norm(embmat[bterm,], "2"),
           a = tfn[which(tfn$feature == aterm),"frequency"],
           b = tfn[which(tfn$feature == bterm),"frequency"],
           ab_cs = lsa::cosine(as.numeric(embmat[aterm,]), as.numeric(embmat[bterm,]))[1],
           ab_ip = as.numeric(embmat[aterm,]) %*% as.numeric(embmat[bterm,]),
           ppmcc = cor(as.numeric(embmat[aterm,]), as.numeric(embmat[bterm,])),
           nprod = anorm * bnorm,
           aZ = word2vec_similarity(as.numeric(embmat[aterm,]),
                                    as.matrix(embmat[-which(rownames(embmat) == aterm),]),
                                              top_n=1, type="cosine") %$% similarity * nprod,
           bZ = word2vec_similarity(as.numeric(embmat[bterm,]),
                                    as.matrix(embmat[-which(rownames(embmat) == bterm),]),
                                              top_n=1, type="cosine") %$% similarity * nprod,
           gK = -1 * (1 + nprod^2 + ab_cs^2)^-2) ->
    random_angles
  
  return(random_angles)
}

# Alternatively, use the random angle set and color by cosine similarity
lnpw_plot <- function(r_angles) {
  r_angles %>%
    arrange(ab_cs) %>%
    ggplot(aes(x=sqrt(log(a$frequency))*sqrt(log(b$frequency)), y=nprod, color=ab_cs)) +
    geom_point() +
    geom_smooth() +
    scale_color_viridis_c() +
    theme(legend.position = "bottom") +
    labs(x="Frequency product (log)",
         y="Local normalization weight")
}

# Get term pair sample
ra_nemb <- make_angles(nemb)
```

```{r nelson_fcomp_setup, echo=F, message=F}
# The analysis is based on arithmetic composites across the term lists
# Let's inspect these directly
# First, modify above functions to work on a composite vector we provide
view_from_composite <- function(embmat, fv) {
  if(is.data.frame(embmat)) {
    embmat_ <- as.matrix(embmat)
  }
  
  # dotp <- word2vec_similarity(fv, embmat, top_n=nrow(embmat))

  word2vec_similarity(fv, embmat_, top_n=nrow(embmat_), type="cosine") %>%
    mutate(angle = acos(similarity) * (180/pi)) %>%
    left_join(tfn %>% as.data.frame() %>% select(feature, frequency, snorm), by=c("term2"="feature")) %>%
    mutate(nprod = snorm * norm(fv, "2"),
           fprod = sqrt(log(frequency)) * snorm) ->
    test_1term
  return(test_1term)
}

add_composite_ip <- function(comp, embmat, fv) {
  comp %>%
    rowwise() %>%
    mutate(inner_product = fv %*% as.numeric(embmat[term2,]))
}

plot_composite_view <- function(csims, fv_label, topsel=NA, mv=F) {
  spflab <- ifelse(mv,
                   "$\\textit{%s}$ (mean vector)",
                   "$\\textit{%s}$")
  # Commented lines use sqrt(log(p(w))) * norm(focal vector) 
  csims %>%
    filter(similarity < 1) %>%  # Drop self-comparison if in data
    ggplot(aes(x=log(frequency), y=similarity, color=snorm)) +
    # ggplot(aes(x=fprod, y=similarity, color=snorm)) +
    geom_point() +
    geom_vline(xintercept=median(log(csims$frequency), na.rm=T), linetype="dotted", alpha=0.8) +
    # geom_vline(xintercept=median(csims$fprod, na.rm=T), linetype="dotted", alpha=0.8) +
    geom_hline(yintercept=mean(csims$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
    geom_smooth(method="gam", color="tomato") +
    geom_smooth(method="lm") +
    scale_color_viridis_c() +
    ggtitle(latex2exp::TeX(sprintf(spflab, fv_label))) +
    labs(x="Word frequency (log scale)",
         y="Cosine similarity",
         color=latex2exp::TeX("$||w_j||$")) -> plt
  geom_smooth(method="lm") +if(is.na(topsel)) {
    return(plt)
  } else {
    return(plt +
             geom_hline(yintercept=csims[topsel,"similarity"],
                        linetype="dotted", color="violetred2", size=2) +
             geom_smooth(method="lm", data=csims %>% arrange(desc(similarity)) %>% slice_head(n=50)))
  }
}

# For two fixed word lists, get mean of all paired sum vectors
# See Nelson (2021) p. 5
compute_fwl_meanvector <- function(embmat, fwl1, fwl2) {
  expand.grid(fwl1, fwl2) %>%
    rowwise() %>%
    summarize(embmat[which(rownames(embmat) == Var1),] + embmat[which(rownames(embmat) == Var2),]) %>%
    colMeans() ->
    mv_fwl
  return(mv_fwl)
}

# Compute the mean social identity vectors
mv_male_black <- compute_fwl_meanvector(nemb, fwl_male, fwl_black)
mv_female_black <- compute_fwl_meanvector(nemb, fwl_female, fwl_black)
mv_male_white <- compute_fwl_meanvector(nemb, fwl_male, fwl_white)
mv_female_white <- compute_fwl_meanvector(nemb, fwl_female, fwl_white)

# Compute the social institution inducing vectors
v_polity <- as.matrix(nemb["nation",] + nemb["state",])
v_economy <- as.matrix(nemb["money",])
v_culture <- as.matrix(nemb["culture",])
v_domestic <- as.matrix(nemb["housework",] + nemb["children",])
v_authority <- as.matrix(nemb["authority",])
```


Nelson (2021) presents a word embedding-based analysis of autobiographical narratives describing intersectional life experiences in the 19th century U.S. South. The study builds from a corpus of English "diaries, autobiographies, memoirs, travel accounts, and ex-slave narratives" as well as a number of "autobiographical narratives," "biographies," and "fictionalized" accounts by fugitive and former slaves. The word embedding analysis in the paper is built on a word2vec SGNS model with $p=100$ dimensions. The model employs a context window of size 5 and a minimum word frequency threshold of 10, indicating a preference for capturing paradigmatic semantic associations (Schütze & Pedersen 1999), and it is trained directly on the target corpus from scratch.

The key quantitative measures in the paper are based on cosine similarity between composed vectors. A set of \textit{social category vectors} is constructed by averaging pairs of vectors corresponding to a small number of fixed terms in lists of "women/men/Black/white synonyms" (see "Appendix: List of Words Used to Create Four Averaged Social Category Vectors"). A set of \textit{social institution vectors} is constructed by identifying the 50 word vectors most closely aligned with the vectors \texttt{nation + state} (polity), \texttt{money} (economy), \texttt{culture} (culture), and \texttt{housework + children} (domestic). Figure 3 (original) reports differences in mean cosine similarity between each pair of social category vectors with respect to each social institution vector, resulting in 24 total aggregate comparisons. After qualitative analysis, an additional set of cross-categorical differences in means is computed with respect to the vector \texttt{authority}, reported in Figure 5 (original).

The qualitative dimension of Nelson's methodology apart from the use of cosine similarity aggregation is relevant to the analysis of word frequencies. Three interlocking analytic moves are particularly interesting from the perspective of frequency bias. First, the analysis is not based solely on quantities derived from word embeddings; Nelson develops substantive insights through a dynamic engagement between model results and close reading of the underlying texts (also see Nelson 2020). Second, this portion of the analysis employs cosine similarity as an indexing measure to surface words closely related to terms of interest. Third, it is through the combination of these two approaches---more dense interpretive engagement with the text \textit{and} the use of the cosine ratio for information retrieval---that Nelson is able to observe rich and meaningful differences in diction across the texts included in the corpus. Notably, the analysis in this portion of the paper focuses a great deal on relative word frequencies; I return to this point at the end of the paper.

The measures of "intersectional discursive space" in the paper are affected by frequency bias in ways that considerably affect their interpretation. In particular, the identity-pairwise differences in mean cosine similarity summarized in Figures 3 and 5 of the paper are skewed by the implicit omission of word frequency from each model. To show this, I compare each original estimated difference in means to a model including an additive term for the local normalization weight $\phi(w_i, w_j)$ for each comparison:  
$$
\begin{aligned}
\cos(w_i, w_j) &= \mathbbm{1}[\text{social category}] + \epsilon\\
\cos(w_i, w_j) &= \mathbbm{1}[\text{social category}] + \phi(w_i, w_j) + \epsilon\\
\end{aligned}
$$

```{r nelson_diffinmeans, echo=F, message=F, fig.height=8.8, out.width="0.9\\textwidth", fig.align="center", fig.cap="Change in estimated difference in mean cosine similarity between paired social category mean vectors and social institution vectors (Nelson 2021). Original model estimate in blue and local normalization weight-adjusted linear model estimate in red with parametric 95\\% confidence intervals superimposed. Labels indicate $\\text{R}^2$ in the adjusted model. The change in the difference in means reflects the relative difference between the implicit word cofrequency distributions $P(Category(A), Institution)$ and $P(Category(B), Institution)$."}
# How frequency bias affects regression: omitted variable bias in Fig. 3
# Predict difference in means from log term frequency
# Each DiM is a linear regression of the form cos(identity vector, institution vector) ~ identity category
# The frequency bias can be captured by adding various frequency measures to the design matrix

# Construct all 24 comparisons shown in Fig. 3
# Also include the "authority" comparison in Fig. 5
# k=50 is the original setting in the paper (try higher/lower)
k <- 50
polity_vmat <- view_from_composite(nemb, v_polity) %>%
  filter(!term2 %in% c("nation", "state") & !is.na(frequency)) %>%
  slice_head(n=k)
polity_words <- polity_vmat$term2
polity_snorm <- polity_vmat$snorm
polity_vmat <- as.matrix(nemb[polity_vmat$term2,])
economy_vmat <- view_from_composite(nemb, v_economy) %>%
  filter(!term2 %in% c("money") & !is.na(frequency)) %>%
  slice_head(n=k)
economy_words <- economy_vmat$term2
economy_snorm <- economy_vmat$snorm
economy_vmat <- as.matrix(nemb[economy_vmat$term2,])
culture_vmat <- view_from_composite(nemb, v_culture) %>%
  filter(!term2 %in% c("culture") & !is.na(frequency)) %>%
  slice_head(n=k)
culture_words <- culture_vmat$term2
culture_snorm <- culture_vmat$snorm
culture_vmat <- as.matrix(nemb[culture_vmat$term2,])
domestic_vmat <- view_from_composite(nemb, v_domestic) %>%
  filter(!term2 %in% c("housework", "children") & !is.na(frequency)) %>%
  slice_head(n=k)
domestic_words <- domestic_vmat$term2
domestic_snorm <- domestic_vmat$snorm
domestic_vmat <- as.matrix(nemb[domestic_vmat$term2,])
authority_vmat <- view_from_composite(nemb, v_authority) %>%
  filter(!term2 %in% c("authority") & !is.na(frequency)) %>%
  slice_head(n=k)
authority_words <- authority_vmat$term2
authority_snorm <- authority_vmat$snorm
authority_vmat <- as.matrix(nemb[authority_vmat$term2,])

# Anisotropy interlude

# An interesting plot: the variance-covariance matrix of these subspaces
# data.frame(v1 = colnames(economy_vmat), data.frame(cov(economy_vmat))) %>%
#   pivot_longer(starts_with("X")) %>%
#   rename(v2=name) %>%
#   ggplot(aes(x=v1, y=v2, fill=value)) +
#   geom_tile() +
#   scale_fill_viridis_c()

# Hausdorff distance from subspace A to subspace B is the maximum distance
#  from a vector in subspace A to its closest vector in subspace B.
#  The quantity is non-commutative when the subspaces aren't the same shape;
#   this is another way of saying the distance matrix is asymmetric.
#  You can use the difference as a measure of anisotropy
#   - you can perturb this to look at how sensitive the anisotropy is to leave-one-out
polity_economy_dm <- as.matrix(pdist::pdist(polity_vmat, economy_vmat))
#polity_economy_dm %>% reshape2::melt() %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + geom_tile()  # Asymmetric
hd_polity_economy_left <- max(apply(polity_economy_dm, 1, min))  # DH(polity->economy)
hd_polity_economy_right <- max(apply(polity_economy_dm, 2, min))  # DH(economy->polity)

# There is often a small set of target points that induces the metric for a large
#  set of the starting points, in both directions. The modal closest point tends to be
#  the maximally far point in the set of closest points (i.e. the Hausdorff inducing point).
#  IDEA: It is better if the comparison 
hd_polity_economy_left.v <- apply(polity_economy_dm, 1, which.min)
hd_polity_economy_right.v <- apply(polity_economy_dm, 2, which.min)
hd_polity_economy_left.i <- which.max(apply(polity_economy_dm, 1, min))
hd_polity_economy_right.i <- which.max(apply(polity_economy_dm, 2, min))

# Another example of anisotropic comparison
# This one is less asymmetric
culture_domestic_dm <- as.matrix(pdist::pdist(culture_vmat, domestic_vmat))
hd_culture_domestic_left <- max(apply(culture_domestic_dm, 1, min))  # DH(culture->domestic)
hd_culture_domestic_right <- max(apply(culture_domestic_dm, 2, min))  # DH(domestic->culture)
hd_culture_domestic_left.v <- apply(culture_domestic_dm, 1, which.min)
hd_culture_domestic_right.v <- apply(culture_domestic_dm, 2, which.min)
hd_culture_domestic_left.i <- which.max(apply(culture_domestic_dm, 1, min))
hd_culture_domestic_right.i <- which.max(apply(culture_domestic_dm, 2, min))

culture_economy_dm <- as.matrix(pdist::pdist(culture_vmat, economy_vmat))
hd_culture_economy_left <- max(apply(culture_economy_dm, 1, min))  # DH(culture->economy)
hd_culture_economy_right <- max(apply(culture_economy_dm, 2, min))  # DH(economy->culture)
hd_culture_economy_left.v <- apply(culture_economy_dm, 1, which.min)
hd_culture_economy_right.v <- apply(culture_economy_dm, 2, which.min)
hd_culture_economy_left.i <- which.max(apply(culture_economy_dm, 1, min))
hd_culture_economy_right.i <- which.max(apply(culture_economy_dm, 2, min))



# Back to frequency bias...!
cs.male_black_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_male_black))
cs.male_black_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_male_black))
cs.male_black_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_male_black))
cs.male_black_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_male_black))
cs.male_black_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_male_black))

cs.female_black_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_female_black))
cs.female_black_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_female_black))
cs.female_black_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_female_black))
cs.female_black_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_female_black))
cs.female_black_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_female_black))

cs.male_white_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_male_white))
cs.male_white_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_male_white))
cs.male_white_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_male_white))
cs.male_white_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_male_white))
cs.male_white_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_male_white))

cs.female_white_polity <- sapply(1:nrow(polity_vmat), function(j) lsa::cosine(polity_vmat[j,], mv_female_white))
cs.female_white_economy <- sapply(1:nrow(economy_vmat), function(j) lsa::cosine(economy_vmat[j,], mv_female_white))
cs.female_white_culture <- sapply(1:nrow(culture_vmat), function(j) lsa::cosine(culture_vmat[j,], mv_female_white))
cs.female_white_domestic <- sapply(1:nrow(domestic_vmat), function(j) lsa::cosine(domestic_vmat[j,], mv_female_white))
cs.female_white_authority <- sapply(1:nrow(authority_vmat), function(j) lsa::cosine(authority_vmat[j,], mv_female_white))

# Function to construct comparison
make_dim_df <- function(a, b, alab, blab, av, bv, flex, ext_freq=words_219k_m2138) {
  an <- norm(av, "2")
  bn <- norm(bv, "2")
  rbind.data.frame(data.frame(cs=a, identity=alab, word=flex) %>% 
                   left_join(tfn, by=c("word"="feature")) %>%
                   left_join(ext_freq %>% select(word, ext.freq=freq), by="word") %>%
                   mutate(nprod = snorm * an,
                          fprod = sqrt(log(ext.freq)) * an),
                 data.frame(cs=b, identity=blab, word=flex) %>%
                   left_join(tfn, by=c("word"="feature")) %>%
                   left_join(ext_freq %>% select(word, ext.freq=freq), by="word") %>%
                   mutate(nprod = snorm * bn,
                          fprod = sqrt(log(ext.freq)) * bn)) ->
    ddf
  return(ddf)
}

# Show linear model frequency bias 
linear_frequency_bias <- function(dimdf, dimtitle, use.scale=F, use.nprod=T, robust=F, ftest=F, stargazer=T, focal.p=F, only.dim=F, fullsumm=F, delta.rsq=F) {
  if(use.scale) {
    dimdf$fbmeasure <- dimdf$snorm^2
  } else if(use.nprod) {
    dimdf$fbmeasure <- 1/dimdf$nprod  
  } else {
    dimdf$fbmeasure <- log(dimdf$frequency)
  }
  ols <- lm(cs ~ identity, data=dimdf)
  ols.freqbias <- lm(cs ~ identity + fbmeasure, data=dimdf)
  ols.fb2 <- lm(cs ~ identity + fprod, data=dimdf)
  if(ftest) {
    anova(ols, ols.freqbias)
  } else if(stargazer) {
    stargazer::stargazer(ols, ols.freqbias, ols.fb2,
                         title=dimtitle,
                         dep.var.labels=c("Cosine similarity"),
                         dep.var.caption="",
                         covariate.labels=c("Social category vector", "Local normalization weight",
                                            "Scaled log COCA frequency", "Constant"),
                         star.cutoffs=c(0.05, 0.01, 0.001),
                         omit.stat=c("n", "adj.rsq"),
                         single.row = T,
                         column.sep.width = "1pt",
                         font.size = "footnotesize",
                         header=F)
  } else if(focal.p) {
    summary(polity_mb_fb.freqbias)$coefficients[2,4]
  } else if(only.dim) {
    which.mod <- ols.freqbias  # Default
    # which.mod <- ols.fb2  # Shows the figure with the COCA frequency estimate instead
    return(c(comparison = paste0(unique(dimdf$identity)),
             r2=summary(which.mod)$r.squared,
             coef(summary(which.mod))[2,1:2]))
  } else if(fullsumm) {
    if(robust) {
      print(coeftest(ols, vcov = vcovHC(ols, type="HC1")))
      print(coeftest(ols.freqbias, vcov = vcovHC(ols.freqbias, type="HC1")))
    } else {
      print(summary(ols))
      print(summary(ols.freqbias))
      summarize_ovb(dimdf)
    }
  } else if(delta.rsq) {
    data.frame(rsq=c(summary(ols)$r.squared, summary(ols.freqbias)$r.squared),
               model=c("original model", "frequency interaction"))
  } else {
    rbind.data.frame(data.frame(summary(ols)$coefficients, model="original model"),
                     data.frame(summary(ols.freqbias)$coefficients, model="frequency interaction")) %>%
      mutate(sig=tsig(`Pr...t..`))
  }
}

# Frequency bias tends to create omitted variable bias because frequency is correlated
#  with the cosine similarity and the relative frequency distributions differ by group
# Adding a measure of frequency into the model tends to increase the standard error on the
#  grouping variable in the difference in mean, and the model fit goes up a lot
# Note that this doesn't always make the effect "go away" per se; the problem is that
#  frequency has a somewhat unpredictable relationship to this problem because it depends
#  on the exact difference in mean log frequency by group and the linear correlation of 
#  the log frequency with the cosine similarity
# In general the reciprocal local normalization weight is the best frequency measure to use
#  because it's a component of cosine similarity, but any functional form will work; the 
#  key thing here is that the LPNW tends to vary by group (frequency bias is relative)
rbind(

# Black male <-> Black female
data.frame(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), comparison="Black, Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), comparison="Black, Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), comparison="Black, Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), comparison="Black, Male x Black, Female (domestic)"),

# White male <-> White female
data.frame(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), comparison="White Male x White Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), comparison="White Male x White Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), comparison="White Male x White Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), comparison="White Male x White Female (domestic)"),

# White female <-> Black female
data.frame(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), comparison="White Female x Black, Female (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), comparison="White Female x Black, Female (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), comparison="White Female x Black, Female (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), comparison="White Female x Black, Female (domestic)"),

# Black male <-> White male
data.frame(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), comparison="Black, Male x White Male (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), comparison="Black, Male x White Male (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), comparison="Black, Male x White Male (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), comparison="Black, Male x White Male (domestic)"),

# White male <-> Black female
data.frame(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), comparison="White Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), comparison="White Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), comparison="White Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), comparison="White Male x Black, Female (domestic)"),

# White female <-> Black male
data.frame(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), comparison="White Female x Black, Male (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), comparison="White Female x Black, Male (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), comparison="White Female x Black, Male (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), comparison="White Female x Black, Male (domestic)"),

# Authority (Fig. 5)
data.frame(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), comparison="Black, Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), comparison="White Male x White Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), comparison="White Female x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), comparison="Black, Male x White Male (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), comparison="White Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), comparison="White Female x Black, Male (authority)")) ->
  all_comparisons

# Get all of the regression results in one place
# This is unspeakably ugly but oh well
rbind(

# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)", only.dim=T, stargazer=F),

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)", only.dim=T, stargazer=F),

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)", only.dim=T, stargazer=F),

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)", only.dim=T, stargazer=F)) ->
  fblm_results_f

institutions <- c("polity", "economy", "culture", "domestic", "authority")
data.frame(fblm_results_f) %>%
  rowwise() %>%
  mutate(institution = institutions[which(str_detect(comparison1, institutions))],
         comparison = str_replace_all(paste0(str_remove(comparison2, institution), "- ",
                                             str_remove(comparison1, institution)), "_", " "),
         comparison2 = str_replace_all(paste0(str_remove(comparison1, institution), "- ",
                                              str_remove(comparison2, institution)), "_", " ")) %>%
  rename(lm.est.dim = Estimate, lm.est.se = Std..Error) ->
  fblm_results

# Plot all comparisons
all_comparisons %>%
  mutate(institution=str_remove_all(str_extract(comparison, "\\([a-z]+\\)"), "[()]")) %>%
  group_by(institution, identity) %>%
  summarize(mean_cs = mean(cs), sd_cs = sd(cs)) %>%
  ungroup() ->
  ac_summ

ac_summ %>%
  left_join(ac_summ, by="institution") %>%
  filter(!duplicated(paste0(pmin(identity.x, identity.y),
                            pmax(identity.x, identity.y))) &
           identity.x != identity.y) %>%
  mutate(diffm_cs = mean_cs.y - mean_cs.x,
         diff.se_cs = sqrt(sd_cs.x^2/k + sd_cs.y^2/k),
         comparison = str_replace_all(paste0(str_remove(identity.x, institution), "- ",
                                             str_remove(identity.y, institution)), "_", " ")) %>%
  left_join(fblm_results, by=c("comparison", "institution")) %>%
  left_join(fblm_results, by=c("comparison"="comparison2", "institution")) %>%
  mutate(lm.est.dim = as.numeric(coalesce(lm.est.dim.x, lm.est.dim.y)),
         lm.est.se = as.numeric(coalesce(lm.est.se.x, lm.est.se.y)),
         r2 = as.numeric(coalesce(r2.x, r2.y))) ->
  nemb_dim_estimates

nemb_dim_estimates %>%
  rowwise() %>%
  mutate(label.pin.x = max(lm.est.dim+(1.96*lm.est.se), diffm_cs+(1.96*diff.se_cs)) + 0.035,
         label.text = sprintf("R² = %s", round(r2, 3))) %>%
  ggplot() +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_point(aes(y=comparison, x=diffm_cs), color="steelblue2",
             position=position_nudge(y=0.1)) +
  geom_errorbarh(aes(y=comparison,
                     xmin=diffm_cs - 1.96*diff.se_cs,
                     xmax=diffm_cs + 1.96*diff.se_cs),
                 height=0.2, color="steelblue2", position=position_nudge(y=0.1)) +
  geom_point(aes(y=comparison, x=lm.est.dim), color="tomato",
             position=position_nudge(y=-0.1)) +
  geom_errorbarh(aes(y=comparison,
                   xmin=lm.est.dim - 1.96*lm.est.se,
                   xmax=lm.est.dim + 1.96*lm.est.se),
               height=0.2, color="tomato", position=position_nudge(y=-0.1)) +
  geom_label(aes(label=label.text, y=comparison, x=label.pin.x), size=3, color="tomato") +
  scale_x_continuous(limits=c(-0.15,0.2), n.breaks=10) +
  facet_wrap(~institution, ncol=1) +
  labs(x="Difference in mean cosine similarity", y="")

# Show scatterplots of means
# all_comparisons %>%
#   group_by(identity, comparison) %>%
#   mutate(lgmean = mean(cs), lgsd = sd(cs)) %>%
#   ungroup() %>%
#   arrange(nprod) %>%
#   ggplot(aes(y=identity, x=cs, color=nprod)) +
#   geom_jitter(width=0.2, alpha=0.3) +
#   geom_point(aes(y=identity, x=lgmean), color="black") +
#   geom_errorbarh(aes(y=identity, xmin=lgmean-1.96*lgsd, xmax=lgmean+1.96*lgsd), height=0.2, color="black") +
#   scale_color_viridis_c() +
#   theme(legend.position="bottom") +
#   facet_wrap(~comparison, scales = "free_y", ncol=5) +
#   labs(x="Focal social category vector",
#        y="Cosine similarity",
#        color="local normalization weight")
```

Figure \ref{fig:nelson_diffinmeans} displays the results of this analysis; the full results are reported in tabular form in the appendix. Several aspects of this analysis are worth noting. First, adding the local normalization weight to the model increases fit considerably; the change in $\text{R}^2$ varies between 0.179 and 0.688. Second, because the variance of the cosine similarity is also dependent on the local normalization weight, the estimated standard error on the identity coefficient also changes across every comparison. Third, the direction of the bias is not always the same, and in particular it does not always move the point estimate of interest toward zero.

To visualize the degree of frequency bias, we can plot the word frequency estimate against the distribution of cosine similarities to the composed vectors that structure the analysis. Figure \ref{fig:nelson_focal_composite} shows the resulting scatterplot for the social category mean vectors; Figures \ref{fig:nelson_frequencycomp_institutions} and \ref{fig:nelson_authority_mv} display the relationship for the social institution unigram/sum vectors. Each plot shows that the mean cosine similarity is non-constant over the word frequency distribution, and tends to be systematically higher for shorter vectors (less frequent words). Another intriguing feature of the measure is the systematic distortion in the shape of the cosine projection with respect to frequency when the focal vector is composed of more word vectors. The manifold shape in Figure \ref{fig:nelson_focal_composite} is markedly shifted upwards on the extreme ends of the word frequency distribution relative to the lower-dimensional focal vectors in Figure \ref{fig:nelson_frequencycomp_institutions}.

```{r nelson_focal_composite, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, social category mean vectors. Color indicates norm of context word; note increasing vector lengths by word frequency."}
# All of the mean vectors exhibit frequency bias as well
ggarrange(plot_composite_view(view_from_composite(nemb, mv_male_black), "male + black", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_female_black), "female + black", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_male_white), "male + white", mv=T),
          plot_composite_view(view_from_composite(nemb, mv_female_white), "female + white", mv=T),
          ncol=2, nrow=2, common.legend=T, legend = "bottom")
```

```{r nelson_frequencycomp_institutions, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, social institution inducing vectors. Color indicates norm of context word; note increasing vector lengths by word frequency. Linear fit above red dotted line shows trend in top 50 vectors selected for analysis."}
# It's also useful to look at what we're averaging over
# The biases on each of the component 2sum vectors vary
# Not shown in the paper
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["man",] + nemb["black",])), "man + black")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["his",] + nemb["black",])), "his + black")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["herself",] + nemb["colored",])), "herself + colored")
# plot_composite_view(view_from_composite(nemb, as.matrix(nemb["she",] + nemb["white",])), "she + white")

# Also look at the "social institution" vectors
# Top 50 terms thresholds, respectively: 0.638, 0.622, 0.665, 0.665
ggarrange(plot_composite_view(view_from_composite(nemb, v_polity), "nation + state", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_economy), "money", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_culture), "culture", topsel = 50),
          plot_composite_view(view_from_composite(nemb, v_domestic), "housework + children", topsel = 50),
          ncol=2, nrow=2, common.legend=T, legend = "bottom")
```

```{r nelson_authority_mv, echo=F, message=F, fig.height=5, out.width="0.6\\textwidth", fig.align="center", fig.cap="Cosine-frequency plot, \\texttt{authority} vector. Color indicates norm of context word; note increasing vector lengths by word frequency. Linear fit above red dotted line shows trend in top 50 vectors selected for analysis."}
plot_composite_view(view_from_composite(nemb, v_authority), "authority", topsel = 50)
```

## "The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings." (Kozlowski, Taddy and Evans 2019)

\textit{In progress.}

## “Aligning Differences: Discursive Diversity and Team Performance.” (Lix et al. 2022) 

\textit{In progress.}

## “The Automatic Analysis of Emotion in Political Speech Based on Transcripts.” (Cochrane et al. 2022)

\textit{In progress.}


# Frequency distortion in cosine similarity arithmetic

This section discusses a number of frequency-related issues with cosine similarity-based analysis: the general dependence of the cosine similarity distribution on the local normalization weight; the impact of applying linear combinations to sets of vectors prior to taking cosines; the effect of subtracting the global mean vector; and the challenges associated with estimating frequency-biased frequency weights.

## Semantic vector spaces and word frequency

In semantic vector space analysis, word vectors have Euclidean norms that scale with their frequencies. Each word vector has been selected such that its scale optimally represents this information. The easiest way to see this is to plot the vector norms and word frequency estimates against each other (Arora et al. 2016). This pattern is reproduced in Euclidean-arithmetic comparisons of word vectors, and it can be visualized by plotting the pairwise product of norms (the local normalization weight, $\phi_{AB}$) against the pairwise product of frequencies. Figure \ref{fig:nelson_aroraplot} shows these relationships side-by-side for a random sample of term pairs.

```{r nelson_aroraplot, echo=F, message=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Squared Euclidean norm of embedding vectors against root-log frequency for all words in the underlying corpus, with median log frequency indicated by red dashed line (top panel). local normalization weight against product of root-log frequencies for all word pairs in the corpus; uniformly random sample of 5000 word vectors without replacement (bottom panel). Note inflection point and changes in variance over the trend in both panels."}
# Squared vector norm is proportional to sqrt log term frequency
# The relationship in this model is a little weird for the high frequency terms
# It should be approximately curvilinear but it's quadratic (???)
aroraplot(nemb, nt_freq, nemb.tok) -> fbp1

# Similarly, local normalization weight is proportional to product of sqrt log term frequency
lnpw_plot(ra_nemb) -> fbp2

# Plot these side-by-side
ggarrange(fbp1, fbp2, ncol=1)
```

This relationship implies that the global distribution of cosine similarity with respect to frequency is not uniform. Figure \ref{fig:nelson_cosfreq} plots cosine similarity against the product of root-log frequencies, optionally split by local normalization weight quantiles. We observe the predicted relationship: mean cosine similarity is systematically larger in the rare region of the word frequency distribution. The degree of association between mean cosine similarity and frequency is not constant; the association is low among pairs representing proportionally more common word and high among pairs representing proportionally more rare words. Also, observe that the relationship also has non-constant variance: the range of cosine similarities observed at a given scale varies smoothly by scale. Figure \ref{fig:global_mean_composite} visualizes the difference in frequency biases between the inner product and cosine similarity.

```{r nelson_cosfreq, echo=F, message=F, fig.height=6, out.width="\\textwidth", fig.align="center", fig.cap="Distribution of cosine similarities by product of log frequencies of component words. Linear trend in blue and generalized additive fit in red; points colored by local normalization weight of this word pair. Bottom panel splits top panel by local normalization weight quantiles; note partial overlap in frequency product. Cosine similarity is inflated on average when frequency is low, and deflated on average when frequency is high."}
# Plot cosine simlarity against the log frequency ratio
# I think this one kinda looks like a peacock.
# If you split by LPNW quantiles, you can see the relationship get flat (or close to flat)
ra_nemb %>%
  ungroup() %>%
  mutate(lfr = sqrt(log(a$frequency)) * sqrt(log(b$frequency))) %>%
  arrange(nprod) %>%
  ggplot(aes(x=lfr, y=ab_cs, color=nprod)) +
  geom_point(size=3) +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  geom_smooth(method="lm") +
  geom_smooth(color="tomato") +
  scale_color_viridis_c() -> rasqf1
ra_nemb %>%
  ungroup() %>%
  mutate(lfr = log(a$frequency) * log(b$frequency),
         ile = ntile(nprod, 9)) %>%
  arrange(nprod) %>%
  ggplot(aes(x=lfr, y=ab_cs, color=nprod)) +
  geom_point(size=3) +
  geom_smooth(method=MASS::rlm, method.args=list(method="MM"), color="tomato") +
  geom_hline(yintercept=mean(ra_nemb$ab_cs), linetype="dashed") +
  scale_color_viridis_c() +
  facet_wrap(~ile) +
  theme(legend.position="bottom") -> rasqf2
ggarrange(rasqf1, rasqf2, ncol=2, common.legend = T, legend="bottom")
```

```{r global_mean_composite, echo=F, warning=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Frequency bias in distribution of cosine similarity (top panel) and inner product (bottom panel) between context words and global mean vector; point color indicates squared norm of context word. Cosine similarity bends the region of the inner product space containing the short/infrequent vectors toward the mean vector."}
global_mean_vector <- colMeans(nemb)
view_from_composite(nemb, global_mean_vector) %>%
  mutate(freq = sqrt(log(frequency))) ->
  gmv

# Append inner product
# Takes a while to compute all of them
gmv <- add_composite_ip(gmv, nemb, global_mean_vector)

# Plot
gmv %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=similarity, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Frequency-cosine plot, global mean vector",
       x="Word frequency (sqrt log)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb1
gmv %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=inner_product, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv$inner_product, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  scale_y_reverse() +
  theme(legend.position="bottom") +
  labs(title="Frequency-inner product plot, global mean vector",
       x="Word frequency (sqrt log)",
       y="Inner product",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb2
ggarrange(gfb1, gfb2, ncol=1, common.legend = T, legend="bottom")
```

## Cosine similarity as a hyperbolic projection of the inner product space

The previous section suggests that cosine similarity can be understood as a non-Euclidean projection of the inner product space that distorts it in a way described by the local normalization weight function. This suggests the following geometric interpretation of the projection: it is smoothly bending the rare region of the underlying word frequency distribution toward the global mean vector. We can visualize this by looking at the mean cosine similarity at every frequency with the global mean vector (Figure \ref{fig:global_mean_csbend}). This brings our attention to the functional form of the cosine similarity operation. What enables this seemingly very simple quantity to perform such a complicated transformation of the inner product space?

```{r global_mean_csbend, echo=F, warning=F, fig.height=6, out.width="0.8\\textwidth", fig.align="center", fig.cap="Mean cosine similarity at observed frequencies (sqrt log); piecewise linear fit split at $\\sqrt{\\log p(w_i)} = 2.3$ overlaid in red, and points are color scaled by their squared Euclidean norm. Vectors with low frequencies (fewer than approximately 200 occurrences) exhibit a strong negative linear correlation with high mean cosine similarity to the global mean vector ($\\rho=-0.9863$). High frequencies are minimally correlated with cosine similarity. The variance in mean cosine similarity is much higher in the common word region, and there is visible residual bias in the scale-cosine relationship among words with similarly high frequencies."}
# Compute means
gmv %>%
  group_by(frequency) %>%
  summarize(n=n(), mean_sim=mean(similarity), mean_norm=mean(snorm), freq=first(freq)) %>%
  mutate(regime=freq <= 2.8) ->
  gmv_means

# Slope of just the negative linear component gives the rarity bias coefficient
# 0.1 units of change in sqrt-log frequency implies a -0.0351 change in mean cosine to the mean vector
#  for terms satisfying p(w) <= 200; note R2=0.973, but also note visible bias
# summary(lm(mean_sim ~ freq, data=gmv_means %>% filter(regime)))
# cor(gmv_means %>% filter(regime) %$% freq, gmv_means %>% filter(regime) %$% mean_sim, use="complete.obs")

# Plot
gmv_means %>%
  arrange(mean_norm) %>%
  ggplot(aes(x=freq, y=mean_sim, color=mean_norm)) +
  geom_point() +
  # geom_smooth(aes(group=regime), method="m", color="tomato", linetype="dashed", alpha=0.5) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Mean cosine-frequency, global mean vector",
       x="Word frequency (sqrt log)",
       y="Mean cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$"))
```

```{r nelson_lld, echo=F, message=F}
# Cosine similarity as a locally linear perspective on the inner product space
# The factorization implies you can look at the manifold from three "sides"
local_linear_decomp <- function(rangs) {
  # Plot some inner product distribution by components. Larger objects in the
  # space have more differentiated IPs, but there are also way fewer of them.
  rangs %>%
    ggplot(aes(y=ab_cs, color=ab_ip, x=nprod, size=ab_ip)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v1
  
  # Cosine similarity differentiates three regions on the inner product space
  # that we can more easily see if we look from the cosine similarity into the
  # manifold. Looking from this perspective helps you see the "twist" structure
  # in A by rotating to the position in which the "flat" structure has a minimal
  # profile and the twist has a maximal profile. The extremal region of the twist
  # (by local normalization weight) is generally more "flat" than it was from the other perspective.
  # The linear relationship between local normalization weight and the IP is controlled by cosine similarity.
  rangs %>%
    ggplot(aes(color=ab_cs, y=ab_ip, x=nprod, size=ab_cs)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v2
  
  # Cosine similarity has a linear relationship to the inner product, but only
  # conditional on the local normalization weight, which defines a class of vector pairs
  # with the same CS-IP linear relationship. The distribution of cosine similarities in
  # any given class is generally skewed and tends to have non-zero mean. The variance of
  # the cosine similarity distribution is non-constant and varies systematically with the
  # local normalization weight. One way of thinking about this is that the cosine similarity
  # is a maximally linear perspective on the manifold.
  rangs %>%
    ggplot(aes(x=ab_cs, y=ab_ip, color=nprod, size=nprod)) +
    geom_point() +
    scale_color_viridis_c() +
    guides(size=F) +
    theme(legend.position="bottom") ->
    v3
  
  ggarrange(v1, v2, v3, ncol=3)
}
```

```{r nelson_lld_print, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
local_linear_decomp(ra_nemb)
```

The cosine similarity between two points is a ratio of three inner products of the form $f(a,b,c)=\frac{a}{bc}$. Consequently, the projecting transformation has the shape of a hyperbolic paraboloid.^[Readers may find this shape surprisingly familiar. The "saddle roof" used to cover large buildings like churches and stadiums is a hyperbolic paraboloid; this is also the shape of each chip in a tube of Pringles.] A more precise way of characterizing the non-Euclidean shape of this surface is in terms of its pointwise Gaussian curvature:  
$$\text{K}(A, B) = -(1 + ||A||^2||B||^{2} + \cos^{2}(A, B))^{-2}$$  
This quantity is non-constant and negative. The relationship between the Gaussian curvature and the local normalization weight is helpful for understanding intuitively what cosine similarity is doing to create semantically meaningful lists of words. In particular, it suggests that cosine differentiates short-norm, low-frequency vector pairs proportionally more from each other than word pairs involving long word vectors. 

This equation can be rearranged to parameterize the cosine similarity between $A$ and $B$ as a function of the discrepancy between the local normalization weight and the Gaussian curvature at this point:

$$\cos(A, B) = \sqrt{\sqrt{\frac{-1}{K}} - ||A||^2||B||^2 - 1}$$  
This representation of the quantity bears an interesting resemblance to the subsampling weight function used to downsample frequent terms in word2vec prior to forming the context window, $1-\sqrt{t/p(w)}$ (Mikolov et al. 2013). Goldberg and Levy (2014) comment that this subsampling procedure considerably changes the geometry of the resulting vector space. Because very common words are down-sampled, training samples are more likely to see more words that tend to co-occur with common words in the wider *effective* contexts where these common words would have been, and this effect is proportionally larger for rarer words because this change is additive. The variance in norms among low-frequency terms in SGNS is lower than it "should be" because some of this variation is taken up by the high-frequency terms, which consequently occupy a less locally scale-biased region of the inner product space. In Figure 5 (top panel) note that there is a smooth downward distortion of the distribution of squared vector norms after a certain point in the frequency distribution, resulting in a region where the mean vector norm is closer to uniform conditional on frequency rather than intensifying this relationship in the opposite direction (compare Figure 2 in Arora et al. 2016). 

```{r nelson_gaussiancurvature, echo=F, message=F, fig.height=8, out.width="\\textwidth", fig.align="center", include=F}
# Let's look at the Gaussian curvature next
# This shows that the decomposition is non-Euclidean
# It also helps us understand what cosine similarity does to the inner product space
# Mainly what it does is it bends the low-frequency space
# If you scale the LPNW the relative curvature in the rest of the manifold becomes more pronounced
ggarrange(ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=nprod, y=ab_cs, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=nprod, y=ab_ip, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=ab_cs, y=ab_ip, color=gK)) +
  geom_point(size=3) +
  scale_color_viridis_c(direction=-1, end = 0.95),
ncol=3, common.legend = T, legend="bottom")

# Show this in terms of frequency
ra_nemb %>% 
  ggplot(aes(x=log(a$frequency), y=gK, color=ab_cs)) +
  geom_point(size=3) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> gkp1
ra_nemb %>% 
  ggplot(aes(x=log(b$frequency), y=gK, color=ab_cs)) +
  geom_point(size=3) +
  scale_color_viridis_c() +
  theme(legend.position="bottom") -> gkp2
ggarrange(gkp1, gkp2, ncol=2, common.legend = T, legend="bottom")

# Better plot of curvature-frequency relationship
ra_nemb %>%
  arrange(desc(gK)) %>%
  ggplot(aes(x=log(a$frequency), y=log(b$frequency), color=gK)) +
  geom_point() +
  scale_color_viridis_c(direction=-1)
```


## Cosine similarities of linear combinations of word vectors

Researchers tend to operationalize discursive concepts by using fixed word lists to induce a set of vectors related to a concept of interest. To represent the concept, researchers aggregate the vectors into a sum vector or mean vector, and use the cosine similarity to these fixed points as a quantity of interest. This strategy tends to intensify the frequency bias relative to a vector-wise analysis.

Figures 2 and 3 jointly suggest that the cosine similarity to sum and mean vectors is systematically frequency biased in a way that increases in proportion to the number of terms included in the vector. The large subspaces implied by the vectors in Figure 2 correspond to cosine distributions that are more skewed in the extreme regions of the frequency distribution. This can be shown analytically. As mentioned in the introduction, the incorporation of sum and mean operations into cosine similarity quantities often takes forms like this:  
$$
\cos(A+B, C+D)
$$  
Comparisons of this type can be factored into a set of inner products between the $j, k$ selected sets of component vectors in the original vector space, but it also implies that the normalization weight function applied to these distances involves a multinomial combination of the pairwise component vector inner products:

$$
\begin{aligned}
\cos(A+B, C+D) &= \frac{\sum_{i}(A_i+B_i)(C_i+D_i)}{\sqrt{\sum_i(A_i+B_i)^2}\sqrt{\sum_i(C_i+D_i)^2}} \\
&(= \frac{\sum_{i}(A_i+B_i)(C_i+D_i)}{\phi(A+B, C+D)} = \frac{\sum_{i}(A_i+B_i)(C_i+D_i)}{\phi_{A+B,C+D}})\\
&= \frac{\sum_{i}(A_iC_i+B_iC_i+A_iD_i+B_iD_i)}{\phi_{A+B,C+D}}\\
&= \frac{\sum_{i}A_iC_i}{\phi_{A+B,C+D}}+\frac{\sum_{i}B_iC_i}{\phi_{A+B,C+D}}+\frac{\sum_{i}A_iD_i}{\phi_{A+B,C+D}}+\frac{\sum_{i}B_iD_i}{\phi_{A+B,C+D}}.\\
\end{aligned}
$$  
$$
\begin{aligned}
\phi_{A+B,C+D} &= \sqrt{\sum_i(A^2_i+B^2_i + A_iB_i)}\sqrt{\sum_i(C^2_i+D^2_i + C_iD_i)}\\
&= \sqrt{\sum_iA^2_i+\sum_iB^2_i + \sum_iA_iB_i}\sqrt{\sum_iC^2_i+\sum_iD^2_i + \sum_iC_iD_i} \\
&= \sqrt{||A||^2_2+||B||^2_2 + \left<A, B\right>}\sqrt{||C||^2_2+||D||^2_2 + \left<C, D\right>}
\end{aligned}
$$

## Centering and frequency bias

Mu and Viswanath (2018) suggest that the vector spaces learned by classic word embeddings exhibit improved cosine semantics when the global mean vector is subtracted from the space. Figure \ref{fig:global_composite_demeaning} illustrates the effect of global demeaning on frequency bias relative to the frequency distributions of cosine similarity and the inner product. Removing the mean vector improves the frequency uniformity of the cosine neighborhoods, but note that there is still some skew in the conditional distribution. Some skew is unavoidable due to the dependence of the shape of the sampling distribution on the dimensionality of the vector space.

```{r global_composite_demeaning, echo=F, warning=F, fig.height=8.5, out.width="0.9\\textwidth", fig.align="center", fig.cap="Centering the vector space (see Mu \\& Viswanath 2018) partially alleviates frequency bias, particularly in the top-N similarity region, but note non-constant variance and persistence of curvature (compare middle to top panel). The frequency bias profile of the demeaned space is closer to the behavior of the inner product, but the semantic behavior in the rare word region is notably less skewed from a top-N perspective than the inner product (compare middle to bottom panel)."}
# Compute demeaned mean vectors for comparison
nemb.demean <- float::sweep(nemb, 2, global_mean_vector)
# gmv2 <- view_from_composite(nemb.demean, global_mean_vector) %>% mutate(freq=sqrt(log(frequency)))
gmv2 <- view_from_composite(nemb.demean, colMeans(nemb.demean)) %>% mutate(freq=sqrt(log(frequency)))

gmv2 %>%
  arrange(snorm^2) %>%
  ggplot(aes(x=freq, y=similarity, color=snorm^2)) +
  geom_point() +
  geom_vline(xintercept=mean(gmv2$freq, na.rm=T), linetype="dotted", alpha=0.8) +
  geom_hline(yintercept=mean(gmv2$similarity, na.rm=T), linetype="dashed", alpha=0.8) +
  geom_smooth(method="gam", color="tomato") +
  scale_color_viridis_c() +
  theme(legend.position="bottom") +
  labs(title="Frequency-cosine plot, demeaned-global mean vector (origin)",
       x="Word frequency (sqrt log)",
       y="Cosine similarity",
       color=latex2exp::TeX("$||w_j||^2_2$")) -> gfb3
ggarrange(gfb1, gfb3, gfb2, ncol=1, common.legend = T, legend="bottom")
```

## Weight function estimation

Another implication of viewing similarity as a separate estimation procedure is that it pulls the source of frequency information in similarity metrics into focus. A recurrent feature of this type of analysis is that the local normalization weight function is estimated from the same vectors as the inner product. One could reasonably object to this procedure. In the context of regression analysis, when the local normalization weight is introduced into the other side of the model, the model fit will always go up considerably, but some of this added explanatory power is due to the fact that the errors are correlated.^[A closely related issue is that word embedding analyses are very sensitive to the intrinsic frequency bias of the \textit{word sampling procedure}. Researchers usually use the observed word count above a threshold in a corpus of documents as a plug-in estimate of the probability of observing a word, but this procedure is frequency-biased because it is much more sensitive to measurement error in the low frequency region near the threshold. The large number of very low-frequency words implies an unobserved set of related words that could have been included, and there is a steep bias-variance tradeoff involved in selecting the minimum frequency window. Both issues motivate considering an external and larger-scale word frequency estimate.] Figure \ref{fig:lfbias_coca_estimates} shows that using external word frequency information results in a similar pattern of bias using an external estimate of word frequency derived from the Corpus of Contemporary American English.^[There is a variable temporal and geographic fit between this dataset and the corpora underlying any given application. The corresponding model is informative about what happens to regression analysis with word embeddings when a covariate is added that is variably correlated to the local normalization weight with non-random measurement error. However, the implications of using an external frequency reference that is more or less relevant to the target corpus should be considered.] Researchers will tend to see a considerable jump in $\text{R}^2$ and changes in the substantive and statistical significance of estimated quantities of interest when an estimate of word frequency is added. The exact nature of the bias depends on how exactly the frequency information is specified into the model and the degree of fidelity to the original word frequency distribution estimated from the corpus.

```{r lfbias_coca_estimates, echo=F, message=F, fig.height=8.8, out.width="0.9\\textwidth", fig.align="center", fig.cap="Change in estimated difference in mean cosine similarity between paired social category mean vectors and social institution vectors (Nelson 2021). Original model estimate in blue and COCA-estimated frequency-adjusted linear model estimate in purple with parametric 95\\% confidence intervals superimposed. Labels indicate $\\text{R}^2$ in the adjusted model. The change in the difference in means reflects the relative difference between the implicit word cofrequency distributions $P(Category(A), Institution)$ and $P(Category(B), Institution)$."}
# XXX: This code produces the replication figure for Nelson (2021) but using an
# exogenous estimate of word frequency derived from COCA. It is copied directly
# from above, which is really ugly.

# Show linear model frequency bias 
linear_frequency_bias <- function(dimdf, dimtitle, use.scale=F, use.nprod=T, robust=F, ftest=F, stargazer=T, focal.p=F, only.dim=F, fullsumm=F, delta.rsq=F) {
  if(use.scale) {
    dimdf$fbmeasure <- dimdf$snorm^2
  } else if(use.nprod) {
    dimdf$fbmeasure <- 1/dimdf$nprod  
  } else {
    dimdf$fbmeasure <- log(dimdf$frequency)
  }
  ols <- lm(cs ~ identity, data=dimdf)
  ols.freqbias <- lm(cs ~ identity + fbmeasure, data=dimdf)
  ols.fb2 <- lm(cs ~ identity + fprod, data=dimdf)
  if(ftest) {
    anova(ols, ols.freqbias)
  } else if(stargazer) {
    stargazer::stargazer(ols, ols.freqbias, ols.fb2,
                         title=dimtitle,
                         dep.var.labels=c("Cosine similarity"),
                         dep.var.caption="",
                         covariate.labels=c("Social category vector", "Local normalization weight",
                                            "Scaled log COCA frequency", "Constant"),
                         star.cutoffs=c(0.05, 0.01, 0.001),
                         omit.stat=c("n", "adj.rsq"),
                         single.row = T,
                         column.sep.width = "1pt",
                         font.size = "footnotesize",
                         header=F)
  } else if(focal.p) {
    summary(polity_mb_fb.freqbias)$coefficients[2,4]
  } else if(only.dim) {
    # which.mod <- ols.freqbias  # Default
    which.mod <- ols.fb2  # Shows the figure with the COCA frequency estimate instead
    return(c(comparison = paste0(unique(dimdf$identity)),
             r2=summary(which.mod)$r.squared,
             coef(summary(which.mod))[2,1:2]))
  } else if(fullsumm) {
    if(robust) {
      print(coeftest(ols, vcov = vcovHC(ols, type="HC1")))
      print(coeftest(ols.freqbias, vcov = vcovHC(ols.freqbias, type="HC1")))
    } else {
      print(summary(ols))
      print(summary(ols.freqbias))
      summarize_ovb(dimdf)
    }
  } else if(delta.rsq) {
    data.frame(rsq=c(summary(ols)$r.squared, summary(ols.freqbias)$r.squared),
               model=c("original model", "frequency interaction"))
  } else {
    rbind.data.frame(data.frame(summary(ols)$coefficients, model="original model"),
                     data.frame(summary(ols.freqbias)$coefficients, model="frequency interaction")) %>%
      mutate(sig=tsig(`Pr...t..`))
  }
}

# Frequency bias tends to create omitted variable bias because frequency is correlated
#  with the cosine similarity and the relative frequency distributions differ by group
# Adding a measure of frequency into the model tends to increase the standard error on the
#  grouping variable in the difference in mean, and the model fit goes up a lot
# Note that this doesn't always make the effect "go away" per se; the problem is that
#  frequency has a somewhat unpredictable relationship to this problem because it depends
#  on the exact difference in mean log frequency by group and the linear correlation of 
#  the log frequency with the cosine similarity
# In general the reciprocal local normalization weight is the best frequency measure to use
#  because it's a component of cosine similarity, but any functional form will work; the 
#  key thing here is that the LPNW tends to vary by group (frequency bias is relative)
rbind(

# Black male <-> Black female
data.frame(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), comparison="Black, Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), comparison="Black, Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), comparison="Black, Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), comparison="Black, Male x Black, Female (domestic)"),

# White male <-> White female
data.frame(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), comparison="White Male x White Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), comparison="White Male x White Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), comparison="White Male x White Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), comparison="White Male x White Female (domestic)"),

# White female <-> Black female
data.frame(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), comparison="White Female x Black, Female (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), comparison="White Female x Black, Female (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), comparison="White Female x Black, Female (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), comparison="White Female x Black, Female (domestic)"),

# Black male <-> White male
data.frame(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), comparison="Black, Male x White Male (polity)"),
data.frame(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), comparison="Black, Male x White Male (economy)"),
data.frame(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), comparison="Black, Male x White Male (culture)"),
data.frame(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), comparison="Black, Male x White Male (domestic)"),

# White male <-> Black female
data.frame(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), comparison="White Male x Black, Female (polity)"),
data.frame(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), comparison="White Male x Black, Female (economy)"),
data.frame(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), comparison="White Male x Black, Female (culture)"),
data.frame(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), comparison="White Male x Black, Female (domestic)"),

# White female <-> Black male
data.frame(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), comparison="White Female x Black, Male (polity)"),
data.frame(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), comparison="White Female x Black, Male (economy)"),
data.frame(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), comparison="White Female x Black, Male (culture)"),
data.frame(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), comparison="White Female x Black, Male (domestic)"),

# Authority (Fig. 5)
data.frame(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), comparison="Black, Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), comparison="White Male x White Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), comparison="White Female x Black, Female (authority)"),
data.frame(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), comparison="Black, Male x White Male (authority)"),
data.frame(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), comparison="White Male x Black, Female (authority)"),
data.frame(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), comparison="White Female x Black, Male (authority)")) ->
  all_comparisons

# Get all of the regression results in one place
# This is unspeakably ugly but oh well
rbind(

# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)", only.dim=T, stargazer=F),

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)", only.dim=T, stargazer=F),

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)", only.dim=T, stargazer=F),

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)", only.dim=T, stargazer=F),

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)", only.dim=T, stargazer=F),
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)", only.dim=T, stargazer=F)) ->
  fblm_results_f

institutions <- c("polity", "economy", "culture", "domestic", "authority")
data.frame(fblm_results_f) %>%
  rowwise() %>%
  mutate(institution = institutions[which(str_detect(comparison1, institutions))],
         comparison = str_replace_all(paste0(str_remove(comparison2, institution), "- ",
                                             str_remove(comparison1, institution)), "_", " "),
         comparison2 = str_replace_all(paste0(str_remove(comparison1, institution), "- ",
                                              str_remove(comparison2, institution)), "_", " ")) %>%
  rename(lm.est.dim = Estimate, lm.est.se = Std..Error) ->
  fblm_results

# Plot all comparisons
all_comparisons %>%
  mutate(institution=str_remove_all(str_extract(comparison, "\\([a-z]+\\)"), "[()]")) %>%
  group_by(institution, identity) %>%
  summarize(mean_cs = mean(cs), sd_cs = sd(cs)) %>%
  ungroup() ->
  ac_summ

ac_summ %>%
  left_join(ac_summ, by="institution") %>%
  filter(!duplicated(paste0(pmin(identity.x, identity.y),
                            pmax(identity.x, identity.y))) &
           identity.x != identity.y) %>%
  mutate(diffm_cs = mean_cs.y - mean_cs.x,
         diff.se_cs = sqrt(sd_cs.x^2/k + sd_cs.y^2/k),
         comparison = str_replace_all(paste0(str_remove(identity.x, institution), "- ",
                                             str_remove(identity.y, institution)), "_", " ")) %>%
  left_join(fblm_results, by=c("comparison", "institution")) %>%
  left_join(fblm_results, by=c("comparison"="comparison2", "institution")) %>%
  mutate(lm.est.dim = as.numeric(coalesce(lm.est.dim.x, lm.est.dim.y)),
         lm.est.se = as.numeric(coalesce(lm.est.se.x, lm.est.se.y)),
         r2 = as.numeric(coalesce(r2.x, r2.y))) ->
  nemb_dim_estimates

nemb_dim_estimates %>%
  rowwise() %>%
  mutate(label.pin.x = max(lm.est.dim+(1.96*lm.est.se), diffm_cs+(1.96*diff.se_cs)) + 0.035,
         label.text = sprintf("R² = %s", round(r2, 3))) %>%
  ggplot() +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_point(aes(y=comparison, x=diffm_cs), color="steelblue2",
             position=position_nudge(y=0.1)) +
  geom_errorbarh(aes(y=comparison,
                     xmin=diffm_cs - 1.96*diff.se_cs,
                     xmax=diffm_cs + 1.96*diff.se_cs),
                 height=0.2, color="steelblue2", position=position_nudge(y=0.1)) +
  geom_point(aes(y=comparison, x=lm.est.dim), color="darkorchid3",
             position=position_nudge(y=-0.1)) +
  geom_errorbarh(aes(y=comparison,
                   xmin=lm.est.dim - 1.96*lm.est.se,
                   xmax=lm.est.dim + 1.96*lm.est.se),
               height=0.2, color="darkorchid3", position=position_nudge(y=-0.1)) +
  geom_label(aes(label=label.text, y=comparison, x=label.pin.x), size=3, color="darkorchid3") +
  scale_x_continuous(limits=c(-0.15,0.2), n.breaks=10) +
  facet_wrap(~institution, ncol=1) +
  labs(x="Difference in mean cosine similarity", y="")
```

\pagebreak

# Discussion

Log-bilinear word embedding models are fundamentally rooted in word-word co-frequency information. Meaningful heterogeneity over the distribution of word frequencies is a fact of life in semantic vector space-based analysis. It seems more reasonable to think that frequency information is not an erroneous feature of word embedding models or an irrelevant nuisance with respect to the search for meaning (Piantadosi 2014). Frequency bias emerges from a mismatch between the mathematical operations at our disposal for interpreting semantic vector spaces and the intuitive null model we use when we expect meaning to be unrelated to frequency. Similarity metrics encode different notions of relatedness that can be used to magnify different parts of the frequency distribution relative to focal positions of interest.

Cosine similarity remains a helpful way of surfacing matches to queries when this is the research goal. However, this is also exactly the part of the space that is the most affected by the size-biased sampling problem inherent in the way we have estimated the word frequencies (i.e. we are more likely to miss rarer words by chance). An implication of this idea is that cutting off the model at a fixed threshold can seriously impact the qualitative interpretation of the model, because this region of terms is also more specific on average. This implies that the minimum word frequency threshold is a critical hyperparameter for applications in the social sciences. When the research goal is to measure concepts by identifying subspaces of the vector space, it is important to consider how qualitative interpretations of this space are affected by perturbations to this threshold.

The use of fixed word lists to generate word pairs heightens the dependence of results on researchers' assumptions about how discourses are expressed in language. As noted above, this practice with word embeddings is commonly justified in terms of the performance of these models on analogy tasks. However, direct evaluation on analogy benchmarks is rare in the applied literature. More importantly, this practice tends to lead researchers toward two frequency-biasing practices: they focus on a relatively small and disproportionately frequent set of underlying lexemes (Antoniak \& Mimno 2018), and the use of multiple lists usually implies comparing different word frequency distributions. A consequence of frequency bias is that it is often possible to construct justifiable term lists that give different answers given the same comparative methodology (Ethayarajh, Duvenaud & Hirst 2018).

The findings in this paper have implications for replicability standards in social research with word embeddings. It can be difficult to describe word embedding models sufficiently specifically to facilitate replication. Pre-trained models are especially problematic from this perspective because the training code is not always available. When the underlying corpus is also unavailable, the interpretability of the corresponding embedding suffers. For example, the training corpora for the four publicly available GloVe models are completely unknown beyond the title of the source, and they cannot be reconstructed given what is known about the training process from the paper. These factors suggest that it is important for researchers releasing pre-trained word vectors to also provide the exact code that generates the word vectors; the corpus of texts included in the analysis; and some form of access to the word frequency distribution after pre-processing (i.e. as input into the model).

A theme up to this point has been that pre-processing decisions matter quite a bit in word embedding model-based analysis (Denny & Spirling 2018; Rodriguez & Spirling 2020). I conclude with an observation also relating to pre-processing. It appears that few researchers discard stopwords prior to estimating embeddings, but most researchers are also using context windows that imply a preference for non-syntactic associations. If this is the case, what is gained by embedding highly syntactic words like "the" that we are not likely to endow with any great significance relative to our research questions? In the social sciences, when the goal is generating expressive descriptions of concepts mobilized in samples of discourse, it might be worth conducting analyses that explicitly target maximally abstract word associations. 

\pagebreak

# Works referenced

\begingroup

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{4pt}
\onehalfspacing
\noindent

Andreas, J. and D. Klein. 2015. "When and why are log-linear models self-normalizing?" *Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, p. 244-249.

Antoniak, M and D. Mimno. 2018. "Evaluating the stability of embedding-based word similarities." *Transactions of the Association for Computational Linguistics* 6, p. 107-119.

Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A. 2016. "A latent variable model approach to PMI-based word embeddings." *Transactions of the Association for Computational Linguistics* 4, p. 385-399.

Arora, S., Liang, Y., and Ma, T. 2017. "A simple but tough-to-beat baseline for sentence embeddings." *5th International Conference on Learning Representations, ICLR 2017.*

Arseniev-Koehler, A. and J. Foster. 2020. "Machine learning as a model for cultural learning: Teaching an algorithm what it means to be fat." arXiv preprint:https://arxiv.org/abs/2003.12133.

Caillez, F., and Kuntz, P. (1996). "A contribution to the study of the metric and Euclidean structures of dissimilarities." *Psychometrika* 61(2), 241-253.

Cochrane, C., Rheault, L., Godbout, J. F., Whyte, T., Wong, M. W. C., and Borwein, S. 2022. "The Automatic Analysis of Emotion in Political Speech Based on Transcripts." *Political Communication* 39(1), 98-121.

Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. 1990. "Indexing by latent semantic analysis." *Journal of the American Society for Information Science* 41(6), 391-407.

Denny, M. J., and Spirling, A. 2018. "Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it." *Political Analysis* 26(2), 168-189.

Dominich, S. 2001. *Mathematical Foundations of Information Retrieval.* Springer.

Duncan, O.D. 1984. *Notes on Social Measurement: Historical and Critical.* Russell Sage Foundation.

Ethayarajh, K., Duvenaud, D., and Hirst, G. 2018. "Understanding Undesirable Word Embedding Associations." *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics* p. 1696-1705.

Goldberg, Y., and Levy, O. 2014. "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method." arXiv preprint arXiv:1402.3722.

Grimmer, J., Roberts, M. E., and Stewart, B. M. 2022. *Text as Data: A new framework for machine learning and the social sciences.* Princeton University Press.

Hofman, J. M., *et al.* 2021. "Integrating explanation and prediction in computational social science." *Nature* 595(7866), 181-188.

Jurafsky, D. and J.H. Martin. 2021. *Speech and Language Processing* (3rd edition). https://web.stanford.edu/~jurafsky/slp3/

Kozlowski, A. C., Taddy, M., and Evans, J. A. 2019. "The geometry of culture: Analyzing the meanings of class through word embeddings." *American Sociological Review* 84(5), 905-949.

Levy, O., and Goldberg, Y. 2014. "Neural word embedding as implicit matrix factorization." *Advances in neural information processing systems* 27.

Levy, O., Goldberg, Y., and Dagan, I. (2015). "Improving distributional similarity with lessons learned from word embeddings." *Transactions of the Association for Computational Linguistics* 3, 211-225.

Lix, K., Goldberg, A., Srivastava, S. B., and Valentine, M. A. (2022). "Aligning differences: Discursive diversity and team performance." *Management Science*.

Lowe, W. 2001. "Towards a theory of semantic space." *Proceedings of the Annual Meeting of the Cognitive Science Society 23*.

Lundberg, I., Johnson, R., and Stewart, B. M. 2021. "What is your estimand? Defining the target quantity connects statistical evidence to theory." *American Sociological Review* 86(3), 532-565.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. 2013. "Distributed representations of words and phrases and their compositionality." *Advances in neural information processing systems* 26.

Mimno, D., and Thompson, L. 2017. "The strange geometry of skip-gram with negative sampling." *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, p. 2873-2878.

Mnih, A., and Hinton, G. E. 2008. "A scalable hierarchical distributed language model." *Advances in neural information processing systems* 21.

Mu, J. and Viswanath, P. 2018. "All-but-the-top: Simple and effective postprocessing for word representations." *6th International Conference on Learning Representations, ICLR 2018*.

Nelson, L. K. 2020. "Computational grounded theory: A methodological framework." *Sociological Methods & Research* 49(1), 3-42.

Nelson, L. K. 2021. "Leveraging the alignment between machine learning and intersectionality: Using word embeddings to measure intersectional experiences of the nineteenth century US South." *Poetics* 88.

Pennington, J., Socher, R., and Manning, C. D. 2014. "Glove: Global vectors for word representation." *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*, p. 1532-1543.

Piantadosi, S. T. 2014. "Zipf’s word frequency law in natural language: A critical review and future directions." *Psychonomic Bulletin & Review* 21(5), 1112-1130.

Rodriguez, P. L., & Spirling, A. 2022. "Word embeddings: What works, what doesn’t, and how to tell the difference for applied research." *The Journal of Politics* 84(1).

Salton, G., and Buckley, C. 1988. "Term-weighting approaches in automatic text retrieval." *Information Processing & Management* 24(5), 513-523.

Singhal, A., Salton, G., and Buckley, C. 1995. "Length normalization in degraded text collections." Technical report, Cornell University.

Turney, P. D., and Pantel, P. 2010. "From frequency to meaning: Vector space models of semantics." *Journal of Artificial Intelligence Research* 37, 141-188.

van Loon, A., Giorgi, S., Willer, R., and Eichstaedt, J. 2022. "Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency." arXiv preprint arXiv:2201.08451.

Xie, Y. 2013. "Population heterogeneity and causal inference." *Proceedings of the National Academy of Sciences* 110(16), 6262-6268.

Zhou, K., Ethayarajh, K., and Jurafsky, D. 2021. "Frequency-based distortions in contextualized word embeddings." arXiv preprint arXiv:2104.08465.

Eck, N. J. V., and Waltman, L. 2009. "How to normalize cooccurrence data? An analysis of some well‐known similarity measures." *Journal of the American Society for Information Science and Technology* 60(8), 1635-1651.

\endgroup

\pagebreak

# Appendix: Additional tables, Nelson (2021)

```{r nelson_lfb_tables, echo=F, message=F, results="asis"}


tsig <- function(p) {
  ifelse(p < 0.001, "***",
         ifelse(p < 0.01, "**",
                ifelse(p < 0.05, "*", "x")))
}

summarize_ovb <- function(dimdf) {
  print(sprintf("Linear correlation of cosine similarity and log word frequency, %s: %f", unique(dimdf$identity)[1],
                cor(dimdf %>% slice_head(n=k) %$% cs, log(dimdf %>% slice_head(n=k) %$% log(frequency)))))
  print(sprintf("Linear correlation of cosine similarity and log word frequency, %s: %f",unique(dimdf$identity)[2],
                cor(dimdf %>% slice_tail(n=k) %$% cs, log(dimdf %>% slice_tail(n=k) %$% log(frequency)))))
  print(sprintf("Covariance of local normalization weight and group (i.e. relative frequency): %f",
                cov(dimdf$identity == unique(dimdf$identity)[1], dimdf$nprod)))
}

# Black male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.female_black_polity, "male_black_polity", "female_black_polity", mv_male_black, mv_female_black, polity_words), "Black, Male x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.female_black_economy, "male_black_economy", "female_black_economy", mv_male_black, mv_female_black, economy_words), "Black, Male x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.female_black_culture, "male_black_culture", "female_black_culture", mv_male_black, mv_female_black, culture_words), "Black, Male x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.female_black_domestic, "male_black_domestic", "female_black_domestic", mv_male_black, mv_female_black, domestic_words), "Black, Male x Black, Female (domestic)")

# White male <-> White female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_white_polity, "male_white_polity", "female_white_polity", mv_male_white, mv_female_white, polity_words), "White, Male x White, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_white_economy, "male_white_economy", "female_white_economy", mv_male_white, mv_female_white, economy_words), "White, Male x White, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_white_culture, "male_white_culture", "female_white_culture", mv_male_white, mv_female_white, culture_words), "White, Male x White, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_white_domestic, "male_white_domestic", "female_white_domestic", mv_male_white, mv_female_white, domestic_words), "White, Male x White, Female (domestic)")

# White female <-> Black female
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.female_black_polity, "female_white_polity", "female_black_polity", mv_female_white, mv_female_black, polity_words), "White, Female x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.female_black_economy, "female_white_economy", "female_black_economy", mv_female_white, mv_female_black, economy_words), "White, Female x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.female_black_culture, "female_white_culture", "female_black_culture", mv_female_white, mv_female_black, culture_words), "White, Female x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.female_black_domestic, "female_white_domestic", "female_black_domestic", mv_female_white, mv_female_black, domestic_words), "White, Female x Black, Female (domestic)")

# Black male <-> White male
linear_frequency_bias(make_dim_df(cs.male_black_polity, cs.male_white_polity, "male_black_polity", "male_white_polity", mv_male_black, mv_male_white, polity_words), "Black, Male x White, Male (polity)")
linear_frequency_bias(make_dim_df(cs.male_black_economy, cs.male_white_economy, "male_black_economy", "male_white_economy", mv_male_black, mv_male_white, economy_words), "Black, Male x White, Male (economy)")
linear_frequency_bias(make_dim_df(cs.male_black_culture, cs.male_white_culture, "male_black_culture", "male_white_culture", mv_male_black, mv_male_white, culture_words), "Black, Male x White, Male (culture)")
linear_frequency_bias(make_dim_df(cs.male_black_domestic, cs.male_white_domestic, "male_black_domestic", "male_white_domestic", mv_male_black, mv_male_white, domestic_words), "Black, Male x White, Male (domestic)")

# White male <-> Black female
linear_frequency_bias(make_dim_df(cs.male_white_polity, cs.female_black_polity, "male_white_polity", "female_black_polity", mv_male_white, mv_female_black, polity_words), "White, Male x Black, Female (polity)")
linear_frequency_bias(make_dim_df(cs.male_white_economy, cs.female_black_economy, "male_white_economy", "female_black_economy", mv_male_white, mv_female_black, economy_words), "White, Male x Black, Female (economy)")
linear_frequency_bias(make_dim_df(cs.male_white_culture, cs.female_black_culture, "male_white_culture", "female_black_culture", mv_male_white, mv_female_black, culture_words), "White, Male x Black, Female (culture)")
linear_frequency_bias(make_dim_df(cs.male_white_domestic, cs.female_black_domestic, "male_white_domestic", "female_black_domestic", mv_male_white, mv_female_black, domestic_words), "White, Male x Black, Female (domestic)")

# White female <-> Black male
linear_frequency_bias(make_dim_df(cs.female_white_polity, cs.male_black_polity, "female_white_polity", "male_black_polity", mv_female_white, mv_male_black, polity_words), "White, Female x Black, Male (polity)")
linear_frequency_bias(make_dim_df(cs.female_white_economy, cs.male_black_economy, "female_white_economy", "male_black_economy", mv_female_white, mv_male_black, economy_words), "White, Female x Black, Male (economy)")
linear_frequency_bias(make_dim_df(cs.female_white_culture, cs.male_black_culture, "female_white_culture", "male_black_culture", mv_female_white, mv_male_black, culture_words), "White, Female x Black, Male (culture)")
linear_frequency_bias(make_dim_df(cs.female_white_domestic, cs.male_black_domestic, "female_white_domestic", "male_black_domestic", mv_female_white, mv_male_black, domestic_words), "White, Female x Black, Male (domestic)")

# Authority (Fig. 5)
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.female_black_authority, "male_black_authority", "female_black_authority", mv_male_black, mv_female_black, authority_words), "Black, Male x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_white_authority, "male_white_authority", "female_white_authority", mv_male_white, mv_female_white, authority_words), "White, Male x White, Female (authority)")
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.female_black_authority, "female_white_authority", "female_black_authority", mv_female_white, mv_female_black, authority_words), "White, Female x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.male_black_authority, cs.male_white_authority, "male_black_authority", "male_white_authority", mv_male_black, mv_male_white, authority_words), "Black, Male x White, Male (authority)")
linear_frequency_bias(make_dim_df(cs.male_white_authority, cs.female_black_authority, "male_white_authority", "female_black_authority", mv_male_white, mv_female_black, authority_words), "White, Male x Black, Female (authority)")
linear_frequency_bias(make_dim_df(cs.female_white_authority, cs.male_black_authority, "female_white_authority", "male_black_authority", mv_female_white, mv_male_black, authority_words), "White, Female x Black, Male (authority)")
```
