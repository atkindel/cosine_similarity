---
title: "Cosine similarity arithmetic induces scale and frequency bias"
author: "Alexander T. Kindel^[PhD Candidate, Department of Sociology, Princeton University. Contact: akindel@princeton.edu.]"
abstract: |
 |  A widely-used measure of correlation between pairs of observations involves the cosine of the coplanar angle between them (i.e. "cosine similarity"). A common methodology for comparing these cosine similarities involves arithmetic operations like addition and subtraction, such as taking the arithmetic mean or comparing differencs in means. I show that various kinds of comparative arithmetic over cosine similarities share a key problem: they result in measures that are biased in proportion to the relative scale of the component vectors. Relative scale bias is a mechanical consequence of the fact that the cosine similarity is a function of the component vectors and their respective norms. An implication is that relative scale bias is not an error in an underlying model or model class, nor is it a feature of any particular application domain. Cosine similarities are useful for local comparisons between two vectors, but sums of cosine similarities intended to describe higher-order comparisons are scale dependent.
 | 
 | **Keywords:** cosine similarity, relative scale bias, measurement, arithmetic
date: |
  | 16 March 2022
  | 
  | 
output: 
  pdf_document: 
    latex_engine: xelatex
    keep_tex: TRUE
    number_sections: true
header-includes:
  - \usepackage{setspace}
  - \doublespacing
  - \setlength\parindent{24pt}
  - \usepackage[bottom]{footmisc}
---

```{r setup, include = FALSE}
library(tidyverse)
library(magrittr)
library(gridExtra)
library(latex2exp)
library(here)

# Document settings
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, cache.lazy=FALSE)
set.seed(2718281)

# Load term frequency data
# TODO: Use COCA frequencies instead of Wikipedia 2019 (see misc code repo for details)
freq_ <- read_table2(here("data", "enwiki-20190320-words-frequency.txt"), col_names = c("token", "freq_wiki2019"))
lefreq <- freq_$freq_wiki2019
names(lefreq) <- freq_$token
rm(freq_)

# Load THE MATRIX (embeddings) and get token list
# The choice of embeddings obviously always matters
# I think the Wikipedia + newswire one has the best semantics
# You see a lot more terms in the bigger ones, but not super "meaningful" ones
# "qantas.com is my favorite example"
glove_d <- read_table2(here("data", "glove.6B.300d.txt"), col_names = FALSE)  # Wikipedia & Gigaword 5
# glove_d <- read_table2(here("data", "glove.twitter.27B.200d.txt"), col_names = FALSE)  # Twitter
# glove_d <- read_table2(here("data", "glove.42B.300d.txt"), col_names = FALSE)  # Common Crawl (42B)
# glove_d <- read_table2(here("data", "glove.840B.300d.txt"), col_names = FALSE)  # Common Crawl (840B)
colnames(glove_d) <- c("token", sapply(1:(ncol(glove_d)-1), function(x) paste0("d", x)))
glove_mt <- glove_d %>% pivot_longer(contains("d"), names_to="dim")
glove_tokens <- unique(glove_mt$token)
glove_d %<>% select(-token) %>% as.matrix()
rownames(glove_d) <- glove_tokens
n_glove <- length(glove_tokens)
embm <- glove_d
etokens <- glove_tokens
nvoc <- n_glove
lefreq <- lefreq[etokens]  # Subset frequency list to this lexicon
rm(glove_mt, glove_d, glove_tokens)

# Alternatively, use word2vec
# I prefer GloVe but people also use this for some reason
# A useful preprocessing step is to prune the vocabulary to GloVe (higher quality)
# w2v <- read.word2vec(here("data", "GoogleNews-vectors-negative300.bin"))
# w2v_tokens <- summary(w2v, type='vocabulary')
# xemb_vocab <- w2v_tokens[which(w2v_tokens %in% glove_tokens)]
# w2v_d <- predict(w2v, xemb_vocab, type="embedding")
# n_w2v <- length(xemb_vocab)
# embm <- w2v_d
# etokens <- xemb_vocab
# nvoc <- n_w2v
# lefreq <- lefreq[etokens]
# rm(w2v, w2v_tokens, w2v_d)

# Set rarity index window
# GloVe is sorted by term frequency, approximately
# In practice, methodologies are hugely biased toward frequent words (proxied by low term index)
# In social science we are almost never looking at ultra-rare positive or negative terms
#  e.g. "statuesque", "rapturous", "felicitous" or "lachrymose", "lugubrious", "dolorous"
# And, a lot of the excess terminology in the larger embeddings is hard to interpret anyways
#  e.g. "qantas.com", "ribbonwork", "rhamnolipid", "synchronal", "thesis-writing"
# The rarity windows implied by the WEAT lexicons are potentially interesting to benchmark against as well
g_rmin <- 1
g_rmax <- nvoc
g10_rmax <- nvoc * 0.1

# Function to norm vectors
scale2 <- function(vc) {
  return(vc / norm(vc, "2"))
}
```

\pagebreak

# Introduction

Cosine similarity is widely used across the social sciences as a measure of correlation between observations. The cosine similarity of two $p$-dimensional vectors $A$ and $B$ is defined as the ratio of their inner product to the product of their norms:

$$cos(\theta_{AB}) = \frac {A \cdot B}{||A||\;||B||}$$  
The ratio describes the amount that the two vectors point in the same, different, or opposite direction; it is nominally bounded from -1 (pointing in opposite directions) to 0 (pointing in different/orthogonal directions) to 1 (pointing in the same direction).

Many researchers in the social sciences apply cosine similarity to measure patterns of cultural association, including schemas, analogies, ideologies, innovations, discourses, and stereotypes. This methodological strategy is particularly common in studies of large-scale patterns in language use, where researchers increasingly employ vector space embeddings of word co-occurrence data (e.g. GloVe, word2vec) to model semantic associations in large text corpora (see Salton & McGill 1986; Antoniak & Mimno 2018; Arseniev-Koehler 2020; Rodriguez & Spirling 2022; Grimmer, Roberts & Stewart 2022). Researchers in a wide variety of fields (cultural sociology, computer science, political science, management) have applied cosine similarity to measure several theoretical qualities of text, such as:  
\begin{itemize}\singlespace
  \item cultural schemas (Kozlowski et al. 2019; Arseniev-Koehler \& Foster 2022)
  \item stereotypical gender and race associations (Caliskan, Bryson \& Narayanan 2017; Jones et al. 2019; van Loon et al. 2022)
  \item intersectional identity (Nelson 2021)
  \item density of clusters of scientific terms (Evans 2010)
  \item sonic similarity of hit songs (Askin \& Mauskapf 2017)
  \item occupation name/description similarity (Martin-Caughey 2021)
  \item discursive diversity (Lix et al. 2022)
  \item contrarian climate discourse (Farrell 2016)
  \item ideology in political discourse (Fuhse et al. 2020)
  \item emotion in political speech (Cochrane et al. 2022)
\end{itemize}

Typically, researchers construct summary quantities over structured pairs of underlying term vectors by adding the corresponding cosine similarities together, as in the arithmetic mean. The typical methodology for this comparison involves adding the corresponding cosine similarities together. The purpose of this paper is to show that any such arithmetic comparison of cosine similarities generates two predictable biases in the resulting statistic.

- Relative scale bias originates from the non-centered distribution -- random angles are positive on average and tend to be more positive when the vectors are shorter; normalization does not solve this problem
- Relative frequency bias originates from the anisotropic distribution -- differences tend to reflect the frequency dimensions because these capture

I discuss the sources of these issues in more detail over the following two sections. Both sections employ the public GloVe embeddings of Wikipedia 2014 and Gigaword 5 (Pennington, Socher & Manning 2014).

```{r iprod_dnorm, echo=F, message=F, fig.height=5, fig.width=5, fig.align="center", fig.cap="Pairwise cosine similarities between $A$ and $B$ of size $k=100$. Pointwise comparisons are locally linear, but globally skewed by the local norm product. The sum norm (color) and difference norm (size) describe the ratio of the diagonals of the parallelogram formed by each pair of vectors; similar parallelograms are clustered, but the distribution of parallelograms is not uniform. Marginal densities are plotted on their respective axes."}
# Sample two sets of k random vectors from GloVe (with optional term index rarity window)
# Compute the norms, cosine similarities, and major/minor diagonals for each term pair
# Takes a long time for k >= 100
# Realistically people are using k = 20 to 50
# Appendix shows k = {75, 50, 25}
# Optionally, make set B be rarer
make_angles <- function(k=100, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
  sA <- sample(etokens[a_rmin:a_rmax], k)
  sB <- sample(etokens[b_rmin:b_rmax], k)
  expand_grid(i=1:k, j=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           lfr=log(afreq)/log(bfreq),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Different sampling procedure
# Split window by quantile, sample higher frequencies proportionately more
# Tries to get the suggested number of vectors, but with rounding.
# Only works with GloVe (w2v is not in order)
make_angles_2 <- function(k=48, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
  qnt <- 5
  a_rng_w <- round((a_rmax - a_rmin)/qnt)
  b_rng_w <- round((b_rmax - b_rmin)/qnt)
  sA <- c(sample(etokens[a_rmin:(a_rng_w*1/2)], k),
          sample(etokens[(a_rmin+a_rng_w*1/2):(a_rng_w*1)], round(k/2)),
          sample(etokens[(a_rmin+a_rng_w*1):(a_rng_w*2)], round(k/4)),
          sample(etokens[(a_rmin+a_rng_w*2):(a_rng_w*3)], round(k/8)),
          sample(etokens[(a_rmin+a_rng_w*3):(a_rng_w*5)], round(k/16)))
  sB <- c(sample(etokens[b_rmin:(b_rng_w*1/2)], k),
          sample(etokens[(b_rmin+b_rng_w*1/2):(b_rng_w*1)], round(k/2)),
          sample(etokens[(b_rmin+b_rng_w*1):(b_rng_w*2)], round(k/4)),
          sample(etokens[(b_rmin+b_rng_w*2):(b_rng_w*3)], round(k/8)),
          sample(etokens[(b_rmin+b_rng_w*3):(b_rng_w*5)], round(k/16)))
  expand_grid(i=1:k, j=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           lfr=log(afreq)/log(bfreq),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Only use each token once
# We can look at many more unique terms this way.
make_angles_3 <- function(k=10000, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
  sA <- sample(etokens[a_rmin:a_rmax], k)
  sB <- sample(etokens[b_rmin:b_rmax], k)
  data.frame(i=1:k, j=1:k) %>%
    rowwise() %>%
    mutate(aterm = sA[i],
           bterm = sB[j],
           anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           lfr=log(afreq)/log(bfreq),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Sample every "os"th word up to k
make_angles_max <- function(k=10000, os=5, ast=1, bst=2, stype="pairwise") {
  sA <- etokens[seq(ast, k, os)]
  sB <- etokens[seq(bst, k, os)]
  
  m <- NA
  if(stype == "pairwise") {
    m <- data.frame(aterm=sA, bterm=sB)
  } else if(stype == "twoway") {
    m <- expand.grid(aterm=sA, bterm=sB)
  } else if(stype == "flipped") {
    m <- data.frame(aterm=sA, bterm=rev(sB))
  }
  
  m %>%
    rowwise() %>%
    mutate(anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           azip = which(etokens == aterm),
           bzip = which(etokens == bterm),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           lfr=log(afreq)/log(bfreq),
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
           minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
    random_angles
  
  return(random_angles)
}

# Derived from make_angles_3 (sampling wo replacement)
# Describe planar geometry of each pair of vectors (using SVD)
# Sample uniformly from top quantile
# The result is a bunch of anisotropic parallelograms
# They are not evenly distributed in the column space or the row space
make_angles_geom <- function(k=10, topile = 3) {
  tok <- names(lefreq)[which(names(lefreq) %in% rownames(embm))]
  tmax <- length(tok) / topile
  sA <- sample(tok[1:tmax], k)
  sB <- sample(tok[1:tmax], k)
  
  data.frame(aterm=sA, bterm=sB) %>%
    rowwise() %>%
    mutate(anorm = norm(embm[aterm,], "2"),
           bnorm = norm(embm[bterm,], "2"),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
           ab_ip = embm[aterm,] %*% embm[bterm,],
           nprod = anorm * bnorm,
           vsnorm = norm(embm[aterm,] + embm[bterm,], "2"),
           vdnorm = norm(embm[aterm,] - embm[bterm,], "2"),
           vsnorm.sc = norm(embm[aterm,]/anorm + embm[bterm,]/bnorm, "2"),
           vdnorm.sc = norm(embm[aterm,]/anorm - embm[bterm,]/bnorm, "2"),
           sv1 = svd(embm[c(aterm, bterm),])$d[1],
           sv2 = svd(embm[c(aterm, bterm),])$d[2],
           sv1.sc = svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$d[1],
           sv2.sc = svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$d[2],
           basis.ang = lsa::cosine(svd(embm[c(aterm, bterm),])$v[,1],
                                svd(rbind(embm[aterm,]/anorm, embm[bterm,]/bnorm))$v[,1])) ->
    random_angles
  
  return(random_angles)
}

test <- make_angles_geom(500, topile=10)
test %>% 
  group_by(aterm, bterm, anorm, bnorm, afreq, bfreq,
           ab_cs, ab_ip, nprod,
           sv1, sv2, sv1.sc, sv2.sc, basis.ang) %>% 
  summarize(v1 = sv1^2 / (sv1^2 + sv2^2), 
            v2 = sv2^2 / (sv1^2 + sv2^2), 
            v1.sc = sv1.sc^2 / (sv1.sc^2 + sv2.sc^2), 
            v2.sc = sv2.sc^2 / (sv1.sc^2 + sv2.sc^2),
            normgap = v1 - v1.sc,
            freqratio = max(log(afreq), log(bfreq))/min(log(afreq), log(bfreq)),
            scaleratio = max(anorm, bnorm)/min(anorm, bnorm)) %>%
  ungroup() -> 
  test2

# Some informative plots
# Norm deflection score: abs(cos(theta)) between first principal axes of original and normed vector pairs
#  This is the angle between the plane implied by the original vectors and the plane implied by the normalized vectors
#  Note that these are **not** always the same subspace!
#  This value lies approximately on the interval [cos(0), cos(pi/4)]
# So they can be parallel or 
# Scale ratio: max/min norm{A, B} (how uneven )
# There is a clear relationship between the scale ratio and the norm deflection score wrt cos(A, B)
# In general the normed vectors aren't in the same column space as the original vectors
# Extremal cosine similarities must have a larger scale ratio when NDS < 1
# This relationship is self-similar (graph always looks the same no matter where you set the NDS threshold)

# Superimposing the positive and negative regions
# You can see the relationship better this way
test2 %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=abs(basis.ang), color=ab_cs)) +
  geom_point(size=2) +
  # geom_text(aes(label=ifelse(abs(ab_cs) > 0.18, paste0(aterm, "+", bterm), ""))) +
  scale_color_viridis_c()

test2 %>%
  filter(abs(basis.ang) > 0.95) %>%
  arrange(desc(abs(ab_cs))) %>%
  ggplot(aes(y=scaleratio, x=abs(basis.ang), color=ab_cs)) +
  geom_point(size=2) +
  # geom_text(aes(label=ifelse(abs(ab_cs) > 0.18, paste0(aterm, "+", bterm), ""))) +
  scale_color_viridis_c()

# Show NDS on the inner product manifold
# NDS describes Var[cos(A,B)|LPNW(A,B)]
# Smaller NDS values constrain cos(A, B) toward zero
test2 %>%
  arrange(desc(abs(basis.ang))) %>%
  ggplot(aes(x=nprod, color=abs(basis.ang), y=ab_cs)) +
  geom_point(size=2) +
  scale_color_viridis_c()

# By component vector % variance explained
# When the space is pointier/longer (?) the cosine similarities are more differentiated
test2 %>% ggplot(aes(x=v1, y=v1.sc, color=ab_cs)) + geom_point() + scale_color_viridis_c()

# Piecewise linear relationship between (scaled) norm of summed normalized vectors and spectral norm
# Cosine similarity is perfectly linear in both quantities
test %>% ggplot(aes(x=vsnorm.sc, y=sv1.sc, color=ab_cs)) + geom_point() + scale_color_viridis_c()

# Cosine similarity differentiates across the manifold when not normalized
test %>% ggplot(aes(x=vsnorm, y=sv1, color=ab_cs)) + geom_point() + scale_color_viridis_c()

# Now start scaling this up to pairs of cosine similarities
praxes <- function(i) {
  t1 <- analogy$aterm[i]
  t2 <- analogy$bterm[i]
  t3 <- analogy$cterm[i]
  t4 <- analogy$dterm[i]
  
  anorm = norm(embm[t1,], "2")
  bnorm = norm(embm[t2,], "2")
  cnorm = norm(embm[t3,], "2")
  dnorm = norm(embm[t4,], "2")
  
  v1ab <- svd(rbind(embm[t1,]/anorm, embm[t2,]/bnorm))$v[,1]
  v1cd <- svd(rbind(embm[t3,]/cnorm, embm[t4,]/dnorm))$v[,1]
  
  return(rbind(v1ab, v1cd))
}

# TODO: Tweaks to the below.
# - Adjust term frequency sampling window
# - What is abcd.sc.ang relative to mean CS?
# - How does this relate to the single pair relationship?
# - Use an SVD of the GloVe dimensions (should be full rank...)
#    - I bet this relies heavily on the skewed dimensions.

# xdim controls number of frequency-correlated dimensions to ignore (default zero)
make_angles_geomquad <- function(k=10, xdim=NA) {
  embms <- embm
  embms <- ifelse(is.na(xdim), embms, embms[,-c(excord$dj[1:xdim])])
  
  tok <- as.character(rownames(embms))
  sA <- sample(tok, k)
  sB <- sample(tok, k)
  sC <- sample(tok, k)
  sD <- sample(tok, k)
  
  data.frame(aterm=sA, bterm=sB, cterm=sC, dterm=sD) %>%
    rowwise() %>%
    mutate(anorm = norm(embms[aterm,], "2"),
           bnorm = norm(embms[bterm,], "2"),
           cnorm = norm(embms[cterm,], "2"),
           dnorm = norm(embms[dterm,], "2"),
           afreq = lefreq[aterm],
           bfreq = lefreq[bterm],
           cfreq = lefreq[cterm],
           dfreq = lefreq[dterm],
           ab_cs = lsa::cosine(embms[aterm,], embms[bterm,])[1],
           ab_ip = embms[aterm,] %*% embms[bterm,],
           nprod = anorm * bnorm,
           cd_cs = lsa::cosine(embms[cterm,], embms[dterm,])[1],
           cd_ip = embms[cterm,] %*% embms[dterm,],
           mprod = cnorm * dnorm,
           cosmean = mean(c(ab_cs, cd_cs)),
           abcd.sc.ang = lsa::cosine(svd(rbind(embms[aterm,]/anorm, embms[bterm,]/bnorm))$v[,1],
                                     svd(rbind(embms[cterm,]/cnorm, embms[dterm,]/dnorm))$v[,1]),
           abcd.sc.ang2 = lsa::cosine(svd(rbind(embms[aterm,], embms[bterm,]))$v[,1],
                                      svd(rbind(embms[cterm,], embms[dterm,]))$v[,1])) ->
    random_angles
  
  return(random_angles)
}

analogy <- make_angles_geomquad(500)
anlist <- lapply(1:500, praxes)  # Get all column spaces

# Angle between principal axes of normed vector pairs
# The mean cosine similarity is biased by the first angle
# The linear relationship tends to be pretty strong
analogy %>%
  ggplot(aes(x=abcd.sc.ang, y=cosmean)) +
  geom_point(size=2) + 
  geom_smooth(method="gam") + 
  geom_hline(yintercept=mean(analogy2$ab_cs), linetype="dashed") +
  scale_color_viridis_c() ->
  a1
analogy %>%
  ggplot(aes(x=abcd.sc.ang2, y=cosmean)) +
  geom_point(size=2) + 
  geom_smooth(method="gam") + 
  geom_hline(yintercept=mean(analogy2$ab_cs), linetype="dashed") +
  scale_color_viridis_c() ->
  a2
grid.arrange(a1, a2, ncol=2)


# Linear relationship between term frequency and each GloVe dimension
# Consider a large uniformly random term sample
freq_dims <- function(k=10) {
  sA <- sample(rownames(embm), k)
  return(data.frame(term=sA, lfreq=log(lefreq[sA]), embm[sA,]))
}

# Some of the dimensions have a strong frequency correlation
# Let's drop these successively and see how things move around
freq_dims(20000) %>% filter(!is.na(lfreq)) -> ex
sapply(1:300, function(j) cor(ex$lfreq, ex[,paste0("d", j)])) -> excor
excord <- data.frame(dj=1:300, fcor=excor, afcor=abs(excor)) %>% arrange(desc(afcor))

analogy2 <- make_angles_geomquad(k=200, xdim=5)
analogy3 <- make_angles_geomquad(200, 20)
analogy4 <- make_angles_geomquad(200, 50)


# Norm-frequency relationship
# NOTE: Currently using out of sample estimate of term frequencies
# The relationship is noisier than it would be if you had the exact frequencies for these embeddings
# Unfortunately these are not readily available for pretrained embeddings
tembm <- embm[names(lefreq)[!is.na(names(lefreq))],]  # Vectors for which we have frequencies
get_fn <- function(i) {
  term <- rownames(tembm)[i]
  freq <- lefreq[term]
  snorm <- norm(tembm[i,], "2")
  return(data.frame(term, freq, snorm))
}

tfn <- do.call(rbind.data.frame, lapply(1:nrow(tembm), get_fn))

# Plot the relationship
# Note that the slope increases in magnitude when trimming outliers
# Again, unfortunately using external term frequency data here
tfn %>%
  filter(freq > 1000) %>%
  ggplot(aes(y=snorm^2, x=log(freq))) +
  geom_point() +
  geom_smooth(method="lm")

# For this one, don't even use the embeddings
# Just generate a bunch of random iid STN vectors
# Generally, use same dimensions as prior ones
# make_angles_norm <- function(k=10000, p=300, a_rmin=g_rmin, a_rmax=g_rmax, b_rmin=g_rmin, b_rmax=g_rmax) {
#   nmat <- replicate(k*2, rnorm(p))
#   sA <- nmat[1:k,]
#   sB <- nmat[(k+1):nrow(nmat),]
#   data.frame(i=1:k, j=(k+1):nrow(nmat)) %>%
#     rowwise() %>%
#     mutate(anorm = norm(nmat[i,], "2"),
#            bnorm = norm(nmat[j,], "2"),
#            afreq = lefreq[i],
#            bfreq = lefreq[bterm],
#            lfr=log(afreq)/log(bfreq),
#            ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
#            ab_ip = embm[aterm,] %*% embm[bterm,],
#            nprod = anorm * bnorm,
#            plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
#            minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
#     random_angles
#   
#   return(random_angles)
# }


# Angles for WEAT
# ioff parameter offsets the index list to get terms with same frequency distribution
# make_angles_weat <- function(ioff = 0) {
#   weat_white <- c("adam", "harry", "josh", "roger", "alan", "frank", "justin", "ryan", "andrew", "jack",
#                 "matthew", "stephen", "brad", "greg", "paul", "jonathan", "peter", "amanda", "courtney",
#                 "heather", "melanie", "katie", "betsy", "kristin", "nancy", "stephanie", "ellen", "lauren",
#                 "colleen", "emily", "megan", "rachel")
#   weat_black <- c("alonzo", "jamel", "theo", "alphonse", "jerome", "leroy", "torrance", "darnell", "lamar",
#                   "lionel", "tyree", "deion", "lamont", "malik", "terrence", "tyrone", "lavon", "marcellus",
#                   "wardell", "nichelle", "shereen", "ebony", "latisha", "shaniqua", "jasmine", "tanisha",
#                   "tia", "lakisha", "latoya", "yolanda", "malika", "yvette")
#   weat_good <- c("caress", "freedom", "health", "love", "peace", "cheer", "friend", "heaven", "loyal", "pleasure",
#                  "diamond", "gentle", "honest", "lucky", "rainbow", "diploma", "gift", "honor", "miracle", "sunrise",
#                  "family", "happy", "laughter", "paradise", "vacation")
#   weat_bad <- c("abuse", "crash", "filth", "murder", "sickness", "accident", "death", "grief", "poison", "stink",
#                 "assault", "disaster", "hatred", "pollute", "tragedy", "bomb", "divorce", "jail", "poverty", "ugly",
#                 "cancer", "evil", "kill", "rotten", "vomit")
#   
#   sA <- c(weat_white, weat_black)
#   sB <- c(weat_good, weat_bad)
#   expand_grid(i=1:length(sA), j=1:length(sB)) %>%
#   rowwise() %>%
#   mutate(aterm = sA[i],
#          bterm = sB[j],
#          anorm = norm(embm[aterm,], "2"),
#          bnorm = norm(embm[bterm,], "2"),
#          azip = which(etokens == aterm),
#          bzip = which(etokens == bterm),
#          afreq = lefreq[aterm],
#          bfreq = lefreq[bterm],
#          ab_cs = lsa::cosine(embm[aterm,], embm[bterm,])[1],
#          ab_ip = embm[aterm,] %*% embm[bterm,],
#          nprod = anorm * bnorm,
#          plus_dnorm = norm((embm[aterm,]/anorm) + (embm[bterm,]/bnorm), "2"),
#          minus_dnorm = norm((embm[aterm,]/anorm) - (embm[bterm,]/bnorm), "2")) ->
#   weat_angles
#   
#   return(weat_angles)
# }

# Plot every "side" of the inner product decomposition
# You can see how the angular decomposition classifies the relative scale (parallelogram shape)
# But cosine similarities that are close have more similar relative scale
# And the distribution of relative scale is not uniform
plot_angle_manifold <- function(r_angles) {
  r_angles %>%
    ggplot(aes(x=nprod, y=ab_cs)) +
    # geom_point(aes(color=plus_dnorm, size=minus_dnorm)) +
    geom_point(aes(color=lfr, size=lfr), alpha=0.5) +
    scale_color_viridis_c() +
    theme(legend.position="bottom", legend.box="vertical", legend.margin=margin()) +
    labs(x=latex2exp::TeX("$||A||*||B||$"), y=latex2exp::TeX("$cos(A, B)$"),
         color=latex2exp::TeX("||D(+)||"), size=latex2exp::TeX("||D(-)||"))
}
```

```{r iprod_dnorm, echo=F, message=F, fig.height=5, fig.width=5, fig.align="center", fig.cap="Pairwise cosine similarities between $A$ and $B$ of size $k=100$. Pointwise comparisons are locally linear, but globally skewed by the local norm product. The sum norm (color) and difference norm (size) describe the ratio of the diagonals of the parallelogram formed by each pair of vectors; similar parallelograms are clustered, but the distribution of parallelograms is not uniform. Marginal densities are plotted on their respective axes."}
# Get a relatively big graph to work with
# Alternatively, also use a rarity window and/or up-sample the more frequent term space
# In practice I think the latter especially results in term samples that are more "realistic"
#  relative to what people are actually doing in the social sciences. Generally you want the
#  rare terms to be more common rare words and not "UCI_ProTour_cycling" or whatever
random_angles_200 <- make_angles(200)

# Plot with marginal densities
p <- plot_angle_manifold(random_angles_200)
ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 6,
  colour = "dodgerblue",
  fill = "dodgerblue",
  alpha = 0.3
)
```

```{r inpro, echo=F, message=F, fig.height=4, fig.width=6, fig.align="center"}
# Cosine similarity as a locally linear perspective on the inner product space
# The factorization implies you can kind of look at the manifold from three "sides"
local_linear_decomp <- function(rangs) {
  # Plot some inner product distribution by components. Larger objects in the
  # space have more differentiated IPs, but there are also way fewer of them.
  rangs %>%
    ggplot(aes(y=ab_cs, color=ab_ip, x=anorm*bnorm, size=ab_ip)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v1
  
  # Cosine similarity differentiates three regions on the inner product space
  # that we can more easily see if we look from the cosine similarity into the
  # manifold. Looking from this perspective helps you see the "twist" structure
  # in A by rotating to the position in which the "flat" structure has a minimal
  # profile and the twist has a maximal profile. The extremal region of the twist
  # (by LNPW) is generally more "flat" than it was from the other perspective.
  # The linear relationship between LNPW and the IP is controlled by cosine similarity.
  rangs %>%
    ggplot(aes(color=ab_cs, y=ab_ip, x=anorm*bnorm, size=ab_cs)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v2
  
  # Cosine similarity has a linear relationship to the inner product, but only
  # conditional on the local norm product weight, which defines a class of vector pairs
  # with the same CS-IP linear relationship. The distribution of cosine similarities in
  # any given class is generally skewed and tends to have non-zero mean. The variance of
  # the cosine similarity distribution is non-constant and varies systematically with the
  # local norm product weight. One way of thinking about this is that the cosine similarity
  # is a maximally linear perspective on the manifold.
  rangs %>%
    ggplot(aes(x=ab_cs, y=ab_ip, color=anorm*bnorm, size=anorm*bnorm)) +
    geom_point() +
    scale_color_viridis_c() +
    theme(legend.position="none") ->
    v3
  
  grid.arrange(v1, v2, v3, ncol=3)
}

local_linear_decomp(random_angles_200)
```

```{r columnspace}
# Up to this point, we know that the cosine similarity is a hyperbolic decomposition
# So we have a little more fear in our hearts about arithmetic operations on this surface
# Now we need to look at how this object refers to the dimensions of the embedding space
# In general, comparisons between two points are low-dimensional -- they don't use all 300 dimensions

# Look at frequency dependence in GloVe dimensions
# The inner product is constructed from frequency data
# Consider how this loads unevenly into (a) the LPNW and (b) the cosine similarity

# First, observe that a few GloVe dimensions are very obviously frequency-skewed
# These are also the dimensions with the highest standard deviation
dsd <- apply(embm, 2, sd)  # d277, d10, d245, d201

# Look at norms with and without these dimensions
dall_norm <- apply(embm, 1, function(x) norm(x, "2"))
dflat_norm <- apply(embm, 1, function(x) norm(x[-c(10, 201, 245, 277)], "2"))
dntop_norm <- apply(embm, 1, function(x) norm(x[-c(1, 2, 3, 4)], "2"))

# Get a big, uniformly random GloVe subspace; join term frequencies
data.frame(embm, rn=rownames(embm), dall_norm, dflat_norm, dntop_norm) %>%
  sample_frac(1)  %>%
  left_join(data.frame(rn=names(lefreq), freq=lefreq)) %>%
  filter(!is.na(freq)) %>%
  arrange(freq) %>%
  relocate(last_col()) ->
  gtest

# Plot
# Very frequent terms vary much less along the top singular vectors
# Consequently a term that has a large absolute d1 value *must* be rare
gtest %>%
  ggplot(aes(x=d1, y=d2, color=log(freq))) +
  geom_point() +
  scale_color_viridis_c() +
  coord_equal()

# Removing the skewed dimensions reduces the norms of the highest frequency terms more
gtest %>%
  ggplot(aes(x=dall_norm, y=dflat_norm, color=log(freq))) +
  geom_point()

gtest %>%
  ggplot(aes(x=dall_norm - dflat_norm, y=log(freq))) +
  geom_point()

epsil <- 0.5
gtest %>%
  filter((dall_norm - dflat_norm) > epsil) %>%
  ggplot(aes(x=dall_norm, y=dflat_norm, color=log(freq))) +
  geom_point()

# Look at the skew by SD
# Some of them are also tighter than average
# But the real action is in the top ~10 dims by SD
gtest_long %>% filter(name %in% names(sort(dsd))[1:20]) %>% ggplot(aes(x=value, y=log(freq))) + geom_point() + facet_wrap(~name)
gtest_long %>% filter(name %in% names(sort(dsd))[140:160]) %>% ggplot(aes(x=value, y=log(freq))) + geom_point() + facet_wrap(~name)
gtest_long %>% filter(name %in% names(sort(dsd))[281:300]) %>% ggplot(aes(x=value, y=log(freq))) + geom_point() + facet_wrap(~name)

# Relationship between Euclidean norms and frequency
# We have seen that high-frequency terms have low-to-negative 

# Now the cosine component
# Look at the relationship between log term frequency components and cosine similarity
# Do this for many uniformly random term pairs (from more common term subspace, for interpretability)
# ~5000 is many more term pairs than people are comparing in practice
# NOTE: Missing term frequency data is due to non-overlap between GloVe and WF vocabularies
#   This is pretty much only numeric tokens (e.g. "1992")
random_angles_pair5k <- make_angles_3(k=5000, a_rmax = g10_rmax*4, b_rmax = g10_rmax*4)
random_angles_pair5k_common <- make_angles_3(k=5000, a_rmax = g10_rmax, b_rmax = g10_rmax)
random_angles_50_common <- make_angles(50, a_rmax = g10_rmax, b_rmax = g10_rmax)


random_angles_50_common %>%
  filter(!is.na(afreq) & !is.na(bfreq)) %>%  # Missing term frequency data
  arrange(bfreq) %>%
  ggplot(aes(x=log(afreq), y=ab_cs, color=log(bfreq))) +
  geom_point() +
  geom_hline(yintercept=mean(random_angles_pair5k$ab_cs), linetype="dashed") +
  geom_smooth() ->
  fp1
random_angles_50_common %>%
  filter(!is.na(afreq) & !is.na(bfreq)) %>%  # Missing term frequency data
  arrange(afreq) %>%
  ggplot(aes(x=log(bfreq), y=ab_cs, color=log(afreq))) +
  geom_point() +
  geom_hline(yintercept=mean(random_angles_pair5k$ab_cs), linetype="dashed") +
  geom_smooth() ->
  fp2
grid.arrange(fp1, fp2, ncol=2)

# Simulate a regression problem
# We want to predict cosine similarity with a couple noisy/offset inner products
# random_angles_pair5k %>%
#   rowwise() %>%
#   mutate(cs_x1 = ab_cs + rpois(1, 4) - rpois(1, 4),
#          cs_x2 = ab_cs + runif(1, -5, 5),
#          cs_x3 = ab_cs + rnorm(1, 3, 3)) ->
#   test
# 
# summary(lm(ab_cs ~ cs_x1 + cs_x2 + cs_x3, data=test))
# summary(lm(ab_cs ~ cs_x1 + cs_x2 + cs_x3 + log(afreq) * log(bfreq), data=test))

# Now try it with frequency offset term sets
# Let the B set be somewhat more rare on average (in 51-75ile by freq rank)
# random_angles_pair5k_uneven <- make_angles_3(k=5000, a_rmax=200000, b_rmax=300000)
# 
# random_angles_pair5k_uneven %>%
#   ggplot(aes(x=log(afreq), y=ab_cs, color=log(bfreq))) +
#   geom_point() +
#   geom_hline(yintercept=mean(random_angles_pair5k$ab_cs), linetype="dashed") +
#   geom_smooth() ->
#   fp1
# random_angles_pair5k_uneven %>%
#   ggplot(aes(x=log(bfreq), y=ab_cs, color=log(afreq))) +
#   geom_point() +
#   geom_hline(yintercept=mean(random_angles_pair5k$ab_cs), linetype="dashed") +
#   geom_smooth() ->
#   fp2
# grid.arrange(fp1, fp2, ncol=2)

# You can do this with the two-way sum components too
# This looks weird because it is a fundamentally weird comparison to make (imo)
# In particular, the reuse of terms makes the values weirdly clustered
# You'd need to adjust for this but not clear how that fits with the underlying RQ
# random_angles_200 %>%
#   ggplot(aes(x=log(afreq), y=ab_cs, color=log(bfreq))) +
#   geom_point() +
#   geom_smooth() ->
#   fp1
# random_angles_200 %>%
#   ggplot(aes(x=log(bfreq), y=ab_cs, color=log(afreq))) +
#   geom_point() +
#   geom_smooth() ->
#   fp2
# grid.arrange(fp1, fp2, ncol=2)

# Look at full inner product space
# Pairs are not evenly distributed over this surface by frequency
# You can also look from the B term's perspective
# Doing this is much more interesting if B_i is drawn from a different frequency distribution
# random_angles_pair5k %>%
#   filter(!is.na(afreq)) %>%
#   arrange(afreq) %>%
#   ggplot(aes(x=nprod, y=ab_cs, color=log(afreq))) +
#   geom_jitter(size=4) +
#   scale_color_viridis_c() ->
#   lf1
# random_angles_pair5k %>%
#   filter(!is.na(afreq)) %>%
#   arrange(afreq) %>%
#   ggplot(aes(x=nprod, y=ab_ip, color=log(afreq))) +
#   geom_jitter(size=4) +
#   scale_color_viridis_c() ->
#   lf2
# random_angles_pair5k %>%
#   filter(!is.na(afreq)) %>%
#   arrange(afreq) %>%
#   ggplot(aes(x=ab_cs, y=ab_ip, color=log(afreq))) +
#   geom_jitter(size=4) +
#   scale_color_viridis_c() ->
#   lf3
# grid.arrange(lf1, lf2, lf3, ncol=3)

# Break down by frequency quantiles
# It becomes more uniform in the middle
# random_angles_pair5k$afq <- ntile(log(random_angles_pair5k$afreq), 12)
# random_angles_pair5k$bfq <- ntile(log(random_angles_pair5k$bfreq), 12)
# 
# random_angles_pair5k %>%
#   filter(!is.na(afreq)) %>%
#   arrange(afreq) %>%
#   ggplot(aes(x=nprod, y=ab_cs, color=log(afreq))) +
#   geom_jitter(size=4) +
#   scale_color_viridis_c() +
#   facet_wrap(~afq)
# 
# random_angles_pair5k %>%
#   filter(!is.na(bfreq)) %>%
#   arrange(desc(bfreq)) %>%
#   ggplot(aes(x=nprod, y=ab_cs, color=log(bfreq))) +
#   geom_jitter(size=4) +
#   scale_color_viridis_c() +
#   facet_wrap(~bfq)

# TODO: How much does cosine vs. IP draw on the glove dimensions at different freq ratios?
# TODO: Do this by comparing the SVD column loadings for pairs of terms
# TODO: Are these highly correlated within A terms (i.e. is the two-way sum more borked)
```